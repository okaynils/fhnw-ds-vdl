{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vdl Depth and Class Constrained Diffusion\n",
    "- **Name:** Nils Fahrni\n",
    "- **Date:** 07.01.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "- 894 classes -> 18 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "from data.nyuv2 import NYUDepthV2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "mean = [0.5105, 0.4662, 0.4438]\n",
    "std = [0.2273, 0.2275, 0.2409]\n",
    "\n",
    "image_t = transforms.Compose([\n",
    "    transforms.CenterCrop(400),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    "\n",
    "crop_t = transforms.Compose([\n",
    "    transforms.CenterCrop(400),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "dataset = NYUDepthV2(root='data', \n",
    "                     download=True, \n",
    "                     preload=False, \n",
    "                     image_transform=image_t, \n",
    "                     seg_transform=crop_t, \n",
    "                     depth_transform=crop_t, \n",
    "                     filtered_classes=[5, 11, 21, 26, 2, 3, 7, 64, 144, 19, 119, 157, 28, 55, 15, 59, 4, 83])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train, validation, and test partitions.\n",
    "\n",
    "    :param dataset: The NYUDepthV2 dataset object.\n",
    "    :param train_ratio: Proportion of the dataset to allocate to the training set.\n",
    "    :param val_ratio: Proportion of the dataset to allocate to the validation set.\n",
    "    :param test_ratio: Proportion of the dataset to allocate to the test set.\n",
    "    :param random_seed: Seed for reproducibility of the split.\n",
    "    :return: A tuple of (train_dataset, val_dataset, test_dataset).\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Total size of the dataset\n",
    "    total_size = len(dataset)\n",
    "    indices = np.arange(total_size)\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_split = int(train_ratio * total_size)\n",
    "    val_split = train_split + int(val_ratio * total_size)\n",
    "    \n",
    "    # Split the indices\n",
    "    train_indices = indices[:train_split]\n",
    "    val_indices = indices[train_split:val_split]\n",
    "    test_indices = indices[val_split:]\n",
    "    \n",
    "    # Create dataset subsets\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unnormalize(img, mean, std):\n",
    "    mean = torch.tensor(mean).view(-1, 1, 1)  # Reshape to [C, 1, 1]\n",
    "    std = torch.tensor(std).view(-1, 1, 1)    # Reshape to [C, 1, 1]\n",
    "    img = std * img + mean\n",
    "    img = torch.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "dataset[0][4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1771907\n",
      "Output shape: torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from core import UNet_Baseline\n",
    "\n",
    "# Instantiate the model\n",
    "net = UNet_Baseline(num_classes=18, device=\"cuda\").to(\"cuda\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in net.parameters())}\")\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randn(1, 3, 64, 64).to(\"cuda\")\n",
    "t = torch.randint(0, 1000, (x.shape[0],)).to(\"cuda\")\n",
    "class_vector = torch.ones((x.shape[0], 18)).to(\"cuda\")\n",
    "depth_vector = torch.zeros((x.shape[0], 18)).to(\"cuda\")\n",
    "\n",
    "# Forward pass\n",
    "output = net(x, t, class_vector, depth_vector)\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        epsilon = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, class_vectors=None, depth_vectors=None, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images from {model.__class__.__name__}...\")\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                \n",
    "                current_class_vectors = class_vectors if class_vectors is not None else None\n",
    "                current_depth_vectors = depth_vectors if depth_vectors is not None else None\n",
    "              \n",
    "                predicted_noise = model(x, t, current_class_vectors, current_depth_vectors)\n",
    "                \n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        \n",
    "        model.train()\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import wandb\n",
    "from core import EMA\n",
    "import logging\n",
    "from metrics import calculate_fid, calculate_psnr\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, diffusion, optimizer, epochs, device, train_dataloader, val_dataloader=None, \n",
    "                 run_name='diffusion_model', project_name='diffusion_project', save_dir='models', ema_decay=0.995, \n",
    "                 sample_images_every=100, resolved_names=None, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.diffusion = diffusion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.run_name = run_name\n",
    "        self.project_name = project_name\n",
    "        self.save_dir = save_dir\n",
    "        self.ema_decay = ema_decay\n",
    "        self.resolved_names = resolved_names\n",
    "\n",
    "        # Initialize EMA\n",
    "        self.ema = EMA(ema_decay)\n",
    "        self.ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
    "\n",
    "        # Initialize Weights & Biases\n",
    "        wandb.init(project=self.project_name, name=self.run_name)\n",
    "        self.run_id = wandb.run.id\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stopping_counter = 0\n",
    "        self.sample_images_every = sample_images_every\n",
    "\n",
    "        # Create a models directory if it doesn't exist\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, segments, depths, class_vectors, depth_vectors in self.train_dataloader:\n",
    "            images = images.to(self.device)\n",
    "            class_vectors = class_vectors.to(self.device)\n",
    "            depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "            # Sample time steps\n",
    "            t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "\n",
    "            # Add noise to images\n",
    "            x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "            # Predict noise using the model\n",
    "            predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update EMA model\n",
    "            self.ema.step_ema(self.ema_model, self.model)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Step the scheduler if it's provided\n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        avg_loss = epoch_loss / len(self.train_dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, segments, depths, class_vectors, depth_vectors in self.val_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                class_vectors = class_vectors.to(self.device)\n",
    "                depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "                t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "                x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "                predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "                loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.val_dataloader)\n",
    "        return avg_val_loss\n",
    "\n",
    "    def _save_model(self, val_loss):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            model_name = f\"{self.run_name}_{self.run_id}.pth\"\n",
    "            save_path = os.path.join(self.save_dir, model_name)\n",
    "            torch.save(self.model.state_dict(), save_path)\n",
    "            torch.save(self.ema_model.state_dict(), save_path.replace(\".pth\", \"_ema.pth\"))\n",
    "            logger.info(f\"Models saved to {self.save_dir} with val_loss {val_loss:.4f}\")\n",
    "\n",
    "    def _plot_samples(self, n=5, n_present_classes=3, depth_lower=0.1, depth_upper=3.0):\n",
    "        indices = torch.stack([torch.randperm(18)[:n_present_classes] for _ in range(n)])\n",
    "        class_vectors = torch.zeros((n, 18)).to(self.device)\n",
    "        rows = torch.arange(n).unsqueeze(1)\n",
    "        class_vectors[rows, indices] = 1\n",
    "        \n",
    "        depth_vectors = torch.rand(n, 18).to(self.device) * depth_upper + depth_lower\n",
    "        \n",
    "        class_labels = []\n",
    "        for i in range(n):\n",
    "            indices = torch.where(class_vectors[i] == 1)[0].tolist()\n",
    "            labels = [(self.resolved_names[idx], depth_vectors[i, idx].item()) for idx in indices]\n",
    "            class_labels.append(labels)\n",
    "\n",
    "        default_sampled_images = self.diffusion.sample(self.model, n=n, class_vectors=class_vectors, depth_vectors=depth_vectors)\n",
    "        ema_sampled_images = self.diffusion.sample(self.ema_model, n=n, class_vectors=class_vectors, depth_vectors=depth_vectors)\n",
    "        \n",
    "        fig_default, axes_default = plt.subplots(1, n, figsize=(n * 3, 3))\n",
    "        for i, ax in enumerate(axes_default):\n",
    "            img = default_sampled_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(\"\\n\".join([f\"{cls}: {depth:.2f}\" for cls, depth in class_labels[i]]), fontsize=8)\n",
    "        fig_default.tight_layout()\n",
    "        plt.close(fig_default)\n",
    "\n",
    "        fig_ema, axes_ema = plt.subplots(1, n, figsize=(n * 3, 3))\n",
    "        for i, ax in enumerate(axes_ema):\n",
    "            img = ema_sampled_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(\"\\n\".join([f\"{cls}: {depth:.2f}\" for cls, depth in class_labels[i]]), fontsize=8)\n",
    "        fig_ema.tight_layout()\n",
    "        plt.close(fig_ema)\n",
    "        \n",
    "        wandb.log({\"Default Model Samples\": wandb.Image(fig_default),\n",
    "                   \"EMA Model Samples\": wandb.Image(fig_ema)})\n",
    "\n",
    "    def _validate_samples(self, num_samples=100, n_plot=5):\n",
    "        \"\"\"\n",
    "        Validates the model by computing FID and PSNR and logs the results.\n",
    "        Args:\n",
    "            num_samples (int): Number of samples to generate for FID and PSNR calculation.\n",
    "            n_plot (int): Number of real and generated samples to log on WandB.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        real_images_list = []\n",
    "        generated_images_list = []\n",
    "        ema_generated_images_list = []\n",
    "        psnr_values = []\n",
    "        plot_real_images = []\n",
    "        plot_generated_images = []\n",
    "        plot_ema_generated_images = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (images, _, _, class_vectors, depth_vectors) in enumerate(self.val_dataloader):\n",
    "                images = images.to(self.device)\n",
    "                class_vectors = class_vectors.to(self.device)\n",
    "                depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "                # Collect real images\n",
    "                real_images_list.append(images)\n",
    "                if len(plot_real_images) < n_plot:\n",
    "                    plot_real_images.append(images[:n_plot])\n",
    "\n",
    "                # Generate corresponding images using the default model\n",
    "                generated_images = self.diffusion.sample(self.model, n=images.size(0),\n",
    "                                                          class_vectors=class_vectors,\n",
    "                                                          depth_vectors=depth_vectors)\n",
    "                generated_images_list.append(generated_images)\n",
    "                if len(plot_generated_images) < n_plot:\n",
    "                    plot_generated_images.append(generated_images[:n_plot])\n",
    "\n",
    "                # Generate corresponding images using the EMA model\n",
    "                ema_generated_images = self.diffusion.sample(self.ema_model, n=images.size(0),\n",
    "                                                             class_vectors=class_vectors,\n",
    "                                                             depth_vectors=depth_vectors)\n",
    "                ema_generated_images_list.append(ema_generated_images)\n",
    "                if len(plot_ema_generated_images) < n_plot:\n",
    "                    plot_ema_generated_images.append(ema_generated_images[:n_plot])\n",
    "\n",
    "                # Compute PSNR for the batch\n",
    "                batch_psnr = calculate_psnr(images, generated_images)\n",
    "                psnr_values.append(batch_psnr)\n",
    "\n",
    "                if len(real_images_list) * images.size(0) >= num_samples:\n",
    "                    break\n",
    "\n",
    "        # Concatenate collected images\n",
    "        real_images = torch.cat(real_images_list, dim=0)[:num_samples]\n",
    "        generated_images = torch.cat(generated_images_list, dim=0)[:num_samples]\n",
    "        ema_generated_images = torch.cat(ema_generated_images_list, dim=0)[:num_samples]\n",
    "\n",
    "        # Compute FID\n",
    "        fid_score = calculate_fid(real_images, generated_images, self.device)\n",
    "\n",
    "        # Average PSNR\n",
    "        avg_psnr = sum(psnr_values) / len(psnr_values)\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb.log({\"FID\": fid_score, \"PSNR\": avg_psnr})\n",
    "        logger.info(f\"FID: {fid_score:.4f}, PSNR: {avg_psnr:.4f}\")\n",
    "\n",
    "        # Log sample images to WandB\n",
    "        self._log_samples_to_wandb(torch.cat(plot_real_images, dim=0)[:n_plot],\n",
    "                                   torch.cat(plot_generated_images, dim=0)[:n_plot],\n",
    "                                   torch.cat(plot_ema_generated_images, dim=0)[:n_plot],\n",
    "                                   class_vectors[:n_plot], depth_vectors[:n_plot])\n",
    "\n",
    "    def _log_samples_to_wandb(self, real_images, generated_images, ema_generated_images, class_vectors, depth_vectors):\n",
    "        \"\"\"\n",
    "        Logs real and generated image samples to WandB.\n",
    "        Args:\n",
    "            real_images (torch.Tensor): Batch of real images to log.\n",
    "            generated_images (torch.Tensor): Batch of generated images to log.\n",
    "            ema_generated_images (torch.Tensor): Batch of EMA-generated images to log.\n",
    "            class_vectors (torch.Tensor): Corresponding class vectors.\n",
    "            depth_vectors (torch.Tensor): Corresponding depth vectors.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(3, len(real_images), figsize=(len(real_images) * 3, 9))\n",
    "\n",
    "        real_images = unnormalize(real_images.cpu(), mean=mean, std=std)\n",
    "        generated_images = unnormalize(generated_images.cpu(), mean=mean, std=std)\n",
    "        \n",
    "        # Add row labels on the left\n",
    "        row_labels = [\"Real Images\", \"Generated Images\", \"Generated Images (EMA)\"]\n",
    "        for row_idx, label in enumerate(row_labels):\n",
    "            axes[row_idx][0].set_ylabel(label, fontsize=12, rotation=0, labelpad=50, ha='right', va='center')\n",
    "\n",
    "        for i, ax in enumerate(axes[0]):\n",
    "            img = real_images[i].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "        for i, ax in enumerate(axes[1]):\n",
    "            img = generated_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "\n",
    "        for i, ax in enumerate(axes[2]):\n",
    "            img = ema_generated_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.tick_params(left=False, bottom=False, labelleft=False, labelbottom=False)\n",
    "            labels = [f\"{self.resolved_names[idx]}: {depth_vectors[i, idx]:.2f}\"\n",
    "                    for idx in torch.where(class_vectors[i] == 1)[0].tolist()]\n",
    "            ax.set_xlabel(\"\\n\".join(labels), fontsize=8)\n",
    "\n",
    "        # Adjust layout to make space for row labels and x-labels\n",
    "        fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        wandb.log({\"Validation Samples\": wandb.Image(fig)})\n",
    "        plt.close(fig)\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            logger.info(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "            \n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "            train_loss = self._train_epoch()\n",
    "            val_loss = self._validate_epoch()\n",
    "            \n",
    "            wandb.log({\"epoch\": epoch + 1,\n",
    "                       \"train_loss\": train_loss,\n",
    "                       \"val_loss\": val_loss,\n",
    "                       \"learning_rate\": current_lr})\n",
    "            \n",
    "            logger.info(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if epoch % self.sample_images_every == 0:\n",
    "                self._validate_samples(num_samples=100, n_plot=10)\n",
    "\n",
    "            self._save_model(val_loss)\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        self.model.eval()\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, segments, depths, class_vectors, depth_vectors in test_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                class_vectors = class_vectors.to(self.device)\n",
    "                depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "                t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "                x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "                predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "                loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_dataloader)\n",
    "        logger.info(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "        wandb.log({\"test_loss\": avg_test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 16\n",
      "Validation samples: 4\n",
      "Testing samples: 4\n"
     ]
    }
   ],
   "source": [
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset[:24], train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mokaynils\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\wandb\\run-20241226_120934-hc5hxgq4</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/okaynils/vdl/runs/hc5hxgq4' target=\"_blank\">Attention_UNet_Overfit</a></strong> to <a href='https://wandb.ai/okaynils/vdl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/okaynils/vdl' target=\"_blank\">https://wandb.ai/okaynils/vdl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/okaynils/vdl/runs/hc5hxgq4' target=\"_blank\">https://wandb.ai/okaynils/vdl/runs/hc5hxgq4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:09:35 - INFO: Epoch 1/5000\n",
      "12:09:37 - INFO: Train Loss: 1.1755, Val Loss: 1.0649\n",
      "12:09:37 - INFO: Sampling 4 new images from UNet_Attn...\n",
      "999it [00:52, 18.93it/s]\n",
      "12:10:29 - INFO: Sampling 4 new images from UNet_Attn...\n",
      "999it [00:50, 19.59it/s]\n",
      "12:11:23 - INFO: FID: 313563047509242803948355584.0000, PSNR: 4.4174\n",
      "12:11:23 - WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-8.738332..9.385325].\n",
      "12:11:23 - WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-9.563246..8.954178].\n",
      "12:11:23 - WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-7.615521..8.594738].\n",
      "12:11:23 - WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-9.645662..8.049078].\n",
      "12:11:25 - INFO: Models saved to models with val_loss 1.0649\n",
      "12:11:25 - INFO: Epoch 2/5000\n",
      "12:11:26 - INFO: Train Loss: 1.0518, Val Loss: 1.0181\n",
      "12:11:26 - INFO: Models saved to models with val_loss 1.0181\n",
      "12:11:26 - INFO: Epoch 3/5000\n",
      "12:11:27 - INFO: Train Loss: 1.0192, Val Loss: 1.0084\n",
      "12:11:27 - INFO: Models saved to models with val_loss 1.0084\n",
      "12:11:27 - INFO: Epoch 4/5000\n",
      "12:11:28 - INFO: Train Loss: 0.9946, Val Loss: 0.9878\n",
      "12:11:28 - INFO: Models saved to models with val_loss 0.9878\n",
      "12:11:28 - INFO: Epoch 5/5000\n",
      "12:11:29 - INFO: Train Loss: 0.9789, Val Loss: 0.9631\n",
      "12:11:29 - INFO: Models saved to models with val_loss 0.9631\n",
      "12:11:29 - INFO: Epoch 6/5000\n",
      "12:11:30 - INFO: Train Loss: 0.9711, Val Loss: 0.9459\n",
      "12:11:30 - INFO: Models saved to models with val_loss 0.9459\n",
      "12:11:30 - INFO: Epoch 7/5000\n",
      "12:11:31 - INFO: Train Loss: 0.9537, Val Loss: 0.9417\n",
      "12:11:31 - INFO: Models saved to models with val_loss 0.9417\n",
      "12:11:31 - INFO: Epoch 8/5000\n",
      "12:11:32 - INFO: Train Loss: 0.9313, Val Loss: 0.9313\n",
      "12:11:32 - INFO: Models saved to models with val_loss 0.9313\n",
      "12:11:32 - INFO: Epoch 9/5000\n",
      "12:11:33 - INFO: Train Loss: 0.9192, Val Loss: 0.9077\n",
      "12:11:33 - INFO: Models saved to models with val_loss 0.9077\n",
      "12:11:33 - INFO: Epoch 10/5000\n",
      "12:11:34 - INFO: Train Loss: 0.9137, Val Loss: 0.9212\n",
      "12:11:34 - INFO: Epoch 11/5000\n",
      "12:11:35 - INFO: Train Loss: 0.9215, Val Loss: 0.8721\n",
      "12:11:35 - INFO: Models saved to models with val_loss 0.8721\n",
      "12:11:35 - INFO: Epoch 12/5000\n",
      "12:11:36 - INFO: Train Loss: 0.8798, Val Loss: 0.8526\n",
      "12:11:36 - INFO: Models saved to models with val_loss 0.8526\n",
      "12:11:36 - INFO: Epoch 13/5000\n",
      "12:11:37 - INFO: Train Loss: 0.8681, Val Loss: 0.8481\n",
      "12:11:37 - INFO: Models saved to models with val_loss 0.8481\n",
      "12:11:37 - INFO: Epoch 14/5000\n",
      "12:11:38 - INFO: Train Loss: 0.8497, Val Loss: 0.9465\n",
      "12:11:38 - INFO: Epoch 15/5000\n",
      "12:11:38 - INFO: Train Loss: 0.8183, Val Loss: 0.8013\n",
      "12:11:39 - INFO: Models saved to models with val_loss 0.8013\n",
      "12:11:39 - INFO: Epoch 16/5000\n",
      "12:11:39 - INFO: Train Loss: 0.8078, Val Loss: 0.9057\n",
      "12:11:39 - INFO: Epoch 17/5000\n",
      "12:11:40 - INFO: Train Loss: 0.8252, Val Loss: 0.8318\n",
      "12:11:40 - INFO: Epoch 18/5000\n",
      "12:11:40 - INFO: Train Loss: 0.7663, Val Loss: 0.7307\n",
      "12:11:41 - INFO: Models saved to models with val_loss 0.7307\n",
      "12:11:41 - INFO: Epoch 19/5000\n",
      "12:11:41 - INFO: Train Loss: 0.7584, Val Loss: 0.7195\n",
      "12:11:41 - INFO: Models saved to models with val_loss 0.7195\n",
      "12:11:41 - INFO: Epoch 20/5000\n",
      "12:11:42 - INFO: Train Loss: 0.7275, Val Loss: 0.7050\n",
      "12:11:42 - INFO: Models saved to models with val_loss 0.7050\n",
      "12:11:42 - INFO: Epoch 21/5000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 35\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Initialize trainer\u001b[39;00m\n\u001b[0;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     21\u001b[0m     diffusion\u001b[38;5;241m=\u001b[39mdiffusion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     32\u001b[0m     sample_images_every\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m\n\u001b[0;32m     33\u001b[0m )\n\u001b[1;32m---> 35\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 274\u001b[0m, in \u001b[0;36mTrainer.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    270\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    272\u001b[0m current_lr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mparam_groups[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 274\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    275\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_epoch()\n\u001b[0;32m    277\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m\"\u001b[39m: epoch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m    278\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: train_loss,\n\u001b[0;32m    279\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m: val_loss,\n\u001b[0;32m    280\u001b[0m            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m: current_lr})\n",
      "Cell \u001b[1;32mIn[6], line 50\u001b[0m, in \u001b[0;36mTrainer._train_epoch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     47\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, segments, depths, class_vectors, depth_vectors \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader:\n\u001b[1;32m---> 50\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m     class_vectors \u001b[38;5;241m=\u001b[39m class_vectors\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     52\u001b[0m     depth_vectors \u001b[38;5;241m=\u001b[39m depth_vectors\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from core import UNet_Attn\n",
    "\n",
    "epochs = 5000\n",
    "learning_rate = 1e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 18\n",
    "model = UNet_Attn(num_classes=num_classes, device=device)\n",
    "\n",
    "# Initialize diffusion process\n",
    "diffusion = Diffusion(noise_steps=1000, beta_start=0.0001, beta_end=0.002, img_size=64, device=\"cuda\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    run_name='Attention_UNet_Overfit',\n",
    "    project_name='vdl',\n",
    "    save_dir='models',\n",
    "    resolved_names=dataset.resolved_names,\n",
    "    scheduler=scheduler,\n",
    "    sample_images_every=500\n",
    ")\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241m.\u001b[39mtest(test_loader)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "trainer.test(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahrn\\AppData\\Local\\Temp\\ipykernel_23376\\4167768017.py:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ema_model.load_state_dict(torch.load('models/Attention_UNet_Overfit_raft37dt_ema.pth', map_location=device))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from core import UNet_Attn\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = UNet_Attn(num_classes=18).to(device)\n",
    "ema_model = copy.deepcopy(model).eval().requires_grad_(False)  # Create a new EMA model instance\n",
    "\n",
    "ema_model.load_state_dict(torch.load('models/Attention_UNet_Overfit_raft37dt_ema.pth', map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 894-dimensional class vector with some 1s and 0s\n",
    "class_vector = torch.tensor(dataset[0][3]).unsqueeze(0)\n",
    "depth_vector = torch.tensor(dataset[0][4]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 18]), torch.Size([1, 18]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_vector.shape, depth_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:41:02 - INFO: Sampling 1 new images from UNet_Attn...\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999it [00:17, 55.76it/s]\n"
     ]
    }
   ],
   "source": [
    "diffusion = Diffusion(noise_steps=1000, beta_start=0.0001, beta_end=0.002, img_size=64, device=\"cuda\")\n",
    "\n",
    "sampled_images = diffusion.sample(ema_model, 1, torch.tensor(train_dataset[1][3]).unsqueeze(0).to(device), torch.tensor(train_dataset[1][4]).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 1., 1., 1., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 1.,\n",
       "        0.], dtype=float32),\n",
       " array([0.       , 5.6192665, 3.973346 , 2.3689427, 0.       , 2.1447854,\n",
       "        0.       , 0.       , 0.       , 3.567893 , 0.       , 0.       ,\n",
       "        0.       , 0.       , 2.3631027, 0.       , 3.085227 , 0.       ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#plt.imshow(train_dataset[1][0].permute(1,2,0))\n",
    "train_dataset[1][3], train_dataset[1][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11:41:24 - WARNING: Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers). Got range [-1.696281..1.820984].\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxoAAAGKCAYAAACLuTc4AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAABtUklEQVR4nO3de5xlVX3m/2dfqutAlV1ISTfQoRtabmkDCRdFiHjBMEgEg6PRxBsoURMHNL6imfy8BKMTFBISHC8JXqJmJOqA6GhUjEYMKowSVEAIIjQ22jgNVNtFquCcrn35/VF2S7ueL5zTbGiEz/svXbVqnbXXXnuf3ufwfSpr27YVAAAAAHQo39ETAAAAAPDww4MGAAAAgM7xoAEAAACgczxoAAAAAOgcDxoAAAAAOseDBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADrHgwY68Za3vEVZlm3X7374wx9WlmX64Q9/2O2k7uGHP/yhsizThz/84QfsNQAAjyxZluktb3lLZ+N99atfVZZl+upXv9rZmMCOxIPGI9y1116rF73oRVqxYoXGx8e155576oUvfKGuvfbaHT21HWLLTf7CCy/c0VMBgEeEa665Rs997nO1atUq9Xo9rVixQscee6ze9a537eipPWRt+YDu3//933f0VIB7xYPGI9hFF12kQw89VP/6r/+ql770pXrve9+rU089VZdccokOPfRQfepTnxp6rDe96U26++67t2seL37xi3X33Xdr1apV2/X7AIBfTpdddpkOP/xwXXXVVXr5y1+ud7/73fqDP/gD5Xmud77znTt6egDup3JHTwA7xk033aQXv/jFWr16tS699FLttttuW3/2mte8RkcffbRe/OIX6+qrr9bq1avDcebn5zUxMaGyLFWW27ediqJQURTb9bsAgF9ef/mXf6mpqSldccUV2mWXXbb52W233bZjJgWgM3yj8Qj1V3/1V7rrrrv0vve9b5uHDEl6zGMeo/POO0/z8/M6++yzt7ZvqcO47rrr9IIXvECPfvSj9aQnPWmbn93T3XffrVe/+tV6zGMeo0c96lF61rOepfXr1yf/Taur0dh77711wgkn6Otf/7qe8IQnqNfrafXq1frHf/zHbV5j48aNet3rXqeDDjpIk5OTWrp0qY4//nhdddVVHa3Uz4/thhtu0Ite9CJNTU1pt91205vf/Ga1basf/ehH+p3f+R0tXbpUu+++u84555xtfn/z5s368z//cx122GGamprSxMSEjj76aF1yySXJa83MzOjFL36xli5dql122UUnn3yyrrrqKltfcv311+u5z32udt11V/V6PR1++OH6zGc+09lxA8AD7aabbtLjHve45CFDkpYtW7bN///Qhz6kY445RsuWLdP4+LjWrFmjv/u7v0t+b8v7x1e/+lUdfvjh2mmnnXTQQQdtrXu46KKLdNBBB6nX6+mwww7Td77znW1+/5RTTtHk5KTWrl2r4447ThMTE9pzzz311re+VW3b3ucxrV+/Xi972cu0fPlyjY+P63GPe5z+4R/+Ien34x//WCeddJImJia0bNkyvfa1r9VgMLjP8SNb5n3LLbfohBNO0OTkpFasWKH3vOc9khb/E7VjjjlGExMTWrVqlf7pn/5pm98f5f103bp1etaznrXN3L/4xS/a+pJvfvObesYznqGpqSntvPPOespTnqJvfOMb232c+OXCg8Yj1Gc/+1ntvffeOvroo+3Pn/zkJ2vvvffW5z73ueRnv/u7v6u77rpLZ555pl7+8peHr3HKKafoXe96l377t39bZ511lnbaaSc985nPHHqON954o5773Ofq2GOP1TnnnKNHP/rROuWUU7apH1m7dq0+/elP64QTTtDf/M3f6PWvf72uueYaPeUpT9Gtt9469GsN4/nPf76aptE73vEOHXHEEfof/+N/6Nxzz9Wxxx6rFStW6KyzztK+++6r173udbr00ku3/t6dd96pD3zgA3rqU5+qs846S295y1t0++2367jjjtN3v/vdrf2aptGJJ56oj33sYzr55JP1l3/5l/rJT36ik08+OZnLtddeqyc+8Yn6j//4D/3Zn/2ZzjnnHE1MTOikk04a6T95A4AdadWqVbryyiv1ve997z77/t3f/Z1WrVqlN7zhDTrnnHO011576VWvetXWf0jf04033qgXvOAFOvHEE/X2t79dP/3pT3XiiSfq/PPP12tf+1q96EUv0l/8xV/opptu0vOe9zw1TbPN79d1rWc84xlavny5zj77bB122GE644wzdMYZZ9zrHDds2KAnPvGJ+vKXv6zTTjtN73znO7Xvvvvq1FNP1bnnnru13913362nP/3p+uIXv6jTTjtNb3zjG/W1r31Nf/qnfzrcwgXqutbxxx+vvfbaS2effbb23ntvnXbaafrwhz+sZzzjGTr88MN11lln6VGPepRe8pKX6Oabb976u8O+n87Pz+uYY47Rl7/8Zb361a/WG9/4Rl122WX67//9vyfz+cpXvqInP/nJuvPOO3XGGWfozDPP1KZNm3TMMcfoW9/61v06VvySaPGIs2nTplZS+zu/8zv32u9Zz3pWK6m9884727Zt2zPOOKOV1P7+7/9+0nfLz7a48sorW0ntH//xH2/T75RTTmkltWecccbWtg996EOtpPbmm2/e2rZq1apWUnvppZdubbvtttva8fHx9k/+5E+2tvX7/bau621e4+abb27Hx8fbt771rdu0SWo/9KEP3esxX3LJJa2k9oILLkiO7RWveMXWtqqq2l/5lV9psyxr3/GOd2xt/+lPf9rutNNO7cknn7xN38FgsM3r/PSnP22XL1/evuxlL9va9slPfrKV1J577rlb2+q6bo855phk7k9/+tPbgw46qO33+1vbmqZpjzrqqHa//fa712MEgIeKf/mXf2mLomiLomiPPPLI9k//9E/bL37xi+3mzZuTvnfddVfSdtxxx7WrV6/epm3L+8dll122te2LX/xiK6ndaaed2nXr1m1tP++881pJ7SWXXLK17eSTT24ltaeffvrWtqZp2mc+85ntkiVL2ttvv31r+y++n5166qntHnvs0d5xxx3bzOn3fu/32qmpqa3HcO6557aS2v/9v//31j7z8/Ptvvvum8zH2fK+ecUVVyTzPvPMM7e2bXlPyrKs/fjHP761/frrr0/mPuz76TnnnNNKaj/96U9vbbv77rvbAw88cJu5N03T7rfffu1xxx3XNk2zte9dd93V7rPPPu2xxx57r8eIhwe+0XgE+s///E9J0qMe9ah77bfl53feeec27X/4h394n69x8cUXS5Je9apXbdN++umnDz3PNWvWbPONy2677aYDDjhAa9eu3do2Pj6uPF/cxnVda2ZmRpOTkzrggAP07W9/e+jXGsYf/MEfbP3fRVHo8MMPV9u2OvXUU7e277LLLskci6LQkiVLJC1+a7Fx40ZVVaXDDz98mzlefPHFGhsb2+ZbojzP9d/+23/bZh4bN27UV77yFT3vec/Tf/7nf+qOO+7QHXfcoZmZGR133HH6wQ9+oPXr13d67ADwQDj22GN1+eWX61nPepauuuoqnX322TruuOO0YsWK5D8F3Wmnnbb+79nZWd1xxx16ylOeorVr12p2dnabvmvWrNGRRx659f8fccQRkqRjjjlGK1euTNrvec/e4rTTTtv6v7Ms02mnnabNmzfry1/+sj2Wtm31yU9+UieeeKLatt16b77jjjt03HHHaXZ2dus9//Of/7z22GMPPfe5z936+zvvvLNe8YpX3PuCDeGe71Vb3pMmJib0vOc9b2v7AQccoF122WW73k8vvvhirVixQs961rO2tvV6veS/cPjud7+rH/zgB3rBC16gmZmZrWsxPz+vpz/96br00kuTb5Lw8EMx+CPQlgeILQ8ckeiBZJ999rnP11i3bp3yPE/67rvvvkPP855vBls8+tGP1k9/+tOt/79pGr3zne/Ue9/7Xt18882q63rrz6anp4d+re2Zz9TUlHq9nh7zmMck7TMzM9u0feQjH9E555yj66+/XgsLC1vb77k+69at0x577KGdd955m9/9xTW78cYb1bat3vzmN+vNb36znettt92mFStWDH9wALCDPP7xj9dFF12kzZs366qrrtKnPvUp/e3f/q2e+9zn6rvf/a7WrFkjSfrGN76hM844Q5dffrnuuuuubcaYnZ3V1NTU1v/v7teStNdee9n2e76vSIsf8vxiEMr+++8vSeHffLr99tu1adMmve9979P73vc+22dLgfu6deu07777JrWNBxxwgP29YfV6vaTucmpqSr/yK7+SvNbU1NR2vZ+uW7dOj33sY5PxfvG96gc/+IEk2f/8d4vZ2Vk9+tGPHvLo8MuIB41HoKmpKe2xxx66+uqr77Xf1VdfrRUrVmjp0qXbtN/zU6UHUpRE1d6jGO/MM8/Um9/8Zr3sZS/T2972Nu26667K81x//Md/3PknJW4+w8zxox/9qE455RSddNJJev3rX69ly5apKAq9/e1v10033TTyPLYc1+te9zodd9xxts8oD3QA8FCwZMkSPf7xj9fjH/947b///nrpS1+qCy64QGeccYZuuukmPf3pT9eBBx6ov/mbv9Fee+2lJUuW6POf/7z+9m//NrnfR/fmYe7Z22vLHF70oheF/7g++OCD7/fr3Jv7c9xdv59u+Z2/+qu/0m/8xm/YPpOTkyOPi18uPGg8Qp1wwgl6//vfr69//etbk6Pu6Wtf+5p++MMf6pWvfOV2jb9q1So1TaObb75Z++2339b2G2+8cbvn7Fx44YV62tOepg9+8IPbtG/atCn5pmFHufDCC7V69WpddNFF23wC9ItFhatWrdIll1yiu+66a5tvNX5xzbZ8yjY2Nqbf+q3fegBnDgA7xuGHHy5J+slPfiJpMcBkMBjoM5/5zDbfVrj0vi40TaO1a9du/RZDkm644QZJi6lWzm677aZHPepRquv6Pu/Nq1at0ve+9z21bbvN+8L3v//9+z/57TTs++mqVat03XXXJXP/xfeqxz72sZKkpUuX8l71CEaNxiPU61//eu2000565StfmfxnPhs3btQf/uEfauedd9brX//67Rp/yyft733ve7dp7/ovvRZFkXwSdcEFFzykahS2fJJ0z3l+85vf1OWXX75Nv+OOO04LCwt6//vfv7WtaZokUWXZsmV66lOfqvPOO2/rm/A93X777V1OHwAeMJdccon9NuHzn/+8pJ//p0TuPjo7O6sPfehDD9jc3v3ud2/9323b6t3vfrfGxsb09Kc/3fYvikLPec5z9MlPftKmaN3z3vzbv/3buvXWW3XhhRdubdsSOb+jDPt+etxxx2n9+vXb1ND0+/1t3rsk6bDDDtNjH/tY/fVf/7Xm5uaS1+O96pGBbzQeofbbbz995CMf0Qtf+EIddNBBOvXUU7XPPvvohz/8oT74wQ/qjjvu0Mc+9rGtn0iM6rDDDtNznvMcnXvuuZqZmdETn/hE/du//dvWT4R+8b/t3F4nnHCC3vrWt+qlL32pjjrqKF1zzTU6//zz7/WPDD7YTjjhBF100UV69rOfrWc+85m6+eab9fd///das2bNNjffk046SU94whP0J3/yJ7rxxht14IEH6jOf+Yw2btwoads1e8973qMnPelJOuigg/Tyl79cq1ev1oYNG3T55Zfrxz/+cad/RwQAHiinn3667rrrLj372c/WgQceqM2bN+uyyy7TJz7xCe2999566UtfKkn6L//lv2jJkiU68cQT9cpXvlJzc3N6//vfr2XLltkPXO6vXq+niy++WCeffLKOOOIIfeELX9DnPvc5veENb0hqIO7pHe94hy655BIdccQRevnLX641a9Zo48aN+va3v60vf/nLW+/nW/4K+kte8hJdeeWV2mOPPfS//tf/Smr0HkzDvp++8pWv1Lvf/W79/u//vl7zmtdojz320Pnnn69eryfp5+9VeZ7rAx/4gI4//ng97nGP00tf+lKtWLFC69ev1yWXXKKlS5fqs5/97IN+nHhw8aDxCPa7v/u7OvDAA/X2t79968PF9PS0nva0p+kNb3iDfu3Xfu1+jf+P//iP2n333fWxj31Mn/rUp/Rbv/Vb+sQnPqEDDjhg6w3p/nrDG96g+fl5/dM//ZM+8YlP6NBDD9XnPvc5/dmf/Vkn43fhlFNO0f/7f/9P5513nr74xS9qzZo1+uhHP6oLLrhgmz9sVBSFPve5z+k1r3mNPvKRjyjPcz372c/WGWecod/8zd/cZs3WrFmjf//3f9df/MVf6MMf/rBmZma0bNkyHXLIIfrzP//zHXCUADC6v/7rv9YFF1ygz3/+83rf+96nzZs3a+XKlXrVq16lN73pTVv/kN8BBxygCy+8UG9605v0ute9Trvvvrv+6I/+SLvttpte9rKXdT6voih08cUX64/+6I/0+te/Xo961KN0xhln3Of9dfny5frWt76lt771rbrooov03ve+V9PT03rc4x6ns846a2u/nXfeWf/6r/+q008/Xe9617u0884764UvfKGOP/54PeMZz+j8eIYx7Pvp5OSkvvKVr+j000/XO9/5Tk1OTuolL3mJjjrqKD3nOc/Z5r3qqU99qi6//HK97W1v07vf/W7Nzc1p99131xFHHLHd/2k2frlkbRcVUMCQvvvd7+qQQw7RRz/6Ub3whS/c0dP5pfDpT39az372s/X1r39dv/mbv7mjpwMAD2unnHKKLrzwQvuf+yB27rnn6rWvfa1+/OMfk3qIrajRwAPm7rvvTtrOPfdc5XmuJz/5yTtgRg99v7hmdV3rXe96l5YuXapDDz10B80KAICf+8X3qn6/r/POO0/77bcfDxnYBv/pFB4wZ599tq688ko97WlPU1mW+sIXvqAvfOELesUrXpFkmWPR6aefrrvvvltHHnmkBoOBLrroIl122WU688wzH7RYYQAA7s1//a//VStXrtRv/MZvaHZ2Vh/96Ed1/fXX6/zzz9/RU8NDDA8aeMAcddRR+tKXvqS3ve1tmpub08qVK/WWt7xFb3zjG3f01B6yjjnmGJ1zzjn653/+Z/X7fe27775617vetc1fqAUAYEc67rjj9IEPfEDnn3++6rrWmjVr9PGPf1zPf/7zd/TU8BBDjQYAAACAzlGjAQAAAKBzPGgAAAAA6BwPGgAAAAA6N3QxeLb/If4HkxNp2/idwavtmo7bK4K+ddLU9ud931tM+5pft13HxjcmbRMKxi3Hk6Z+8IfmyvH0r3n2KrM2kjSfLnu/mvV9J81ryc+h6qfzVS89XknqFW5ufr5l2U/aNmy4PeibrsOy5XsGfc26l/4vvJaVS6kyxyupN5WOW0XrGxxzNTBrXFV+iDIduy+/PoPqVjNuel1IUtVP90lv4Kcwl54izc18w3c2eyorftV2LbUsbRyYF5PsuasKv2btwNx6zPFKkuaXpm3rgr98PmNKzvb3XR9zaHqOV+7p09BWrNgvaTvk4GNt32Of9jtJ25O0j58E7rc/e8WrbHvebk7aKqXvK5JUZOn7UFS+mJv2LPOf2WWmrQ7mkLvP/XLft27SkfNgDk3mjsNfa9VCeoPZ8leek/Ze+nrZZt+3bdP7wHx1lx83W5K0Le35e71ac97qYH3NMuTlmO3b1Oa+lfk1a1rzb5XKz6Eyy5NnC37cMT+3wu3LhWCvNun61Hl6XUhSY6Y8Vvp/nzVZuj5t8Ll1rnSMIthTVdOkfYNxa3Oeo2sra9MxGrMnJcktb3QNSOl888KvWbVg5rbEn+Nxs+6bK79PSqVj1G06L0nKzb1gofV/K2an8fTfRtm4H/fMd/6Tbd/6uvf6UwAAAADYDjxoAAAAAOgcDxoAAAAAOseDBgAAAIDODf+XwW+52bf3TEHXeFDgbbS9YAoTpkA2qMXV+tvStilTxCppYSqd23w15cc1heoLE764OuulBcFVtYcftkyLbEpf363SFDeVhS9srqZMX+3mBzanvhr4ovg5XZ+0tbNp2+IcjkraZmZ8gZY95iotJpek3kRaRF0GRdT9vikSK4ff6pI0rnRP9eQrsau5dB516de9qsy1UQeFmab4fNOcD1roz5gi88nltu+YuT6jOnd70QVrWZn1aWt/vag0RdezM77v+rVpW1AUr4PTpt3X+HVYtnzvpG35cl8Mfsihv5a0HX300bbvkRR+P6jazIcTNKYUO7fl2VJjiyeDwuY8bW+CP3vra0iDom33WqbYeXHc9AWjP72b2dfzF3xhClnDcc016Aqjo0F6QZhHbhYtqINVawr+8yIozDcFtq0tlJf8OfKFsJkpYM6W+DmMmXVog3+GZcGmslMOCpCbIj0fWetfzy1bHe6p9PWyPPjc2hxzNcKyV65KXVIjs5ZBzXbrCvajviNcW3ZedbBPivQF3d6RpIUFs6eC12szU5AenIrMjLJE/t9cCyYzYDAY/t/228xnu34LAAAAAO4FDxoAAAAAOseDBgAAAIDO8aABAAAAoHPDV8iu9sXV9i85V+Yv+UqS/auNURWqK0wO/oK3q/Wc/ZHvuyEtpF6oor8MbuY7Efw1ycm0ILjv/vK1pP6EKYKb9NXgY+4veJu/1C1J47aw3hcPl2VayVcGhdhzG0zVX98XBU2vMeMGRX+ueFiV32eVTOjAwP8V8f5sOu586fdkJb9P3EqOR3/pXWkhdtXz6+6vDb9PBv10jHbGF+HL/OXc3rg/5tr9ZfmghrMqzByCa0B983qzwV/0nTfHbK5NSbbwu3eQ77pi/zTYYXo6+Gvfq9K/9n3QIb9u+x591ElJ25Ha1/bdvnI5bK+2Doq2s3RT27++Ldlqz/CPAfsBhh3WFnL/7CejvKD59Whc0z5C8XpYhGoOroiK7c3A4V8cz80VZApeJV/cH66iLfgPCq5HOPkuHCD+bdc3KOQO/rqznVtYrZz2bUc65mBct6WCwmY7QjRfUwAfzdfu92BcW/gdVYP7EYbvOsL1HW4zs6eKLAiGcPvP/CX0Re6GFIQnmNO5k/mL8MPgGw0AAAAAneNBAwAAAEDneNAAAAAA0DkeNAAAAAB0jgcNAAAAAJ0bOnVqbLmPpSl6adpSqbRNkiptNI17BDNL027qIEGp0oJp9MlM6qfJQW2QiiQ334F5LUmqZtK2TaZNkuZM2wbfdcFFapW+8n/BpU6NB+vgTr05l5KkdT9I26anbde52XTNyjLK4Un3VFkECWBKE43KINVrfNK0F+ZcKk6dUp2uRdX3r1dXaSxSNZcmUUlSO0hTkTTn56YZs1Gi5ZlKE8P6vdt9Xxee1YvOkdk//THfdd7sy/kggWOdaTNLI0m7H5q27bnnnrbvpLlc9ly5t+178MFPSNoe/4SjbF+XMBVlkI3iCrNkp73mmbbvN9/zuQ5e8eGnDpJQXMKUS0qSRkvGaW2m0PBJPm34+d4IyTYj/L49DJNUI0XBTNG4aecRcq+UB2k3bnmzKMfJ9Y3SrExqT5hoZMN5gjmEiU9uEsP/fhaeT9c+fIJSdBjNCPvPhEMFjcE1EEeDpS3hKXLjRmtmXjCYrxujCSaRuzQ12zOYb9DbpalFZ8cm6eX3P9kuM/9EbPPo38r3jm80AAAAAHSOBw0AAAAAneNBAwAAAEDneNAAAAAA0Lmhi8H3XJEWQ0pSNZ5Wp5a1L5OsqmVmAmmR7+IP0gLZKqhXrd3r9feyfQvNmtfyReaDyoxRpoW/kjReXZ927R/oxzVF7VXpi7b7ps5xMPB9iyo9nbX8fCtTwNwGxfZydd+lL3Tvr3NjmGLyn80uHXc337XcJ22r/D7LJtO9M14EWz1orqu0MtmXnEqqzR6OXq+6LW3b4NIBJE2atume7zu4K23rB8EFbvtEfV0GxFzQ122JoMBbK9KitMeuSe8PkjS1PG2fMsXvkrRiZbruaw714x5ySHpPO1KH2b6jFH670v5XvvNdtu/F//wP5sVuHOHVUC+4CmbZj9Gi4ktXBZ0Fxcq5KzgNCpBt0autuPaFyVER9P0rBw4GkD+MqHA3a9L1aaJjG6m42r2Y7+oqm6NicFe8Hsnc5gl+vQmKiv24wwcJVLV/wej4fN+hX86Ki/vNXg1PpynwjtZylMJ6N27UszEvGBaOj1CQPsK5cINE4RQ2KCEq4nf3rnspSR+WK6AvFnwo1H3hGw0AAAAAneNBAwAAAEDneNAAAAAA0DkeNAAAAAB0jgcNAAAAAJ0bOnVqenmQJFWYP0leR8O6MXyUVGnialxqlSTVVZoHVI37+Y73gugqO66ZQ8+nLY1Xv25+31fo92SSlaogTchGBEVJUuZcBClZlRmjGri8HGnQT9esVJo2tjiHNNVrbt6nFNXm0Kr5223ftjJpVn1zvJLaDelx9KO0hWirmuMIs4d6Zi1KkwIlSTeYtYjirFabPTHwxyyZvhPBwKVpn3DHK9nT7C8BmzCVHey7rpxO13Jq2qepTU6nqV7L9/T3gv0PSJPeDl6TXpuStCY7KmmL0qXclfzGS0xilKSPvOfcpG1u9mbbt2f2Xz/ON4MTJL/Y1jAVySXYBNE4ZuCs9Z/ZZS6RKJqvSXlpo2QcM26URuTSnUZJqAqmYNOoorSbzAwS5d+MkszkmqNx8yx9H4tSf9o2veLrYODa9I3WzKZ6hZFGwdxs4pPff3aI4OWa+xlHFV4v7teDKDN/zfp5ufnGyVfDtUU/ycN0qXTdwz01SvKVe6VREq6Co7PXVpQq545jbOhHhm3wjQYAAACAzvGgAQAAAKBzPGgAAAAA6BwPGgAAAAA6N3Rlx9S0L74sq7QytCpNVWg0AVdMLqk0ZZlV6adbmXrnuvJF3+PaM/193Wr71q5gutzZj1uZIuYqLWKVpEFlSk6Dom1XC+4KuRfb09erXKGyJFWmfbCr7Vr30/NZ6Dbbd778QdI2FRTFq7+XmYMvhK1MVfKsW3NJMuvbH8zZrq4gXZIW+qY4ejYomHZjRPW8w2cRSOvNwINgwgrm5oxSz+VeboXvOrZ/2rZs2leaTU+k95Ppab//li/fI2lbfcBv2L5rfjUt/N6/d5jt6wIn3rfuC7bv37z1DUnbuhu+68edHEvaBqU/NpnrqArDNOCEn5a5Au+oAtTVbAcD+x0dFHXa6t+gaNYVg/ueI8xA0ggF6W6UcFxbQB8Vgw85r+j1ouLqfPg5uML6qNjeFlfX/qaemSLoaJ/ZQ472Q7Sz3RhZtKfSzs3wpz48966wOSoGz1zBdNh3hCAA094Ee8oPMXxxdRtes+bfNiN8fJ8Fx9a4YnlXTC6/ZlEogw1wCNMTTLH9KDekew61fb8GAAAAADEeNAAAAAB0jgcNAAAAAJ3jQQMAAABA53jQAAAAANC5oeNNpvdMk18WB0jbS5MQtMikLQWpSKXSVJpKPrnFJRLZKCrJJkGVlU++cklQVen79qqNSVu/8ikVA5N2o9qsjSS5VC/9JOhr1sylS0mqzDFX9Z22b22ScfruXEpSdaD5fT/uuEs0mvdpVrPaJ2mbDlLIyrn02PrBHIJTpHpgjm/OJztVg3TsWS3YvgsusSm6XNxWC7a1XKhWFETl2qM5rEybJk26lCRNmW09OR6k1U2n1+GUSZeSpNUr03Sy/fc/1vZd/pgDkrZv3/Et2/e9//PVSdvXLv+k7dtWadxGmi31M5tMMl1vg+06MFt4oe/3Drw881FumUwiTBAH1Jj0lyhhxaUatUF6TBQG5Ad2ST5BMo4LkoqSr+xLBUmA9qPHIMHGpnoFyTgjpG+5caPgodxMOM+Hn2/EpVFFST6ZiXFqglSlpk7bowCwKJkpd/OIxhghxKkx6xPnMg1/jhp7noORzQUTXVvDTisaI7xm7QUTvJ7pnAdxdX5PRZ/1u2srSkgbtjHYU0Ffu99HuqH9HN9oAAAAAOgcDxoAAAAAOseDBgAAAIDO8aABAAAAoHNDF4OvWJEW+UpSzxRHl1EBsivILX2BdzluqnRrU20qSYUp2pav8h031bT1wFUlS654ve8KDCVNVGkh66C83fbtm6L2qu/n60+Qn4M9jDotpJWkvnm9oC5afXM+54PO/YE7n3vavmX/1rRt0heZl5U7Dr8fyvG0MH8uWLM6OA6bJTDugwCqQVqMWgbF4HPuHE31/Lhl2nkQFIO3btmiq9stWzBu5orBl/u+k1NpefTUlC/UnV6eTm7FSn/ul6/+tbTRhDpI0js/8O6k7f/8nzfZvpWpz7brGIiuF1dtXwW3mNbdKuOBYbSZX7DMVcJGNagjtLr2PKgcd63RqKMUpLuSzNZW/sZF4ravKWyOq+JHKNJ1vx6ejLQ9i+Zgfz9oNh+rNlVQkG6HDYIEXDFtEwQUmDkEtevxWRtpKUyhelSwbwum7z9bGx3MoRnlmrVFzFGBt9lT0dGZcaPCcVf4nUXhFHn6b5AoCMBdW9EcWjsHP7ALT8iCAm8bDhBO+N7xjQYAAACAzvGgAQAAAKBzPGgAAAAA6BwPGgAAAAA6x4MGAAAAgM6NkDq1LBggTeIpiig2JU0DKrVPMDMTx1JG45rDCFJpytLE65Q/CsZ1CUo+IahUGitTVT6pS2Ua+1NFSV1VumZ11Hc+bY/SbuZd3FLlU5z6VXqO5/vTvu/AxPb0Z/wkTNdq3ic29JTuv36ZJn0tDvKT9KX6PlZpEK1lna57NeUjifrz6VqUfb/w41PmHFXBnhqk6zZf+ku2Mslrfc3Zvk7fL4OWm9M8NeVjp6an9k5/f4Vfsz1XpklkU1P+fF5xxZVJ25e+fK7tu/7716SNPnDMpzsF6yCzLduBT+BY6Jt0kCDVy56iaA6w6uj82qSiMOYl7RkkHbnklShBybcG45recbCTS6Xx6TFuFpmNAvLJNkFAkO0bh+gMn2jkxsiCaCZ3HG4dJb8dmmDN/LmI0reiDej6mqYRg3z8PKKdlh5fG6VOjTKPUVLAzAFGCUojJV+ZH0Tn3n+kHlwDZn3CPWWGGCXNKrpic3eOwgQ6k2YVzdect3h90zEWqu3LIeMbDQAAAACd40EDAAAAQOd40AAAAADQOR40AAAAAHRu6GLw5cv2su2FKQYve1HRthvDF1L1bKFvUHQ1nlZqlkFNUGEOuTSFxlF7tGCVreCMKkDTQvXKVqZKle5M2urWj+uKweeCKt95U7Tdr262fWtTWD8XHNqgb86RKZaWJFVPSMcNirYnTdF2fxAVr5tzXM36vn1X8C/V2s283l22b1Wl586cip+9nplH5ccdmC1RBgO79qDGXK5OvfZLqenlk0nb1LQv2l4+lRZ4L5/2xeC1Kfr/5D9/0va99joT1lAt2L72FhHdjoKghKHNB4VxoxR4R7cIDC8qYjXFl1lQ0BuVpg7bHI1rK2yD6t+RCmFHYQfxZaiua1wEnbY3wYx9+bL/nNMV20d93Ryi+dbN8MX2rjn6VLZt0rWs6mBHmaL23MxrVFGBty26Hj4PYbSi76BvHGhg+rq5NcG4NjUgGth0Darf7V7Lo+vbhBEEQQuZK8z3o4504dvC72A/NObYontX05g30+Bc3Be+0QAAAADQOR40AAAAAHSOBw0AAAAAneNBAwAAAEDneNAAAAAA0LmhU6emp9MUnmiI8aDy3yUrRRMolcbgBCE6ls+6idp98pCbWzwHk74V9HQhOHEwTroO88H69tOAIM1PBqlT2mhey8+iX6VzmHDJOpL68+m46t/m51ClyUNFkCRVm9SoKkiM6tfpuKrS8yNJg01R2leauNAf+LWsTHTQVLA+Lu1L/Vt9X5dONu93cDV3VTrswO/AXmUmFyQwTS0/MGmbDpKkxsfTdbj629+zfa+74sakbdNskGjh5hZdXMG6Wy7xaZSEKrPV8eArRkm1CWNpRhjDJh35vjaIZ5SUrCh+xqboDJ/iFCUEueY2SOexqVMmgelnnU1bkJRkjiNMVXKHFiTjZHl6T2+ivma+0ZHVZh3yKCRrpByxaN1dUzQ7k7Q1yhzC7Tf8NWCTwaKcNxegNELf+OJyPYO0JbPX2iApTlm6p9rg3LvEMZsYFfSNkqTsGME12zbpG1wdbB237gtVkPx6H/hGAwAAAEDneNAAAAAA0DkeNAAAAAB0jgcNAAAAAJ0buhh8IvNFuqMUaDumbDccN5qsK02N5jVK32huO5ovSfbtc0H5+7wpXp8PVniuTNsHu/iq2bmp9PWqeg/bd7KfFnj3532xUVWlhdF15ap5pX7fnNF6Lz/uhB/DrWU1iIrl06rgespXCk/Om13V9zvNjTs75eer+fT4+n2/U+o6PY6q8n2nzGU/N5OeN0m6/Lp/S9ruWG+7Bgsc9HWF2FFNmhsjKhB3SxldXHjIioqVc1tZGlVtu6agUHOEglVXTRuVq7bucz9XFKqgaDsY1y+P7+0K3W3FdThCMF9XPGx7Sq2pTm1y3zsvzL0zXODhi7b95Iav+I8K890cbLH+vfB7bfiC/ZFea5TfH6FreMS2MDk6NlNcHV9cqeDasqEMw1/eahr/5pTnY0O9VjRudC5a82ZoatQX+7prKzg4dyqa8I333vGNBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADrHgwYAAACAzg2dOuUzp0YbdJSEKpeVNMq4PmvpoZskNYro2Fz7dNB31qzmnHw61MAkVEVBPrNZGttTlT6BaTCZjtsPDq4yZ24w8ONW/Z3TtoEfuD/lUxSm+ml8UZD3pL4Zu5rb1fYdTP4w/f3K7+wJ0z7d/4ntOz+fruXsXLA+A5dQ5ZOkbrjhyqTtpm8HERwzpm2UdKgodWqUvi41KjpxD6b09CxanjaN7f+AzuRhpwiig2zwT5SgNFI6j0mlCT+yS/uGr9W6nJcoRmf4xB3X3IwwbmPnJbWuvfF93UeaYVKXTfXyC+zWMgoTcqLp+jihIPWnMWsWJPm4JLM4TWj4ZLAmzABzx2G72vQ2e44l5WaRw/Ss4ZfSHnN0uRR2vsNfBHl0DZhDjs6FG6Eo/Pt5UZp/fQbH1piNGSWZuTmE4XpmCnm0r82bd1uNlpC29TW267cAAAAA4F7woAEAAACgczxoAAAAAOgcDxoAAAAAOjd0Mfhk0D5KcbV7sahA3PWN6imj4mjE58eVKkfrWJnevnRY6mlp0tYPRq5Mle68q1aSVCktYK56P7J9654pSK/8TutXff96/XRug9IXYlf9Zem44363Dqp0LfqKirbTVa77ftxy/s6krTfl+87NpvMtZ3119cqVv560TZVX2b63rE2Lyu5Yb7v6NAF/KnyBd9Q3Kj5/IEQXjCnm3n1/fwc9aM2BSdvy1en5QSwLi6uHL5i2ReJh3aOtVvbDRkPYvum4eTBhO25QgBwWyLoh3DKEa2aKkqNaXDO1sBjczTd4I3NFyVE1eG7GrbVg+9Z1eiOJlsG+3AiF0W103qKKafdy0ezMOYpGtYXfQee6dj8ICqbNWkQBA/b3ox+46z5aBlNcHb1V2IL0aA4jFK+7c1Q3wSzsIMPfC+JrK12HKBgia8xFV45yR/s5vtEAAAAA0DkeNAAAAAB0jgcNAAAAAJ3jQQMAAABA53jQAAAAANC5oVOnpkYYwOfX+L7RBFxeztCTxXaJUr1ce5TY0DNnqW8zrnxwUGmSqCTZ1irYEZWJKapKF10klaU/6qqX7uJJpclOktTv7Zy09cpobiZ1qvLjDgZp37lZP+54L03Vmp/z6z5ujrk3How7dWvSNr1LmpQkSctXpm2zM7fZvhtunUna1n/fdlXfDbHJ9w0CvIYX3ejMse2+xieZrTw4TUhbuezXbN9ddknP8fjUKFl+iBNWTNMIoSlZlNaUjzCISw4KEoky97FflGBjkopGObY8SrDJ0lSaKPXHBw+FEzZ9h+8aacwkMvlEo2aEDWETmIL3m7ywkVp+XDsH3zcM+zK/EPc144YBVS5ybIQ4tQfo3EfH5pKrovQtd21FiU8+zCraJ8OfzwWb6Obn25i5Zbl/X3CpZ03tr4HWtNfBmrn1ravh08LuiW80AAAAAHSOBw0AAAAAneNBAwAAAEDneNAAAAAA0Lmh66vTksXR+7oXo+zxl9PSoN2VXEebzJfS+uLsgdKK4Cqo3K1M6fh8UGRehjtwmRnX72xXXD2Y8gXeLihhvPYrNN9PK5uLMi08l6R67q6krSxvsX3LXjpuOeHXYXw+XYf5cb8O9fhP0sapYM2Wp+3LV/qC/bmZtHD81vW2q+64wjTO+r5akzY9Zn/fdc89J5O26V2ebPtOr0zXcpegwHuilxbsl9Oj3G1RaMy2t64oOPhozRZ1hlzx5PAVvVFhqZtcWLg75KwkKXPVopE2nUOe+wLQzPRtm6CvmUIenQzTOTw9rjA/6NyY/dC0UayJe62gwNYWRg8/bHziwqrtIScRTGSE4vMmGDezFdP+HueCC6Ii6Ny0Z7nfJ65gPww5MG1lcF00JsChDfaJO458zM83N+sQlVZnttA9+LeKmUNR+DlU5vrMg1nU5pgHNgHivvGNBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADrHgwYAAACAzg2dOjV0R0W5QXgkcHk5Pu/JJ47tEvSdMwlTVbjT9khaeurbnn35NInK9K+Dq6AyY/SCBI6+TMpQcHGVk7ulbcF8+0qTpIJYL2k8fcGi5/Kw/NzK8nbf15zp/iBNrZKk+TJd36rnd8r09I+SthUr0zZJWr8iTcW4ZYPtqunlaZJUb3l6fiRpcnk6t964X7PeRLr/xif9SZ4eN+tTksU3iqYM0oBccxDzYlOcojQgk7gzQq5TGBBkQmnUuEb5lKxovlk7yuxcPJTfjy7wKUweco0mtUqSmuAeZ+dgXi+Lrp86HbexByHlNtnJD9uOkuzkUop8Vz/u4ihDtvnBG7MOiy9oEp+i1KkivZ9lwRzGTApTveDvnW4T51mwT0xzFq1mY14vWLLCpFG1wbi5ScQaG1viB65NqlyQZtWY9Y0zqsw1EKxZYeZbm5Stxbml61COdC/5Ob7RAAAAANA5HjQAAAAAdI4HDQAAAACd40EDAAAAQOfudzE4ZYu4L1HJ9iihAaXpHZSTac627jPSHAauuDpQmZn0g5F7pu9c8FrjpgC+nPRHPd6bTdrmN+1s+xbaK20MFrM0BeXjZTovSSqrfdO28jbbt1emRXBz5bwf1xRMVxMH2r79Ol3L+d4P/Bwm0nHHJ3wF/cT40rStl7ZJ0uRkWlA+OeFiEqTBZHoHnSiiKn44WRu8C2Xppm6jwkfXZopCF/uaYtGowtuO69tbU73uiod/NgkzQDRu+oM2GNc1Z0ENqhuhCtahMJ9p5sEcGlMfmxfBopkxmvAcm2Ln4Nhqcxx59LmsKdiPCqPtSYrOcVT7a+aRBUXtLhEh2lOuKDgqKrZF+NExmwLiOigqLkzwQFTcn7s66uCazYsx09UfW9uka9YE87VF4sH6tu4cBXvVzi26H7m5ReetMJvKFIhLUmvO58KS4YMatnmJ7fotAAAAALgXPGgAAAAA6BwPGgAAAAA6x4MGAAAAgM7xoAEAAACgc0OnTpEuhQfDKHk7/Q5eL0qu6ilNDor6uvaeBrZvX2myUqEgkUgu1egu27dXmiSp6Z/YvppPk5nKMpjDXDrf/sD31XTaVPZ9+lZ/YNZnLhjXnIvSrKMkDUwoxm19nzg2MZ2mTk2Vfr6TJmmrLH3qVGXGqMrgVluk7VXZxc5+5DChP4tcSssIYUBRKo3rPELolJoR0nnaIHqoqdP2oghSaUyqTBZM2CVUZSZhKJIHSUmNSdyJzltuzptL5JKkxs43SmAyv+8irhQkVAXjupm1YQSYaaqjlKzgHJnT3AapSK5zdD6L0qQtVcNHjkXJYLnZf3kdrLsZoghSkSqzbmFKm32xIPnKpFxlbXAvaFyamr/Xt+biaIMkKf9a/lxkxfDXZ9OM8C/5PP2XTfQ2dp9Dbd+vAQAAAECMBw0AAAAAneNBAwAAAEDneNAAAAAA0LntLO0AHhijhA5EpcOuoDwqrx3lAoiKwX1fX1RcmJmU2tf29YXxP7KtfVMwPZn5gnRNpk3zVXB0lTkjQUVYVdxquqaF55Kknjm6cX/2K1M32Ov7sz81nq77eLBRSs2aOfhVdwXeE4UvBi/76brfsO6Htu+6W9M1u+Gm223fN3/wz2z7I91YUKS7YGono5rZzBYgD18w7YqSJV/QG9Yqmx+0QQFoXZvrNSzadsfme7pC7Ea+cNfVKrtCWknKyvQzzaL1fRcWzPpGc2jSvmPlmO3qinHr2m+I1lU7hwXXpsg3KOR25z43gRCS1AQF004dFLW7vdpmfk8VZnJNUKw8lqfnLi/8uofF+Ubtis+DY6ub9BoYy/1aFmPp3KJry33+ngdr1pgABncNSVLjCsqDpbEjBMX2tqh9lHSK6Noyw9bm2hwG32gAAAAA6BwPGgAAAAA6x4MGAAAAgM7xoAEAAACgczxoAAAAAOgcqVN42HFZJj5LaLQkKZ8j5QV5T1apZcFP0sSmSlO2pzuOMsjl6pn2uhekQ5mVqwY+paJXpclXJoApnlsd3I76LjPMz7fsmTGClKzZ+XSMmXm/Uybm90jarl5/m+277mvXpo3/9z9s35F88P4P8XBUudQVSY2JdKnqKB0qTZXJg3FdokuUYJPZpBg/h9x87tcEc7ApWT6rRnmR3hGj+dqUoiD1x6bdBHMwy6uq9eM2Lv2oDuZbuGgxPwWXBJUFOYcuOCgKnWpMclWUWOaWp4j2b/AxcG3SqJpoLU0qV575gVszjzw6DtNsk9AkNSaxaWHzZj+u2SjNWJAOZebQBHNwh1EH18CSsSVJWxacI3dtZaZNktxWrYPz5tLUov1n55X7c5zl6bhFdC8wmzVrRpjEPeezXb8FAAAAAPeCBw0AAAAAneNBAwAAAEDneNAAAAAA0DmKwfGI4Muz4vb7KyocHzfF1b5k2xeUD+QKo6VKaVFZVOju2otgwm4OZTVv+9ZlWqheBWXxfTOLqK875Plg1dabgvRvX3GpH/cTM77dumaEvngwZabYVJKaBVM0Ww1f2ByUoMrV0kaF2LaIORg4L0wBaFDZ7Ip8bd25fPFvVOMrex/xx1aYimkzLUnS5sFC2rcJioddQXqwaJlZ3yKoms1cgayr+pY/n20w39wV5gdzaM0C2XlJWpKP2fY6T89Rnvl/ytXt8PvPFZRXpvB8sT29V5tpSfJF5u4cS1Jh1sK1SVJhljioa1ZrjiMPrq3W7KmoMN+FKkT3DXN5h9eLveSi+0aZTi4KsmizdB0W2qBwPEv/dTS20/Z9N8E3GgAAAAA6x4MGAAAAgM7xoAEAAACgczxoAAAAAOgcDxoAAAAAOkfqFPAgGjd5VFFClZQmKI0HyUzuQi7lU5VK3Zm2ZX4WdemSVpbavjKJWtJevms/PY5BdbPtOjdI+87M+Dl86Q8/GMwND1eFS9aRTwMKwlikLI1/idJjcpOYE6XzuKm5BBxJyor0Ko6m25j5hp8bLklHKZZEnzGavkE2n0tQqqsgSaowqT9BmpBL6mqCZByb9hUlSZlUpc2Dzb6viTRy5z2aQ7h3TAJYVvh0qbL0x7HEpSUFKVeVScrK7d6RGrNZ6ypIdDPn3u0HScqLdP+ULrpNUmb6ujWTJLd9wv1n1iwLrhd3j6ij9C2TYhetQ5q7Jsml0knKRkiSkjm2Nri2Bma/h/cus4trf7ncJ77RAAAAANA5HjQAAAAAdI4HDQAAAACd40EDAAAAQOcoBgeM75i2FUFfV37WD/q6Um5fvibNm5/MBb0Htj0t+t4y8i8qg5L0QeVmPG37VmbcOphvVc8mbf05X+g+s2kiabt13bdtXzzyVEHxb24+R8tr37dpXEFk9DmcGdcUD0tSm5nCZlfMGwimq8wUkQ4WbLmpMnM3WjK+xPYdM4ec5b5gtTXF0a4AX5LKMr2/FGO+yFymoHeh9sfWmgLZJUv8P2taM8aCr4NVYwpk66AY3BaqB33dMbsCaEnKgygAV3we7b+x3IydB/vPLcZYcBzu+KJt7cITsuCYC3NtBcPaqIQimK8LZQgmnJk5jAWF+W67B3XYqk2ReGPuD5IPRFDu93WWmaCF4L5Rlu6Yg+J1M4c2OG/3hW80AAAAAHSOBw0AAAAAneNBAwAAAEDneNAAAAAA0DkeNAAAAAB0jtQpwHBZSb2gr0uYii4sF3ISpU5VdpQ9bN9Nuj5p69mMq+C1Wh+/UrnUqTKYcbkx/X3NBC9o2tLQqkV12rk/iDrjkWZJ65NQNmtz2hiksbjglbbxiTALSttdwtXisC7RJUoTSvtmJqkmHtcnM/UH6Xwzm7IlZeNjSVsexeiYw4jWrDHHEYRZqc3MOmR+Dq65dok9kr35BqE/at39KUhrals3iJ9v4dKLKj+JME3NJG010fqYeWRRIpa5jIIwK3s+6sa/L7TmfGTBu17uroHg2NxR5EEqUpZua5lttmWQtG/0mbxJqCrKoG+VbsDG7h3JNdfmfVCSmiadby9IqCqL9F8x0blom/R+0pZmIYfANxoAAAAAOseDBgAAAIDO8aABAAAAoHM8aAAAAADoHMXggOEujKhoe9jfH3UM//t+5BvW35i0TU/4vtOTuyZtgyot5F58PVPqHlav75w0lUHn0gwbFdv3irQgPapHxyPPQh0UV7emYHVzsHFMJWyWBwXTpvgyKupsTOF4E8xXpq8veZUaU9AbjSpToL1Q+8LxwswtL3wBqCtK1pi/5xTmMFyBuCS1puh1ofbnomhN4W5QEJy7cxEUg+emCjouik/bW7NHJCk3fXM/XWVufeX3Whn0zeyuiCq8zXEEheOZqaQugnVvzLIVQYBDYdY4qtlWbX4SHFrufhBeMOkPgsvbFmi3mb8GqoU0nGLBtEm+AN5sdUlSaQrSq8rf5+qF9DqqgyAYO6+x4fveE99oAAAAAOgcDxoAAAAAOseDBgAAAIDO8aABAAAAoHM8aAAAAADoHKlTwJC+FrSvM22TQd8p0xalLZlgJt0yl6ZLSdL57/m7pO3gg/exfU94xnOStrIc93PopzOu6jQFSpKqKr2dFPIpFf3JW5O2ufkg+apMx+2XQVQLHnHanfznZZtn0703f9fdtm++JI10KcM4INMWpNK0JtYoSqiysTJ5kPrjUpyCWBoToBSmLbmwmrElQTqU+ZzSHa8ktSbNqm19Mk5jEpsKBSlFJnGnyH1KljtJZZCiU5vjKIM0IbfsVZSo5RKNgmSnaPsVZgy3H6RorwUn3xxHnLaU/sAdmyQtNOY8B+vj0pZsupnkrw2TNLc4N9c3OEemvXYJV5IWqjS9rQwWrV4w6xCcN7fVSrM2kjTeM+/dQThUZX7QBsmQLhVuYZ7UKQAAAAAPETxoAAAAAOgcDxoAAAAAOseDBgAAAIDOUQwOGK7c+aTpoCjN1zA/ICZf/T9s+5r9lyZt1639ku37+Nm9krYVyw+1fXtlOu5cfbvtW5amqCy4w7ja896ULzKf76cFaGV/3g+MR5xHjy+x7bePpYXfeeErbJeYatqgVlS1qeCMinHlClmDom1beRv0tXXjQWVpYw4kKtreXKXtC3PBRWyKubOgIDjL0zGyNijENocRnDaN5+mNJC/8sRWmyDyo8bVr1gQfy+a5OeYsKIo3hdFZULxux5VUm9tsnfvXsyMEx5GZTRXUH9vi6MoURkt+r9nibPlAg2hPtaZYOboM/QDBHFyAgyl+j9TBvpY5n3mwwK1Z3za4BpqF9PVcSIIkZaVZy0Fwks1ebRuKwQEAAAA8RPCgAQAAAKBzPGgAAAAA6BwPGgAAAAA6x4MGAAAAgM6ROgUYNuDh2KDzJx7AifyCufX+xWZW7pq0zfdnbd/Z+X7StrzyiU+VTOKTTOyJJGki/f3Cp0MNymWmLRi2TMeobC4YHomqOZ9Ks3lzuncb+fSYynzmlkcfw5m0mzqKqDI5OK1Ll5JPCHLJOpKUmQSbLIq+MslXQaCRstwlavk0IbdAeemjcUpzR22jhCqTfNUG522wkM5tc+XvT5lZh2qw2fZtTGpUlH6kLD22ykVDLQ6cNG0ug5Ss4JgbsyeKKJbLJCC1QZKZS1sy2+Fnw5r0oWhcl6AU5EO15j3AJYBJ8XXkZG26VxuTqrQ4rpmDO15JtTnPWR3sE3M/iUKc3PpG69CYfbkk6FuZa6upopSstKnobd8jA99oAAAAAOgcDxoAAAAAOseDBgAAAIDO8aABAAAAoHMPajG4KwvtBX2D0ibgQeEujNec9S+279pXp4XJE0GxstvvpXzRdn8+HWPDjO97zdX/lLTNDTb6cefStioqCCvvNG3+6qyLPfwYxnh9W9JW9P3taKq8a7h54RFpJtjnC5vT0IPWhBtIUtOkn7lFxdW1qUHNouJWW5QZFEHbCu2ocNz0DarXC9vXdrXNWVBoXLhicD/saEwhdh0VeJu2NliH3BSDR//QKNw7QFDj6wr2XfGxJFWmADkbpPtUkppmzL+gKShfEkzOzaI16xsxddw/GzhduLKI9nU6SOUuIkkyBenhbN215c6x/LXcBMXkrZlDUOduVz0MkTC9awVBC/Y4gntMnY6xYO5nklQvmHCA4J7o5AvbF8LCNxoAAAAAOseDBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADr3gKRO/UfQfoUJjhgPUh/2N0X3h2z3jIDRzJvUqHce8F985+0LYujWr6ZNk0f6qIxqkOa/lT2XCSeV5a7p7/ejTLifpH2DRC0VaWpUWfrb0eT43knb8nGfcLVOa4O54eGqDtJjXPxL2fg95kKC2jAdyrQF6Tx2akFfGzRjk6j8GFkQjVOWLlHLf8bYuMSdaL7uc8ogcqdeSJNx2iAhyL1em/l7TusWuPF9s7F0bm3tE3fcuGUwB5tmFSQ7tSZDqa59opZPLJMaEwWVB4lPhUkIjEKn3NzsOZZPSIvm664ju2aLL2gGiOZgDiS4BlpzwUSfslfmYo5SxOxrBddLY34Qra8bwoR3LfY1a1mHcWGpKFUuq9L1LcaWDD3uPfGNBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADr3gBSDX7zJt59zxj8kbSvGJ2zfb579/A5nBIzmg5d9JG1cGXT+gWkbD/q6ur/oKnRF5mlt9qIVpq0XFIRNpC84Pr7Udi3liq6j6ve7zAD+4Mo6PZBSG23fQS8tKF958GG277cO+kbaeI3tioeJpvCflxVFuveKwhf/mrrH8FM4V8daB0WorvA2KpoNXm3oOURV5o0ZY9wUCUtSlo+lozZRwXT6eq0pJpekepQic1PkG61ZYQq026AwOjN9gzpYZbUpYA46t6aYO1qzzJy4PCjMz4LjGLPF51GlcNruipKjMYooucAV/Td+vi7PIDpHrqi9DQqbm8a0B4Xu7mJuooJ0dy1H62vGiKbgLmW3HyR/HUX3GFckXgZ7NY1kkFq3jpJaM0QVVaTfB77RAAAAANA5HjQAAAAAdI4HDQAAAACd40EDAAAAQOd40AAAAADQuQckdWrTDTfb9vX/89Skrfectz8QUwCGcot+Yts/fMYr08aZEQaOgpkcH1AiuUC25UHfubRpcmLKdp2cSCOxepnvW5oDqUwKlCSVVZq+Ug78Laay8VvBopXzSVNvwqdqHP/ffi9pu/rbH7d917/Pvxx+uew85iPeqnpz0hYltzRtehFmQeKOD3zyn9k1JkEpj5KkTOJO+ElgEJjju6adx8b8dVkUS9Lfb32OzkKdrlkb3MyKope0uTWXpKY2a2bWUZJ64zunjVE6j4k/qoNzXJkkqSiBqTYnKR+LUsjS9jJI5sujNLUlaf8oESszx5wFKUNuhZvgmAuz31sXsaYgCy04Rz7xKUqzMolaQYqYm0QUOpWZqy5KACtNc2GS2xal+72J7kdFuv+ivmNj6TUb3TcaEzvVLLgsKqkx1327EP1j5d7xjQYAAACAzvGgAQAAAKBzPGgAAAAA6BwPGgAAAAA6x4MGAAAAgM49IKlTRz9hH9v+7Hd8JGk76UUnPBBTAIayUnvY9jPPenbSdsaffMr2XdhgGl2okiSlAUrxVehSp9LwlkW7pE17TvvrcKKXJkzFNwKTBFXc6buW7qB9IpB0m/l9n2Y1Pj6dtFU9HwHWm0oXbf8j03MpSdMrrkrarr5wre2ra3wzdrzB3X3b3tpUI//ZWt6msTStzeGRstyk0kQJNq59hL6tmZck5aazCeH52cuZYzPpR5KUF6ZvkH6Um9drg/Vts/RcbN7sb5Iu5CrPgmSmMo0ZcslOkpTlaV+/R6S8Stvz0qcJNbVbeH9shZlvUfhji85nbVKjCnPeFjubZCbf027A1iSASdKCTSILRnbXVjBdlzqVBR+Ht2aBsiCZqTXXQBPsE7fwbZDU5eZbmvuDJDXm2siDRLcmT9833X1HkgpzbeQuDmtxlKRlIdhoVZXOrQ7uBfeFbzQAAAAAdI4HDQAAAACd40EDAAAAQOd40AAAAADQuftdDG5KRdX7lu/75he9JG1c7vv+x4Z05F9dHhWWAt069dC/T9o2/PmNtu+XP3190jY3t2D7zs+n1dyzpS9mtYIi8+mptFDx4EP3tX1XrUgL4CttDF4wveYGtZ9EpbRIvAomXBZLk7ZeUOjer9J7QdlzlfLSlLmlFWVw31i+a9J06O+l85Kk76z6btLW/rMfFg+uLKgsdfWbZZEW40pSZoppq6i42tRZuoLrn/1gqN+XpNwUltZBga2rIY0KxwtTTZtFk7CV6kHhuPmY0hWQSlJj5rakDC740h2HP7aFzel9diEoMs/LdMJRsbObb9H4cRvTXm/2ReZjOy1J2rKgwNaNK0mt2SdN68dwdb5ZHhRMm6WItknr9kRUL20GLkxhviS15gUbv5RqzLURHZu7PosiKgZP30vr4D3PBQG4Y5Ci4AG/aLUbN9ir+Xi6llHtejGWzi3PgzACM7exqID+PvCNBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADrHgwYAAACAzt3v1CmX5/KkJ9zfUSWRMIUdaFrLkrY/f9rXbN9XPu1HSducSWCSpFmT0zarWdu30nzSNj9nu6qaTxMtpqb9NTRVpqlTtXktSXJ5WGXhY0BKczcYHw9isqqptKkfjGvuUhPjPh2qKNP0rKryc+iZ5RlM72X7HvLU9Lzdsud/2L53fNo03ma7ogtBak/ZmJShII3FhSX5jBefwRSlvAw9gKTWRARF87XZUCZdSpLysTSVpjQJTJLkwouCIB+1JnYqDzo35hoMpmuPOUrUWlgwqVN1kPjkEsdsEpDUmJNfFGkakSTlJkGpHIs2RHozq6rNtmd0zJlZ98ZNWFJhdnFdB/vEnJBgWDUmAakJEgZbcz7aMT+HwuzspvUJjo1J3yqCFCfTVVnwObtLrirkU7JK8+ZUZv6f1W0whh3XpENFSXE2aSu4Ibl0Mrc2klSY48h6wx/DPfGNBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADp3v4vBgYcjV/K0q9ICZkmaMu2+tFpytdxBubQvBp/0I1eT6aU80I3BHNISb18SKdlbRL1r0DeteC5dxbWkemCK6Mq04HpxjPT1qmDVqvm03RWTS1Kvlx51UDeusjeRtK3e/3Dbd+LkW5O2W9ambZLU+kPGCLKg8LFq0vNbB31texZshsaWYvu+btio+tJ0zl11dvByrkhYkvJiSdJWBBdFY6ri28xXBLemeDgLakVzs75NVLjrzkVQZD42lh5HVMRfmPVpw87mB6ZYX5IKc9BZ6efbujttMIng5VSYda+rYC3NlsiCgV1RcVb44yhycxy171ub/d4GVeatCSnIo03lwhOCPWUTBqKUA3d922teykwQRfRempuLNr687+c9JuhbuOCMKr0/SFKTpyEFzcIoqRc/xzcaAAAAADrHgwYAAACAzvGgAQAAAKBzPGgAAAAA6BwPGgAAAAA6R+oU8CByF1yUoFTLRRL5TIvK9I3G9bOIsjLSvoXSBCZJ6pl0qH610Q9rw6iW2q6V6dyP4qGKdL5Vz0c7FWU6Rk/LbN/5QXrMpfl9SVq96nFJ23TvWtt3dv5HSVt/3K8DvNqkS0lSUw+fOuUSaLLWp920JsGmjbKORujr0qiyzKfHlKWbW5SKlLZHqTQu4KcJ5uBuLzYxSlJrEp/C9C2T+JTn/lzkZm5RolHjkq+CRC03rkutkqQyN/ecILGsqc25MClSi+22WZnbU03wHmJOR2HukZLUmvSrKB+tMSlMbZA6VdXmfAZ71e3LaJs05rpv2uB9zL1cMO6SYiz9dZdaJX+OsuBf1a2LmApvG+kPooA0F54VXd+taW+C9/7aXEfu94fBNxoAAAAAOseDBgAAAIDO8aABAAAAoHM8aAAAAADoHMXgwP3kyvCiMuyo7Nvx5Y/RyPOmp+9bqZe09YNXc1XbvTJ9LUnqt3eaxuAW44pZaz9u6Yq5B/7Y3KtNVb54fbKaStrW9W+2fS87++Np4wbbVbu/8sSkbXoiOBdFWnwe1JgjENV/umrPLPhsrXEFxKaIWvKFmq5t8QfDzOpn7CUYFKGa5jY4NlfHGtS22mrwYBl0d3V30lZt9ps3K9Jxi2AhXOGtPT+SlixJ72WZKeZdHDidWxkcmy1KDhatNhuwroOL2FQ25yOcN0lqXNF2uJhDN9rmqADZXnRBYb07kDo4uNxdR8GwtftBmPVgivsLv+75WLpXW1dVLylzIQfR5/f2mIPzZiq886Aq3u3LLLhoXZ5BZgr7JUljpijepUUMgW80AAAAAHSOBw0AAAAAneNBAwAAAEDneNAAAAAA0DkeNAAAAAB0jtQpYEhRuM1oqVNpgtJAtwV908uzH1yydfiKw+kpTWBanIN/Neea73w3aZuZ8fOtTLRStelG23fehE71K9MoaX5+Nh13zh+bql2TpnW3+TnYZfdT0P/76GfTto2+r56zMmlatTqYL6zWJLRIkkyaShvE0rQm0SVKkmptQlU4uyFaFhVuDtGwmfmMMJjvwsJC0tYvNtu+uUniCVN0qvQabpv0tSSf+mOT5+Q//WyD6KG2Na9n0qUkqXAJYEGKTlOnrxfdY5sw9SyV5+nrNY0/y0HQlgskChOx3LK3rT+ftdltWTBu45KkKr8QTZ22RwlplUnrClPlTLKSS1WSpMocRh4kdbnLKLoO3dSiJDN/kwjuR2Z9omNzYV95EfzT3p3PIEgqNwvfbOc/M/hGAwAAAEDneNAAAAAA0DkeNAAAAAB0jgcNAAAAAJ0buhg8qHvUeEcTAR5KXJFXf4S+0YVVKq0KHgRFhn2lxcoD3RmMO0JFor1q0yLqxXHn03m1/qrv991d4nY/bmVer05fa3Hg9NgqU/QtSVU/XZ+ZeT+HDevStk1XrPVzCJqtqPDb+eQtSdO6J43w+9BY7gsqB6YY3BXHSrIVp1GBrROOa+TB53u22NMUiEtSbSqFg9pWO7nGFN1umV3y67m/mxVFWkWamTbJFzBXpkhY8occ1Ln7ou16+MJoU0/8s76mMLqJqnFNW7h33J4MDi6anFnMog2K2s0YYeG42VNBnboaUwEfHXJuNnYTFkG7iv2oIN2EJwQTzkyYQB2cTxcCEV5bZtw26GyL+3N/7nNzHMGtQJm9+fjOmU0S8H1dsXx0fd8XvtEAAAAA0DkeNAAAAAB0jgcNAAAAAJ3jQQMAAABA53jQAAAAANC5oVOnoowKUqfwyyzKaor2+7B9o4Sqvrli+ho+yaHUbcEc0sSmKri8B3YOU8HrpUc3G6TFTE3uk7RN924O5pCaK/3dxB+HX7OZ2WuTtu9/YcH21f/1zTvc13f0BH65LNRBHIuLKoqicVzfMOXFJDMFyTg2ESZKOnKJT0GclU1QcokywQtGy9C4xKZqczCq6Rt8dNnaBDA/XzduYyN7pKpKx6grf/fO3fqE6ztCX7eYYQyZ25O+Z7Q+eWbuh0Fyld2W0eVif+DHbewxB0lSbpuYa2hxamYO0b62yUzBuCOkb7UuSSo693Zefs1cmlX4Dw2T7hSlTrlTlAf/pHDrGx2bi8Frg4S/+8I3GgAAAAA6x4MGAAAAgM7xoAEAAACgczxoAAAAAOjc0MXgEVfUSYE4HmpGLfp2+zrS16wZNyrEdr8fzW6jaYsKptP2QVg47sZNi8m3jPKLNqy70vb8xMc/mbSVPb/Cq/c/LGlbs/9v2r4H779HOu6sLzKfufAraeM1tiseJuo2un5M4WNYy2iKlU2xqSS5elNbxHov7cP2zYugINgUyAY1qMpctXFUvG7HDfq64tbgVDRFumhNFXR2xfa+p5omHSMqms1dcWswrtsocRlsug6usD98qajQOFh3WwQdBgGk65Nlw4ePREXtY+VY+kpN8G7qmqOibXOO8mAOC1UQ8mHYrIco7MGczzxPj3ex3RVXB+O602mui8W5mevF7HUpCE8IbnSFOWgbkrD4g/T3R9g62wy1fb8GAAAAADEeNAAAAAB0jgcNAAAAAJ3jQQMAAABA53jQAAAAANC5+506BewooyRJRelSkVHGqDSVtA1MEtVi33SUgfq2b6mead0r6Osu5R8Ffe80bT5nq1IaMzG1fB/b97eOT5OkSvl0qOuuuzZpu/AjV9m+s/302FauOMD2PfTxr0/ann/Trrbv1NRE0jaY8+uw4dbL0rabbFetveV7SdvMvE/1mpm7NWnzOweRvPafl7UmPaYNY6dM2lKQM5TH8UNmjHQOpbmmJJ+uMxbGvJikmHBiLpVmlDSsaH3duD76qq3Tu3WUZlXaeJ4gfctFB0WHNvywQeeIG8QP7PZUlEwWnk2behb0tdFKfuTGxJZFl0tu0pLyIM2qzsw7dTCwC0CKPg33QwTRa25gNy9JtVngIKRNmbmOhr+ypDy4DjP3gsGauaS4KKnL7Z1oP7jrW5ujf3XdO77RAAAAANA5HjQAAAAAdI4HDQAAAACd40EDAAAAQOeGLgb/3fMuse0z610hqytilTSRFmVOTY/brpO7pEWzU9Np0a3ki0Wner7AdsXKtJj2VY/1y7DctG3nX2B/xHClQtGa3d++vrxWmgvanajA243td5RsGfWcNgZ90wLkKrhe/K7047p1iwvg0+uwCkuQ01n0gpUfzKWzuGWDLxx/wtFPSNqO/K2f2L7LtSxp+853Pmv7funi/5O0feSDtqsmenskbasPebLte/Sxf5S0Pf7Z/thOMEX488HuWXvTPydtN9zwLdsXXjO22ba3lSnqjApvTaFlVNSZmUJLW7wp/0lenvvP94rSFNhGnwWa5txXetry2LIIiqvN3OqgWLRqTBFq4e9apnZY7ZjtKlOTHNb4ypyL+NNTV2A7dM+wMFqZ22dBVzuFqNI4GMRMpI2qlcNJGyZMoK2DQmFzPsox/+7dyp3oqLA5HSO4XLSkSfu25vcXf2COLSyCdo1ByIFrDv+RmJ7QKJMhNwXebTCwC3ZogqsgMxOOwh4UhDVsD77RAAAAANA5HjQAAAAAdI4HDQAAAACd40EDAAAAQOd40AAAAADQuaFTp75w3vn+BzeYpJj5K4NRXA6OT5KyGT/jwXQHLgUnTfeRJD3pJUnT+rNeZruuXGXSbtb51J/Z2XRuRenTeXq9NGlrYnyp7bvLVHocE5N+HXoT6euVZZA9VA596lWY89YLfn+6TOfr0rskabX7/aFnJX1HN9v2s888K2m79Gvf9IP0XZ6VlE2niUQTZbofJGnuCpOAtCFKcXJzCNpLE0kx7feJDj4waTrqpKNs16OPPiJpW7Paj+tCZOZmXc6WdMMt1yZtX/uSP0ef/UR6P1m1v1/f6em9k7ajjz7W9j355DQ1qg7yt/r99FreMOPvXVd84c1J28Uf9KlT33/PZWnjwYfZvieec1LSdvLxacIVYsWCvxfVeZqw0gRJKi60JzNpQpJs1EyYHGQ+y4sSglqXGhWkQ5mwJakI4m7q9B7XBjFOjUmSqm0MlJQ1Zr7RsWXmHAVJPj4pKZjD0I1+hDZKE/LRQ76vGTkM7HEnLkiGyoLXs8cR7WvzcnkUdWSaS5OEJkmtGSMKuCpNbFQVnPoiM0lSUUKVizKLUuVcbmWwZk2b9s3MdSEFCXJVlNLmDnrB9m3dG6+96H17tHdczFUzQvRalg//78Z74hsNAAAAAJ3jQQMAAABA53jQAAAAANA5HjQAAAAAdC5r2+H+Rn2W7R78xBVdBwXISougpbSIVZJUmELL6dt8X1eYfOutwRy+lTat3st3XfsfwRjoxnOTlrPvvsD2fGovbfvyuktt3zee9gdJW/vPPxhtag9nT0qbzv7oB23X8V56bc1u+JHte91N16eNpb8OZ279YdJ26/oZ2/eGG9LC+l9dkxa0S9LBh/xa0tYze0eSLr/ie0nbeBA4sf/q9D518Ip9bd83HPH/+Re8n4a8VT/inP6i59v2pkqLOuugqNOW2IZF2+kYeVCsbGtTXQGppMJks4yFxeCuyNz3rev0/TjPg+JWM0Zejtm+hSnGjeq7XV192/ggDrfq8dZ3Rcl+XDdyUwcTNp/BNmFBuivG9TKz7k1QbB8Wn5v9F9cJux/4zmWZFmLn0ZG4fWKLs6XGVH7Ha+kCBvwU3L6Or1nTPnwNtIrgms2y9NqI9l/TpPN151IKrsPgJLtDGx8P3vTMENHeae3C+0X7y7/7gB/kZ/hGAwAAAEDneNAAAAAA0DkeNAAAAAB0jgcNAAAAAJ3jQQMAAABA50b4e+IbHqApBGlArnA/CJ2630iX2kEuTFr++xv/2va84JyXJW2zM/O2b3tZBwlTaQCH35O/jFZNJk0zM3farhMTaVLcret9ktSnz/9E0lbbhZSe9Iw0NerIQ3wC3TOetkfSNtu3XdWv0ltaqV1t38cfcljSNjP7Q9t3+fI0mW52fqOfBB5U9cKCbzdtLvVHkg2dihJhXPJK1NWmJQVJR1mWptLU0WeBJl0nOrbMTC46Nps000SRT2YdgniozPQNAoKiMCDfd6Q0IXNsQZqQGyOab2Y2T+tithQkVEVbMlpLlxwURDPlZtJRgldrIsPCPKw2va9nuR/YJUzVQeKY2yfRKXInKc/8+01e2tgz27c216dNw5LUmiSpqg5Sp9zCB4lj+Vj6PtYEa+ZerhjzfcdMslgWJNs1tZmvS3gdAt9oAAAAAOgcDxoAAAAAOseDBgAAAIDO8aABAAAAoHPbV9kBPEDab11v2zdsSpMA5jbc7gfpokb34VL47Vw9lzRtCArrp5UWg8/N+gVetTwt2v7+LbfYvmtvuTJpK2f2tX0rzSRtK/b3heO9ybTwu6rSgj1JmugtS9vM70tSaYrgZm/9ie2LB1c+QkFvXARtO9/vvgqKgu24Zm5NGxUVu+JqP4fa9g3GNUNUwaEtmOL1aB3KIi1CzaPCfCcs8HZV21FR/LCN/jDyYFxXZF4EBbZu1ZvGn4toX7tlC2qKbbF8WGxv+kYF6a72vA1CA9p6+H1iFyi4Zm1rsKXcaY4yDtxx1PLvIe7lsmBPjXBoak1BetNG/yhJB6krH5Dhrrk8uBe0pog/jzbafeAbDQAAAACd40EDAAAAQOd40AAAAADQOR40AAAAAHSOBw0AAAAAnSN1Cg8t13zLNs8M0lSkmX6aRIUhXJM2fe1rl9quxx57UtI2O+sTOG64JT0fk1OTtu+U9knaBvP+dnTpp7+ZtO2yxqeTHf2Mk9PXmkrTpSLlpE/U6ldp4sfcXJqkgwdfEXxe1thEoigZxyQ+BX19elE4veE7Z2miS5RKk5kEpCbqbIQJQSYlq22CtBuzEFnurwmbftQEaUIjXFbuXIx0KoI5tCOcZJdGlQd9C9M3CiZro5Qh8wtuP0RjuMSyxTHS+29ZRslrZg5BkpTbE1kbpBeZvVYHG8IeR7Cva3dtRfM144Zb0qx7lGxX2/MZJD6ZIaJvBVwyWBukQzW1mUNwn2va9H0+CFO7T3yjAQAAAKBzPGgAAAAA6BwPGgAAAAA6x4MGAAAAgM49dIvBx5enbYMND/488OCaNZXKkq77dlr8Oz87+0DP5hHjpov/xbavWX1g0jYrX6TYmva5db7vzNTStLHn53b8C5+SzmHg+87O35y0+dJ1aaocT9p2Ccr+BoO079SKvYKR8WAaaLNtz+r0c7SgBNUX/4aV2EM3SqZQM6jbDcaIOrsiVH90mesbFAS7Vl8YLeWmd1P7q80VqmdBFXTZpu15EX0magphg57KzTrUQfHwglnLoBK2zM05Lvw/rYrcFNtHcwhqwavNC2nf6J5shi6Ddc+LJUlbVGTemKLrOri4zCFL5tpc7GwK64M5uLm59f1Z76TFFkZLat05CorM3Rh1eD5NYX4ZhSeYayAMezDF62YdpeDcB8PWJiBDVfRueu/4RgMAAABA53jQAAAAANA5HjQAAAAAdI4HDQAAAACd40EDAAAAQOceuqlTJEzhHj5+2muStsc9/8k7YCYPUzO++bobbkwbyzTZaVGahjK2Yk/ftZxPmtbOXGu7Xn3dzknbwWv8uV+157Kkbc2ap9m+06smkrb1N11m+/Y3pTFXZc8nhmi/tOlxx/uu115oGm/1feEtaX1cWZWnKS9FkKBUN2lkTpj3ZNJuohSn0ZKkjCBJqjHpaK2LGJKUj5I005gkniD1xwX8BFOwcyuCNWtMuwl22jK59LWC9CN3jtooKikz7U2QwGQ+r22DvrVZy8atuXxK0ZZX/EV5cELdsmVBepb9/SChyq5PkOlmz0dwPt1xRHsqM0lk0bo3TZqWtBAkpNkAOj+FaGJ+DuZAWnPfkSRV7t7lxx1bYtqD8+b2apb5fdaYE1dn2/fIwDcaAAAAADrHgwYAAACAzvGgAQAAAKBzPGgAAAAA6NxDtxgcuKe1abXytW//1A6YyMNUUNfcr9KCufkZX0SnTWnTgu60XQf96aRtetmutu+q1fumfaf9rauv29LGatz2nZp4QtI2sb+fw9pr0iLxmQ0/sn312Mmk6YRT32i7nvqqtJC5f9sP/LiwivGgqLNKP0dr2uCzNVcAagteg75Rxaopbs3yoBjX1o1HRb5pAWcbFAQ3bmq2UVJuiuKDAtu2SG8aWVA265YnWrHWzK02xytJuatIH6F0t43qol0teNDVHXMTFfmaY6tMofK9vZ7c/olOZ5muT5n7e2duxi2CouLcFbXbc+HXIjP7bPEHphi8DhMGkqZafi1tyMEI2iBhwO0/FxYhSXnpggCCcApz9oNh5f8Z789F1pr7RjCHzIyxvcvINxoAAAAAOseDBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADr34KZOuUCXjQ/qDAA4wZ2g15tI2mZnBr6za+77xKf+/GzSNl4GaVa9dHKDvu87UDrfqkzbJOm6G9LUqMkVvm+5ZzqH/nqfqKW5NCWr7O9luxZmeaZX+77wimyJbW+zYD8ZLikmCmZyaSxB4I5clNTw+VRxa2vikrJovnn6gzbINHLJVVlwcDZJKgr1qtPXa4MYndqkXEVhN0uiBC83B3NCo1Sl2jSHiVouScqk+yyOkYoCqqJ0MpdUFKWeuREWguig3LTneZqKJ/k9XNXR9Zb2zqKEtCHbFscYftzSJKRFQXG1SbnKgs6Vm0OUImZOdBskumXmYo7m25jXqxsfI+lSyKIFdveILLjX3he+0QAAAADQOR40AAAAAHSOBw0AAAAAneNBAwAAAEDnHtxicAq/gYem4E5QaqlpDYr+XHPfX/Tzmjetvsi8V6V9x+f38HMo0zHKSd91YtwXOjr9+bRqu+r3fefKFJRXvgBzvk7Xp66CY4PVDl/zHVd4u/LWqM7YtBe5L76sXQFoMIXCFF1HBci+6NVP2NZyN0GBtxs1KErOTTF3G5W6m/XJsrjMN2kxxe+LY7jCfD+HxszXhQBIUmGmFgwr5a5o1q+vKwgOpmCLviWpNWM0wbo3pgg/zi1If1K4hQher1rwF6Lb13mUntCaYzPHsPiDdA55sO6uOD/aU62bQ1RsbwrHXTG5JFV1OocoEKE0xxFMV7VZnyYIwsjztJg7i4IhXEpBttlP4j7wjQYAAACAzvGgAQAAAKBzPGgAAAAA6BwPGgAAAAA6x4MGAAAAgM49uKlTAB6aggCluf6dadtsMMacawySU2bTdKj5yiVcSbMyiU8mXUqSev00bWN2k0+++vu3fSlpe805r7B9B2a+szNBatVtJvHDh06pqtI0nio4F/Cywi9u1poUpyA5KDOpMnlwzhwTVLM4rtn/0ad7PoDG93bzDQJ37BWYB0lSTuEDtVSaCdfBJOz61L6vm1sWpBS59K0Fk+4jSS68KEqdsulbYbJT2pabJCpJak1Skkuiknxi2eI8hm91q5YVUYpYeqKjOdTVgunr1z3Px5K2Jrgh2gSlKCnOHFw0brNgEp+iWDlz7tx52zLKL6qjWDk3RnBttebgoqSupjHXS5DUVZn0wzK4b9TmOnLJWcPgGw0AAAAAneNBAwAAAEDneNAAAAAA0DkeNAAAAAB0buhi8APe/D9t+8rVy5K2yckJ2/dLX3hf0rbq4MNs32v/+C3DTg3A/TZpW+vaFF3PBEPMm7boDtNPC7xbTfmuE2lxdTl/azDwdNo08IXjWvftpOmG63zh+PKJtGpvduAOWFKVFtD3K19BP95P51bpNj8urDZbYtuzNi1Yjeo/bf1mVAVt6izboADUFXUGVd+2YLqpTbCAZI8jKnRvTd82qhw3ffOo71jaXrrFCcato3Vwhfl+VDVu0YJCWFf43YSl1b4c3I+b9o32gxs2KkjPgnW3xxzOLW2rgi2VZ+kPqioqSE8HLovhC/brerPt2zauCDo4NvdybrNLkimAz4M1a02IRJMFxf2VKe4PQgPsJo7CKdxtI1iHxuz3aE+5YvmF2k+iatO+43lwT7wPfKMBAAAAoHM8aAAAAADoHA8aAAAAADrHgwYAAACAzvGgAQAAAKBzQ6dOff+8T/t2mzDl02M0k6a0XPsP5w87BQAPmDnb2u+biJIqSHwaJXXKhTAFyUyDIk2o6pn0DEmqqjQ1aq7vJ/EH5/1R0jY1naboSdK676dJUjN9n1AlpfOtgjkM3HFUQ9+WIakugugWk2Ajk6QiSblLaQkClCoTFeNTihSmyjiN6dsGUVKuuQmSjjKbfBVMwqb2+HGbJl2gNujrkoeKYJs3tYvUCibsugYD5216L6vlI5hysw5RQlW7kK5DXvrPcN3q1O7ES2qjdrsxg8Qnk1SUZ/4a8C/n52CTyDK/7m1urhd3jiW1Zm6lfKqc3e7mtSTJvl0ESVJy+zq6tswhZ02QzGTSwvJ8zHYt8nR9o/C31qTjuWtzcQpp+yCIIXNj1Nv51QTfaAAAAADoHA8aAAAAADrHgwYAAACAzvGgAQAAAKBzw1cd3vaVoL2jmQDYcdb65k3X3Zw23hKM4Wq5g7o4jfdN311t16qX3qb6A1d5Ls3NpIXqN6z9hu178CG/l7RNyMxL0vzsjWnb/A9sX82lReKzfV/oXppbcFFGiwbLFX1Lapu0yDEqmHa1k3lQWOqLdH3f2lRtZ7bg2hd4R8XVvjg6KrA1c4iKwd3rBVWojVmH4NDszFyBeDSHqGB6rEjHyEybJLV1el0VUbGzKbCtg1NRmUrjzBTzSlJrCm/jou8oScAcc7CvW1fUHpyk2lwEZRBG0JiC9IEpipekJSP8M9OtRTYW7ZN03dsRroForzbhtTFc5zYYwAVO+MJ+qarT9iIY13S1heeSVC2kbUHduF2gsSXb990E32gAAAAA6BwPGgAAAAA6x4MGAAAAgM7xoAEAAACgczxoAAAAAOjc8HEAAB6+0gCPRV8NkpWGNRihfT5NjJKktjeVtM3N9Wzf3i6TSdt1V3/Z9p2dTcc4+shjfd9BmkbVn/mJ7SvNpX1r33e+v8y0jgfjwsmCVKQsM+ldeZAI44K+gjiWPHcXSzAHl6Jje0qFmZtL95GidJ0gcWfo3/fL0wRxSy7oKAtSilySWlH4NatMNM5CHdygxsaSpjIY162PS1qSpNYO4Y8tc+3RuFHCTzCy49KLmmhubh42psiPG6aTmSGCy1CZS1uKoo7MNZuX/p+pubnmwtQzlwIWnIy6TZPBFoJksOjqGnYO0TK41KkmOBmNO45gIVyaWtuYKCpJrRnXLM1Q+EYDAAAAQOd40AAAAADQOR40AAAAAHSOBw0AAAAAnaMYHMADZz5oH+XOU96ctvlacNWbJtJf7/lJrL/pS0nbFzaZ15LUK3+UtPXngoPbJW0a9DfarnOlq65LC88RywpfodhUprg1LMU2gsLxrDHFrUEBaG5eLyqYdq1RAWhr5pAH9aquqDOuSU4/e8zy4SuY3bwkqTHt1YIv8LaFwkExbtOk574Obi62HjjYDq4wOqqMbpv0OMKi7yztG+7I6Hxmpkg3KlY2xdXhNWDGqILjaM0OKvO0MH+xPZ1DVLxet2Z9TJsUFOwHy2BDDtyGkFQqPY48CERYqNP9VwRr5q5PW8gtBcfh12HBnKTxYontO9Yzx7bZ3z8Hm909ZqQ0g5//3nb9FgAAAADcCx40AAAAAHSOBw0AAAAAneNBAwAAAEDneNAAAAAA0DlSpwA8+GZNWxpOsmhiPG2bG9iuC1Xat/JhHRo3yVUzt11r+/Ym0gn353f1AxdmDkHf+WKpaZ3248LKsp19e7k5aasHC7ZvXqYxL20TfA5nUqOyIO7GJgQFn+8VLhknCHlxrxcE7kRhSUFnk1AVDWzk0TrU6UVYBdlXLoknjyK1RpCblKFovi6ZqTHpUj/rnCgKfzPLzKlvghSyuvbrU7uoLHPeJJ84FkVt1SbVqA0SqmqzKQpzjhfHMH0Ln1A1ZtKSlrgbtfw+Cc+RmVth0rAkn1DVjvlzNFal/4Qe5D7FqTbpUFlwDbg55MEb5BKzr3s9nzq105J0LTcH77tj5p64c2+Um8nP8Y0GAAAAgM7xoAEAAACgczxoAAAAAOgcDxoAAAAAOkcxOICHBlcgLkk984NJXyAoU4fX9tbarv1N80lb1jOF55KkibRpLv19SVJvKn2tftC1l47R7wcFjbB6G33xfLvbbUnbbFDZXJpCzXzcF3VWd6XVk1nui8zV2upf2zUzxZcyhZ6SlJv5mqbFKdhC92gOaXsRVJNvbtJjzoJi3MxUurvid0mqzOefPVdFLakcS8/FkmC+rekb/QtorE1/cNfd/tjyIl3fsvbz3ewKjV3lr6Ss9YEXtrq/71+vzc2+DgqQXU1wmfsFGpgC7yYYt63SfZKPB8Xy7U5J25LM9x24QvdgT2VmzfIiOPkmgSEKVChsmECwDqZvsK3VmvtG0Qb3LnPN5kUQOOHWrPbj9pt0//VaH7xxX/hGAwAAAEDneNAAAAAA0DkeNAAAAAB0jgcNAAAAAJ3jQQMAAABA57K2DUrZAQAAAGA78Y0GAAAAgM7xoAEAAACgczxoAAAAAOgcDxoAAAAAOseDBgAAAIDO8aABAAAAoHM8aAAAAADoHA8aAAAAADrHgwYAAACAzv3/0yIGb709HOsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 3.7311e-02,  7.5761e-02,  1.2611e-02,  ..., -7.8920e-02,\n",
       "           5.3719e-02,  7.0011e-02],\n",
       "         [ 2.8552e-02,  4.1602e-02,  4.8338e-02,  ...,  7.6114e-03,\n",
       "          -2.3722e-02, -1.2175e-01],\n",
       "         [ 1.8701e-02,  1.3992e-02,  4.0341e-02,  ...,  5.0359e-02,\n",
       "           3.4249e-02,  9.6143e-03],\n",
       "         ...,\n",
       "         [-5.1202e-02, -7.8198e-02, -7.2400e-02,  ..., -1.5632e-01,\n",
       "          -2.4988e-01, -9.7125e-02],\n",
       "         [-2.7557e-02, -6.7322e-02, -1.1067e-01,  ..., -6.9369e-02,\n",
       "          -8.2046e-02,  7.7235e-02],\n",
       "         [ 8.0680e-02,  6.4730e-02, -1.4687e-01,  ..., -6.5196e-02,\n",
       "          -1.9238e-01,  4.0087e-02]],\n",
       "\n",
       "        [[ 6.3577e-02,  4.7199e-02,  5.9052e-02,  ..., -1.4432e-02,\n",
       "           9.3928e-02,  1.9572e-01],\n",
       "         [ 8.0538e-02,  6.2441e-02,  7.0518e-02,  ...,  2.0192e-03,\n",
       "           9.1649e-05, -1.2000e-02],\n",
       "         [ 4.1432e-02,  4.4350e-02,  5.5098e-02,  ...,  6.2148e-02,\n",
       "           4.0629e-02,  4.5015e-02],\n",
       "         ...,\n",
       "         [ 5.3327e-02, -4.9383e-02, -6.8943e-02,  ..., -1.4855e-01,\n",
       "          -2.2711e-01, -1.5111e-01],\n",
       "         [ 7.3230e-02,  2.8214e-04, -9.8831e-02,  ..., -1.3875e-02,\n",
       "          -8.9901e-02,  3.9138e-02],\n",
       "         [-2.0639e-01,  1.1286e-01, -2.7636e-02,  ..., -5.5603e-02,\n",
       "          -1.1382e-01,  2.7101e-02]],\n",
       "\n",
       "        [[ 1.9484e-01,  1.1097e-01,  7.2642e-02,  ..., -1.6364e-01,\n",
       "          -7.6479e-03, -2.6762e-02],\n",
       "         [ 7.9951e-02,  9.8966e-02,  8.2017e-02,  ..., -1.1467e-02,\n",
       "          -7.5398e-03,  6.4657e-03],\n",
       "         [ 9.9677e-02,  5.9683e-02,  8.5172e-02,  ...,  5.6709e-02,\n",
       "           5.1100e-02, -8.1967e-03],\n",
       "         ...,\n",
       "         [-3.4088e-03, -3.6731e-03, -2.3502e-02,  ..., -1.5408e-01,\n",
       "          -1.8614e-01, -1.7277e-01],\n",
       "         [ 1.4727e-01,  5.2427e-02, -1.6163e-02,  ..., -3.1762e-02,\n",
       "          -4.8874e-02, -2.5108e-02],\n",
       "         [ 4.1237e-01,  9.3633e-02,  4.2023e-02,  ..., -4.4899e-02,\n",
       "          -1.3723e-01, -2.2649e-03]]], device='cuda:0')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot the sampled image next to the original image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.imshow(dataset[0][0].permute(1, 2, 0))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(unnormalize(sampled_images[0].cpu(), mean, std).cpu().permute(1, 2, 0))\n",
    "plt.title('Sampled Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "sampled_images[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
