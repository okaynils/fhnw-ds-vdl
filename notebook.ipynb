{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vdl Depth and Class Constrained Diffusion\n",
    "- **Name:** Nils Fahrni\n",
    "- **Date:** 07.01.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "- 894 classes -> 18 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "from data.nyuv2 import NYUDepthV2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_t = transforms.Compose([\n",
    "    transforms.CenterCrop(400),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "crop_t = transforms.Compose([\n",
    "    transforms.CenterCrop(400)\n",
    "])\n",
    "\n",
    "dataset = NYUDepthV2(root='data', \n",
    "                     download=True, \n",
    "                     preload=False, \n",
    "                     image_transform=image_t, \n",
    "                     seg_transform=crop_t, \n",
    "                     depth_transform=crop_t, \n",
    "                     filtered_classes=[5, 11, 21, 26, 2, 3, 7, 64, 144, 19, 119, 157, 28, 55, 15, 59, 4, 83])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Subset\n",
    "import numpy as np\n",
    "\n",
    "def split_dataset(dataset, train_ratio=0.7, val_ratio=0.2, test_ratio=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Splits the dataset into train, validation, and test partitions.\n",
    "\n",
    "    :param dataset: The NYUDepthV2 dataset object.\n",
    "    :param train_ratio: Proportion of the dataset to allocate to the training set.\n",
    "    :param val_ratio: Proportion of the dataset to allocate to the validation set.\n",
    "    :param test_ratio: Proportion of the dataset to allocate to the test set.\n",
    "    :param random_seed: Seed for reproducibility of the split.\n",
    "    :return: A tuple of (train_dataset, val_dataset, test_dataset).\n",
    "    \"\"\"\n",
    "    assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.\"\n",
    "\n",
    "    # Set random seed for reproducibility\n",
    "    np.random.seed(random_seed)\n",
    "    \n",
    "    # Total size of the dataset\n",
    "    total_size = len(dataset)\n",
    "    indices = np.arange(total_size)\n",
    "    \n",
    "    # Shuffle the indices\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    # Calculate split points\n",
    "    train_split = int(train_ratio * total_size)\n",
    "    val_split = train_split + int(val_ratio * total_size)\n",
    "    \n",
    "    # Split the indices\n",
    "    train_indices = indices[:train_split]\n",
    "    val_indices = indices[train_split:val_split]\n",
    "    test_indices = indices[val_split:]\n",
    "    \n",
    "    # Create dataset subsets\n",
    "    train_dataset = Subset(dataset, train_indices)\n",
    "    val_dataset = Subset(dataset, val_indices)\n",
    "    test_dataset = Subset(dataset, test_indices)\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unnormalize(img):\n",
    "    mean = torch.tensor([1.3268, 1.1391, 1.0176]).view(-1, 1, 1)  # Reshape to [C, 1, 1]\n",
    "    std = torch.tensor([0.5704, 0.5411, 0.5077]).view(-1, 1, 1)    # Reshape to [C, 1, 1]\n",
    "    img = std * img + mean\n",
    "    img = torch.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "dataset[0][4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 1771907\n",
      "Output shape: torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "from core import UNet_Baseline\n",
    "\n",
    "# Instantiate the model\n",
    "net = UNet_Baseline(num_classes=18, device=\"cuda\").to(\"cuda\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in net.parameters())}\")\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randn(1, 3, 64, 64).to(\"cuda\")\n",
    "t = torch.randint(0, 1000, (x.shape[0],)).to(\"cuda\")\n",
    "class_vector = torch.ones((x.shape[0], 18)).to(\"cuda\")\n",
    "depth_vector = torch.zeros((x.shape[0], 18)).to(\"cuda\")\n",
    "\n",
    "# Forward pass\n",
    "output = net(x, t, class_vector, depth_vector)\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        epsilon = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, class_vectors=None, depth_vectors=None, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                \n",
    "                current_class_vectors = class_vectors if class_vectors is not None else None\n",
    "                current_depth_vectors = depth_vectors if depth_vectors is not None else None\n",
    "              \n",
    "                predicted_noise = model(x, t, current_class_vectors, current_depth_vectors)\n",
    "                \n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        \n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import wandb\n",
    "from core import EMA\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import copy\n",
    "import os\n",
    "import wandb\n",
    "from core import EMA\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, diffusion, optimizer, epochs, device, train_dataloader, val_dataloader=None, \n",
    "                 run_name='diffusion_model', project_name='diffusion_project', save_dir='models', ema_decay=0.995, \n",
    "                 sample_images_every=100, resolved_names=None, scheduler=None):\n",
    "        self.model = model.to(device)\n",
    "        self.diffusion = diffusion\n",
    "        self.optimizer = optimizer\n",
    "        self.scheduler = scheduler\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.run_name = run_name\n",
    "        self.project_name = project_name\n",
    "        self.save_dir = save_dir\n",
    "        self.ema_decay = ema_decay\n",
    "        self.resolved_names = resolved_names\n",
    "\n",
    "        # Initialize EMA\n",
    "        self.ema = EMA(ema_decay)\n",
    "        self.ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
    "\n",
    "        # Initialize Weights & Biases\n",
    "        wandb.init(project=self.project_name, name=self.run_name)\n",
    "        self.run_id = wandb.run.id\n",
    "        self.best_val_loss = float('inf')\n",
    "        self.early_stopping_counter = 0\n",
    "        self.sample_images_every = sample_images_every\n",
    "\n",
    "        # Create a models directory if it doesn't exist\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def _train_epoch(self):\n",
    "        self.model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for images, segments, depths, class_vectors, depth_vectors in self.train_dataloader:\n",
    "            images = images.to(self.device)\n",
    "            class_vectors = class_vectors.to(self.device)\n",
    "            depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "            # Sample time steps\n",
    "            t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "\n",
    "            # Add noise to images\n",
    "            x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "            # Predict noise using the model\n",
    "            predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "            # Backpropagation\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            # Update EMA model\n",
    "            self.ema.step_ema(self.ema_model, self.model)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        # Step the scheduler if it's provided\n",
    "        if self.scheduler:\n",
    "            self.scheduler.step()\n",
    "\n",
    "        avg_loss = epoch_loss / len(self.train_dataloader)\n",
    "        return avg_loss\n",
    "\n",
    "    def _validate_epoch(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, segments, depths, class_vectors, depth_vectors in self.val_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                class_vectors = class_vectors.to(self.device)\n",
    "                depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "                t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "                x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "                predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "                loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.val_dataloader)\n",
    "        return avg_val_loss\n",
    "\n",
    "    def _save_model(self, val_loss):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            model_name = f\"{self.run_name}_{self.run_id}.pth\"\n",
    "            save_path = os.path.join(self.save_dir, model_name)\n",
    "            torch.save(self.model.state_dict(), save_path)\n",
    "            logger.info(f\"Model saved to {save_path} with val_loss {val_loss:.4f}\")\n",
    "\n",
    "    def _plot_samples(self, n=5, n_present_classes=3, depth_lower=0.1, depth_upper=3.0):\n",
    "        indices = torch.stack([torch.randperm(18)[:n_present_classes] for _ in range(n)])\n",
    "        class_vectors = torch.zeros((n, 18)).to(self.device)\n",
    "        rows = torch.arange(n).unsqueeze(1)\n",
    "        class_vectors[rows, indices] = 1\n",
    "        \n",
    "        depth_vectors = torch.rand(n, 18).to(self.device) * depth_upper + depth_lower\n",
    "        \n",
    "        class_labels = []\n",
    "        for i in range(n):\n",
    "            indices = torch.where(class_vectors[i] == 1)[0].tolist()\n",
    "            labels = [(self.resolved_names[idx], depth_vectors[i, idx].item()) for idx in indices]\n",
    "            class_labels.append(labels)\n",
    "\n",
    "        default_sampled_images = self.diffusion.sample(self.model, n=n, class_vectors=class_vectors, depth_vectors=depth_vectors)\n",
    "        ema_sampled_images = self.diffusion.sample(self.ema_model, n=n, class_vectors=class_vectors, depth_vectors=depth_vectors)\n",
    "        \n",
    "        fig_default, axes_default = plt.subplots(1, n, figsize=(n * 3, 3))\n",
    "        for i, ax in enumerate(axes_default):\n",
    "            img = default_sampled_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(\"\\n\".join([f\"{cls}: {depth:.2f}\" for cls, depth in class_labels[i]]), fontsize=8)\n",
    "        fig_default.tight_layout()\n",
    "        plt.close(fig_default)\n",
    "\n",
    "        fig_ema, axes_ema = plt.subplots(1, n, figsize=(n * 3, 3))\n",
    "        for i, ax in enumerate(axes_ema):\n",
    "            img = ema_sampled_images[i].permute(1, 2, 0).cpu().numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.axis('off')\n",
    "            ax.set_title(\"\\n\".join([f\"{cls}: {depth:.2f}\" for cls, depth in class_labels[i]]), fontsize=8)\n",
    "        fig_ema.tight_layout()\n",
    "        plt.close(fig_ema)\n",
    "        \n",
    "        wandb.log({\"Default Model Samples\": wandb.Image(fig_default),\n",
    "                   \"EMA Model Samples\": wandb.Image(fig_ema)})\n",
    "\n",
    "    def run(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            logger.info(f\"Epoch {epoch + 1}/{self.epochs}\")\n",
    "            \n",
    "            current_lr = self.optimizer.param_groups[0]['lr']\n",
    "\n",
    "            train_loss = self._train_epoch()\n",
    "            val_loss = self._validate_epoch()\n",
    "            \n",
    "            wandb.log({\"epoch\": epoch + 1,\n",
    "                       \"train_loss\": train_loss,\n",
    "                       \"val_loss\": val_loss,\n",
    "                       \"learning_rate\": current_lr})\n",
    "            \n",
    "            logger.info(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "            \n",
    "            if epoch % self.sample_images_every == 0:\n",
    "                self._plot_samples()\n",
    "\n",
    "            self._save_model(val_loss)\n",
    "\n",
    "    def test(self, test_dataloader):\n",
    "        self.model.eval()\n",
    "        test_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for images, segments, depths, class_vectors, depth_vectors in test_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                class_vectors = class_vectors.to(self.device)\n",
    "                depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "                t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "                x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "                predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "                loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_test_loss = test_loss / len(test_dataloader)\n",
    "        logger.info(f\"Test Loss: {avg_test_loss:.4f}\")\n",
    "        wandb.log({\"test_loss\": avg_test_loss})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 7\n",
      "Validation samples: 2\n",
      "Testing samples: 1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_dataset, val_dataset, test_dataset = split_dataset(dataset[:10], train_ratio=0.7, val_ratio=0.2, test_ratio=0.1)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(val_dataset)}\")\n",
    "print(f\"Testing samples: {len(test_dataset)}\")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:a48hn35l) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <style>\n",
       "        .wandb-row {\n",
       "            display: flex;\n",
       "            flex-direction: row;\n",
       "            flex-wrap: wrap;\n",
       "            justify-content: flex-start;\n",
       "            width: 100%;\n",
       "        }\n",
       "        .wandb-col {\n",
       "            display: flex;\n",
       "            flex-direction: column;\n",
       "            flex-basis: 100%;\n",
       "            flex: 1;\n",
       "            padding: 10px;\n",
       "        }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇███</td></tr><tr><td>learning_rate</td><td>████████▇▇▇▇▇▆▆▆▅▅▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▂▂▂▂▂▂▁</td></tr><tr><td>train_loss</td><td>██▆▅▆▃▃▃▄▃▄▂▃▂▂▂▂▄▁▆▁▂▁▁▁▆▁▁▃▂▁▂▂▁▃▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▇▃▃▂▂▂▃▂▁▂▁▄▂▁▁▂▁▁▂▁▁▁▂▂▂▁▃▁▁▁▁▁▁▁▁▁▁▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>501</td></tr><tr><td>learning_rate</td><td>0.0</td></tr><tr><td>train_loss</td><td>0.07555</td></tr><tr><td>val_loss</td><td>0.0648</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">nyu_depth_diffusion</strong> at: <a href='https://wandb.ai/okaynils/vdl/runs/a48hn35l' target=\"_blank\">https://wandb.ai/okaynils/vdl/runs/a48hn35l</a><br/> View project at: <a href='https://wandb.ai/okaynils/vdl' target=\"_blank\">https://wandb.ai/okaynils/vdl</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 10 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20241220_201733-a48hn35l\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:a48hn35l). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\wandb\\run-20241220_202540-654eyo9e</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/okaynils/vdl/runs/654eyo9e' target=\"_blank\">nyu_depth_diffusion</a></strong> to <a href='https://wandb.ai/okaynils/vdl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/okaynils/vdl' target=\"_blank\">https://wandb.ai/okaynils/vdl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/okaynils/vdl/runs/654eyo9e' target=\"_blank\">https://wandb.ai/okaynils/vdl/runs/654eyo9e</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "08:25:44 - INFO: Epoch 1/1000\n",
      "08:25:44 - INFO: Train Loss: 1.1445, Val Loss: 1.0623\n",
      "08:25:44 - INFO: Sampling 5 new images....\n",
      "999it [00:13, 73.57it/s]\n",
      "08:25:58 - INFO: Sampling 5 new images....\n",
      "999it [00:12, 77.41it/s]\n",
      "08:26:11 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 1.0623\n",
      "08:26:11 - INFO: Epoch 2/1000\n",
      "08:26:11 - INFO: Train Loss: 1.0580, Val Loss: 1.0151\n",
      "08:26:11 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 1.0151\n",
      "08:26:11 - INFO: Epoch 3/1000\n",
      "08:26:11 - INFO: Train Loss: 0.9999, Val Loss: 0.9796\n",
      "08:26:11 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.9796\n",
      "08:26:11 - INFO: Epoch 4/1000\n",
      "08:26:11 - INFO: Train Loss: 0.9676, Val Loss: 0.9601\n",
      "08:26:11 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.9601\n",
      "08:26:11 - INFO: Epoch 5/1000\n",
      "08:26:11 - INFO: Train Loss: 0.9450, Val Loss: 0.9357\n",
      "08:26:11 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.9357\n",
      "08:26:11 - INFO: Epoch 6/1000\n",
      "08:26:12 - INFO: Train Loss: 0.9217, Val Loss: 0.9058\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.9058\n",
      "08:26:12 - INFO: Epoch 7/1000\n",
      "08:26:12 - INFO: Train Loss: 0.8807, Val Loss: 0.8701\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.8701\n",
      "08:26:12 - INFO: Epoch 8/1000\n",
      "08:26:12 - INFO: Train Loss: 0.8614, Val Loss: 0.8471\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.8471\n",
      "08:26:12 - INFO: Epoch 9/1000\n",
      "08:26:12 - INFO: Train Loss: 0.8689, Val Loss: 0.7770\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.7770\n",
      "08:26:12 - INFO: Epoch 10/1000\n",
      "08:26:12 - INFO: Train Loss: 0.8073, Val Loss: 0.7495\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.7495\n",
      "08:26:12 - INFO: Epoch 11/1000\n",
      "08:26:12 - INFO: Train Loss: 0.7727, Val Loss: 0.8018\n",
      "08:26:12 - INFO: Epoch 12/1000\n",
      "08:26:12 - INFO: Train Loss: 0.7402, Val Loss: 0.6381\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.6381\n",
      "08:26:12 - INFO: Epoch 13/1000\n",
      "08:26:12 - INFO: Train Loss: 0.6596, Val Loss: 0.5780\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.5780\n",
      "08:26:12 - INFO: Epoch 14/1000\n",
      "08:26:12 - INFO: Train Loss: 0.6340, Val Loss: 0.5717\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.5717\n",
      "08:26:12 - INFO: Epoch 15/1000\n",
      "08:26:12 - INFO: Train Loss: 0.5177, Val Loss: 0.4987\n",
      "08:26:12 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.4987\n",
      "08:26:12 - INFO: Epoch 16/1000\n",
      "08:26:13 - INFO: Train Loss: 0.6232, Val Loss: 0.5020\n",
      "08:26:13 - INFO: Epoch 17/1000\n",
      "08:26:13 - INFO: Train Loss: 0.5153, Val Loss: 0.4806\n",
      "08:26:13 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.4806\n",
      "08:26:13 - INFO: Epoch 18/1000\n",
      "08:26:13 - INFO: Train Loss: 0.5091, Val Loss: 0.4102\n",
      "08:26:13 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.4102\n",
      "08:26:13 - INFO: Epoch 19/1000\n",
      "08:26:13 - INFO: Train Loss: 0.4713, Val Loss: 0.4049\n",
      "08:26:13 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.4049\n",
      "08:26:13 - INFO: Epoch 20/1000\n",
      "08:26:13 - INFO: Train Loss: 0.4120, Val Loss: 0.4331\n",
      "08:26:13 - INFO: Epoch 21/1000\n",
      "08:26:13 - INFO: Train Loss: 0.4060, Val Loss: 0.3811\n",
      "08:26:13 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.3811\n",
      "08:26:13 - INFO: Epoch 22/1000\n",
      "08:26:13 - INFO: Train Loss: 0.4830, Val Loss: 0.5150\n",
      "08:26:13 - INFO: Epoch 23/1000\n",
      "08:26:13 - INFO: Train Loss: 0.3648, Val Loss: 0.4838\n",
      "08:26:13 - INFO: Epoch 24/1000\n",
      "08:26:13 - INFO: Train Loss: 0.3350, Val Loss: 0.5831\n",
      "08:26:13 - INFO: Epoch 25/1000\n",
      "08:26:13 - INFO: Train Loss: 0.4448, Val Loss: 0.2965\n",
      "08:26:13 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2965\n",
      "08:26:13 - INFO: Epoch 26/1000\n",
      "08:26:14 - INFO: Train Loss: 0.2982, Val Loss: 0.5844\n",
      "08:26:14 - INFO: Epoch 27/1000\n",
      "08:26:14 - INFO: Train Loss: 0.3347, Val Loss: 0.3335\n",
      "08:26:14 - INFO: Epoch 28/1000\n",
      "08:26:14 - INFO: Train Loss: 0.3351, Val Loss: 0.3452\n",
      "08:26:14 - INFO: Epoch 29/1000\n",
      "08:26:14 - INFO: Train Loss: 0.4465, Val Loss: 0.2669\n",
      "08:26:14 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2669\n",
      "08:26:14 - INFO: Epoch 30/1000\n",
      "08:26:14 - INFO: Train Loss: 0.3743, Val Loss: 0.2655\n",
      "08:26:14 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2655\n",
      "08:26:14 - INFO: Epoch 31/1000\n",
      "08:26:14 - INFO: Train Loss: 0.2643, Val Loss: 0.2710\n",
      "08:26:14 - INFO: Epoch 32/1000\n",
      "08:26:14 - INFO: Train Loss: 0.3690, Val Loss: 0.4969\n",
      "08:26:14 - INFO: Epoch 33/1000\n",
      "08:26:14 - INFO: Train Loss: 0.2848, Val Loss: 0.2324\n",
      "08:26:14 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2324\n",
      "08:26:14 - INFO: Epoch 34/1000\n",
      "08:26:14 - INFO: Train Loss: 0.3657, Val Loss: 0.2695\n",
      "08:26:14 - INFO: Epoch 35/1000\n",
      "08:26:14 - INFO: Train Loss: 0.2512, Val Loss: 0.2244\n",
      "08:26:14 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2244\n",
      "08:26:14 - INFO: Epoch 36/1000\n",
      "08:26:14 - INFO: Train Loss: 0.3340, Val Loss: 0.2850\n",
      "08:26:14 - INFO: Epoch 37/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2349, Val Loss: 0.3637\n",
      "08:26:15 - INFO: Epoch 38/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2317, Val Loss: 0.2178\n",
      "08:26:15 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2178\n",
      "08:26:15 - INFO: Epoch 39/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2156, Val Loss: 0.2025\n",
      "08:26:15 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2025\n",
      "08:26:15 - INFO: Epoch 40/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2017, Val Loss: 0.2195\n",
      "08:26:15 - INFO: Epoch 41/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2103, Val Loss: 0.2014\n",
      "08:26:15 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.2014\n",
      "08:26:15 - INFO: Epoch 42/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2008, Val Loss: 0.4038\n",
      "08:26:15 - INFO: Epoch 43/1000\n",
      "08:26:15 - INFO: Train Loss: 0.1881, Val Loss: 0.1970\n",
      "08:26:15 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1970\n",
      "08:26:15 - INFO: Epoch 44/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2453, Val Loss: 0.2015\n",
      "08:26:15 - INFO: Epoch 45/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2184, Val Loss: 0.1791\n",
      "08:26:15 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1791\n",
      "08:26:15 - INFO: Epoch 46/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2422, Val Loss: 0.2016\n",
      "08:26:15 - INFO: Epoch 47/1000\n",
      "08:26:15 - INFO: Train Loss: 0.2047, Val Loss: 0.3090\n",
      "08:26:15 - INFO: Epoch 48/1000\n",
      "08:26:16 - INFO: Train Loss: 0.3261, Val Loss: 0.3546\n",
      "08:26:16 - INFO: Epoch 49/1000\n",
      "08:26:16 - INFO: Train Loss: 0.2346, Val Loss: 0.1802\n",
      "08:26:16 - INFO: Epoch 50/1000\n",
      "08:26:16 - INFO: Train Loss: 0.2164, Val Loss: 0.1747\n",
      "08:26:16 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1747\n",
      "08:26:16 - INFO: Epoch 51/1000\n",
      "08:26:16 - INFO: Train Loss: 0.2262, Val Loss: 0.3025\n",
      "08:26:16 - INFO: Epoch 52/1000\n",
      "08:26:16 - INFO: Train Loss: 0.1841, Val Loss: 0.1908\n",
      "08:26:16 - INFO: Epoch 53/1000\n",
      "08:26:16 - INFO: Train Loss: 0.2197, Val Loss: 0.1840\n",
      "08:26:16 - INFO: Epoch 54/1000\n",
      "08:26:16 - INFO: Train Loss: 0.1916, Val Loss: 0.1950\n",
      "08:26:16 - INFO: Epoch 55/1000\n",
      "08:26:16 - INFO: Train Loss: 0.1735, Val Loss: 0.2163\n",
      "08:26:16 - INFO: Epoch 56/1000\n",
      "08:26:16 - INFO: Train Loss: 0.1682, Val Loss: 0.1718\n",
      "08:26:16 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1718\n",
      "08:26:16 - INFO: Epoch 57/1000\n",
      "08:26:16 - INFO: Train Loss: 0.1719, Val Loss: 0.1804\n",
      "08:26:16 - INFO: Epoch 58/1000\n",
      "08:26:16 - INFO: Train Loss: 0.1770, Val Loss: 0.1523\n",
      "08:26:16 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1523\n",
      "08:26:16 - INFO: Epoch 59/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1583, Val Loss: 0.1572\n",
      "08:26:17 - INFO: Epoch 60/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1574, Val Loss: 0.1768\n",
      "08:26:17 - INFO: Epoch 61/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1954, Val Loss: 0.1458\n",
      "08:26:17 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1458\n",
      "08:26:17 - INFO: Epoch 62/1000\n",
      "08:26:17 - INFO: Train Loss: 0.2249, Val Loss: 0.1544\n",
      "08:26:17 - INFO: Epoch 63/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1523, Val Loss: 0.2552\n",
      "08:26:17 - INFO: Epoch 64/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1478, Val Loss: 0.1527\n",
      "08:26:17 - INFO: Epoch 65/1000\n",
      "08:26:17 - INFO: Train Loss: 0.2869, Val Loss: 0.1396\n",
      "08:26:17 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1396\n",
      "08:26:17 - INFO: Epoch 66/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1484, Val Loss: 0.1543\n",
      "08:26:17 - INFO: Epoch 67/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1604, Val Loss: 0.1978\n",
      "08:26:17 - INFO: Epoch 68/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1638, Val Loss: 0.1879\n",
      "08:26:17 - INFO: Epoch 69/1000\n",
      "08:26:17 - INFO: Train Loss: 0.2249, Val Loss: 0.1548\n",
      "08:26:17 - INFO: Epoch 70/1000\n",
      "08:26:17 - INFO: Train Loss: 0.1954, Val Loss: 0.1476\n",
      "08:26:17 - INFO: Epoch 71/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1587, Val Loss: 0.1492\n",
      "08:26:18 - INFO: Epoch 72/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1903, Val Loss: 0.1691\n",
      "08:26:18 - INFO: Epoch 73/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1393, Val Loss: 0.1374\n",
      "08:26:18 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1374\n",
      "08:26:18 - INFO: Epoch 74/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1497, Val Loss: 0.2806\n",
      "08:26:18 - INFO: Epoch 75/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1570, Val Loss: 0.1331\n",
      "08:26:18 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1331\n",
      "08:26:18 - INFO: Epoch 76/1000\n",
      "08:26:18 - INFO: Train Loss: 0.2175, Val Loss: 0.1339\n",
      "08:26:18 - INFO: Epoch 77/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1350, Val Loss: 0.1388\n",
      "08:26:18 - INFO: Epoch 78/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1914, Val Loss: 0.1455\n",
      "08:26:18 - INFO: Epoch 79/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1689, Val Loss: 0.1390\n",
      "08:26:18 - INFO: Epoch 80/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1419, Val Loss: 0.1426\n",
      "08:26:18 - INFO: Epoch 81/1000\n",
      "08:26:18 - INFO: Train Loss: 0.1529, Val Loss: 0.1530\n",
      "08:26:18 - INFO: Epoch 82/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1797, Val Loss: 0.1248\n",
      "08:26:19 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1248\n",
      "08:26:19 - INFO: Epoch 83/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1346, Val Loss: 0.2351\n",
      "08:26:19 - INFO: Epoch 84/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1754, Val Loss: 0.5253\n",
      "08:26:19 - INFO: Epoch 85/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1319, Val Loss: 0.1299\n",
      "08:26:19 - INFO: Epoch 86/1000\n",
      "08:26:19 - INFO: Train Loss: 0.2102, Val Loss: 0.1690\n",
      "08:26:19 - INFO: Epoch 87/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1775, Val Loss: 0.4350\n",
      "08:26:19 - INFO: Epoch 88/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1519, Val Loss: 0.1332\n",
      "08:26:19 - INFO: Epoch 89/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1339, Val Loss: 0.5350\n",
      "08:26:19 - INFO: Epoch 90/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1416, Val Loss: 0.1411\n",
      "08:26:19 - INFO: Epoch 91/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1263, Val Loss: 0.1633\n",
      "08:26:19 - INFO: Epoch 92/1000\n",
      "08:26:19 - INFO: Train Loss: 0.1951, Val Loss: 0.1203\n",
      "08:26:19 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1203\n",
      "08:26:19 - INFO: Epoch 93/1000\n",
      "08:26:20 - INFO: Train Loss: 0.1326, Val Loss: 0.1335\n",
      "08:26:20 - INFO: Epoch 94/1000\n",
      "08:26:20 - INFO: Train Loss: 0.2136, Val Loss: 0.1281\n",
      "08:26:20 - INFO: Epoch 95/1000\n",
      "08:26:20 - INFO: Train Loss: 0.1331, Val Loss: 0.1236\n",
      "08:26:20 - INFO: Epoch 96/1000\n",
      "08:26:20 - INFO: Train Loss: 0.1380, Val Loss: 0.1245\n",
      "08:26:20 - INFO: Epoch 97/1000\n",
      "08:26:20 - INFO: Train Loss: 0.1773, Val Loss: 0.1769\n",
      "08:26:20 - INFO: Epoch 98/1000\n",
      "08:26:20 - INFO: Train Loss: 0.1578, Val Loss: 0.1249\n",
      "08:26:20 - INFO: Epoch 99/1000\n",
      "08:26:20 - INFO: Train Loss: 0.2460, Val Loss: 0.1488\n",
      "08:26:20 - INFO: Epoch 100/1000\n",
      "08:26:20 - INFO: Train Loss: 0.1737, Val Loss: 0.1543\n",
      "08:26:20 - INFO: Epoch 101/1000\n",
      "08:26:20 - INFO: Train Loss: 0.1310, Val Loss: 0.1956\n",
      "08:26:20 - INFO: Sampling 5 new images....\n",
      "999it [00:13, 75.26it/s]\n",
      "08:26:33 - INFO: Sampling 5 new images....\n",
      "999it [00:10, 99.33it/s] \n",
      "08:26:44 - INFO: Epoch 102/1000\n",
      "08:26:44 - INFO: Train Loss: 0.1292, Val Loss: 0.3197\n",
      "08:26:44 - INFO: Epoch 103/1000\n",
      "08:26:44 - INFO: Train Loss: 0.1387, Val Loss: 0.1390\n",
      "08:26:44 - INFO: Epoch 104/1000\n",
      "08:26:44 - INFO: Train Loss: 0.1670, Val Loss: 0.2048\n",
      "08:26:44 - INFO: Epoch 105/1000\n",
      "08:26:44 - INFO: Train Loss: 0.1747, Val Loss: 0.1215\n",
      "08:26:44 - INFO: Epoch 106/1000\n",
      "08:26:44 - INFO: Train Loss: 0.1242, Val Loss: 0.1197\n",
      "08:26:44 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1197\n",
      "08:26:44 - INFO: Epoch 107/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1277, Val Loss: 0.1195\n",
      "08:26:45 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1195\n",
      "08:26:45 - INFO: Epoch 108/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1957, Val Loss: 0.1254\n",
      "08:26:45 - INFO: Epoch 109/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1500, Val Loss: 0.1152\n",
      "08:26:45 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1152\n",
      "08:26:45 - INFO: Epoch 110/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1225, Val Loss: 0.1154\n",
      "08:26:45 - INFO: Epoch 111/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1563, Val Loss: 0.1213\n",
      "08:26:45 - INFO: Epoch 112/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1819, Val Loss: 0.1183\n",
      "08:26:45 - INFO: Epoch 113/1000\n",
      "08:26:45 - INFO: Train Loss: 0.2692, Val Loss: 0.1614\n",
      "08:26:45 - INFO: Epoch 114/1000\n",
      "08:26:45 - INFO: Train Loss: 0.2367, Val Loss: 0.1364\n",
      "08:26:45 - INFO: Epoch 115/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1338, Val Loss: 0.1257\n",
      "08:26:45 - INFO: Epoch 116/1000\n",
      "08:26:45 - INFO: Train Loss: 0.2164, Val Loss: 0.1948\n",
      "08:26:45 - INFO: Epoch 117/1000\n",
      "08:26:45 - INFO: Train Loss: 0.2244, Val Loss: 0.1158\n",
      "08:26:45 - INFO: Epoch 118/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1778, Val Loss: 0.1449\n",
      "08:26:45 - INFO: Epoch 119/1000\n",
      "08:26:45 - INFO: Train Loss: 0.1291, Val Loss: 0.1249\n",
      "08:26:45 - INFO: Epoch 120/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1440, Val Loss: 0.1285\n",
      "08:26:46 - INFO: Epoch 121/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1178, Val Loss: 0.1100\n",
      "08:26:46 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1100\n",
      "08:26:46 - INFO: Epoch 122/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1344, Val Loss: 0.2412\n",
      "08:26:46 - INFO: Epoch 123/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1619, Val Loss: 0.1130\n",
      "08:26:46 - INFO: Epoch 124/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1667, Val Loss: 0.1678\n",
      "08:26:46 - INFO: Epoch 125/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1551, Val Loss: 0.1157\n",
      "08:26:46 - INFO: Epoch 126/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1968, Val Loss: 0.1147\n",
      "08:26:46 - INFO: Epoch 127/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1166, Val Loss: 0.1253\n",
      "08:26:46 - INFO: Epoch 128/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1429, Val Loss: 0.1139\n",
      "08:26:46 - INFO: Epoch 129/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1093, Val Loss: 0.1599\n",
      "08:26:46 - INFO: Epoch 130/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1164, Val Loss: 0.1265\n",
      "08:26:46 - INFO: Epoch 131/1000\n",
      "08:26:46 - INFO: Train Loss: 0.1287, Val Loss: 0.1129\n",
      "08:26:46 - INFO: Epoch 132/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1080, Val Loss: 0.1057\n",
      "08:26:47 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1057\n",
      "08:26:47 - INFO: Epoch 133/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1038, Val Loss: 0.1258\n",
      "08:26:47 - INFO: Epoch 134/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1232, Val Loss: 0.2370\n",
      "08:26:47 - INFO: Epoch 135/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1128, Val Loss: 0.1071\n",
      "08:26:47 - INFO: Epoch 136/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1213, Val Loss: 0.1012\n",
      "08:26:47 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1012\n",
      "08:26:47 - INFO: Epoch 137/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1041, Val Loss: 0.4827\n",
      "08:26:47 - INFO: Epoch 138/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1204, Val Loss: 0.1001\n",
      "08:26:47 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.1001\n",
      "08:26:47 - INFO: Epoch 139/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1501, Val Loss: 0.1030\n",
      "08:26:47 - INFO: Epoch 140/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1536, Val Loss: 0.1070\n",
      "08:26:47 - INFO: Epoch 141/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1068, Val Loss: 0.1122\n",
      "08:26:47 - INFO: Epoch 142/1000\n",
      "08:26:47 - INFO: Train Loss: 0.1551, Val Loss: 0.1207\n",
      "08:26:47 - INFO: Epoch 143/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1100, Val Loss: 0.1034\n",
      "08:26:48 - INFO: Epoch 144/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1324, Val Loss: 0.1168\n",
      "08:26:48 - INFO: Epoch 145/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1043, Val Loss: 0.1099\n",
      "08:26:48 - INFO: Epoch 146/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1307, Val Loss: 0.1064\n",
      "08:26:48 - INFO: Epoch 147/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1050, Val Loss: 0.1668\n",
      "08:26:48 - INFO: Epoch 148/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1240, Val Loss: 0.2159\n",
      "08:26:48 - INFO: Epoch 149/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1064, Val Loss: 0.1160\n",
      "08:26:48 - INFO: Epoch 150/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1006, Val Loss: 0.1160\n",
      "08:26:48 - INFO: Epoch 151/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1401, Val Loss: 0.1025\n",
      "08:26:48 - INFO: Epoch 152/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1626, Val Loss: 0.1006\n",
      "08:26:48 - INFO: Epoch 153/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1081, Val Loss: 0.1056\n",
      "08:26:48 - INFO: Epoch 154/1000\n",
      "08:26:48 - INFO: Train Loss: 0.1950, Val Loss: 0.1030\n",
      "08:26:48 - INFO: Epoch 155/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1159, Val Loss: 0.0970\n",
      "08:26:49 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0970\n",
      "08:26:49 - INFO: Epoch 156/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1118, Val Loss: 0.3009\n",
      "08:26:49 - INFO: Epoch 157/1000\n",
      "08:26:49 - INFO: Train Loss: 0.0995, Val Loss: 0.0996\n",
      "08:26:49 - INFO: Epoch 158/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1469, Val Loss: 0.1308\n",
      "08:26:49 - INFO: Epoch 159/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1152, Val Loss: 0.1598\n",
      "08:26:49 - INFO: Epoch 160/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1392, Val Loss: 0.1032\n",
      "08:26:49 - INFO: Epoch 161/1000\n",
      "08:26:49 - INFO: Train Loss: 0.0983, Val Loss: 0.1022\n",
      "08:26:49 - INFO: Epoch 162/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1630, Val Loss: 0.1107\n",
      "08:26:49 - INFO: Epoch 163/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1264, Val Loss: 0.1174\n",
      "08:26:49 - INFO: Epoch 164/1000\n",
      "08:26:49 - INFO: Train Loss: 0.1166, Val Loss: 0.1364\n",
      "08:26:49 - INFO: Epoch 165/1000\n",
      "08:26:49 - INFO: Train Loss: 0.2849, Val Loss: 0.0950\n",
      "08:26:49 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0950\n",
      "08:26:49 - INFO: Epoch 166/1000\n",
      "08:26:49 - INFO: Train Loss: 0.0993, Val Loss: 0.0967\n",
      "08:26:49 - INFO: Epoch 167/1000\n",
      "08:26:50 - INFO: Train Loss: 0.1088, Val Loss: 0.1027\n",
      "08:26:50 - INFO: Epoch 168/1000\n",
      "08:26:50 - INFO: Train Loss: 0.1317, Val Loss: 0.0994\n",
      "08:26:50 - INFO: Epoch 169/1000\n",
      "08:26:50 - INFO: Train Loss: 0.0955, Val Loss: 0.1135\n",
      "08:26:50 - INFO: Epoch 170/1000\n",
      "08:26:50 - INFO: Train Loss: 0.0996, Val Loss: 0.1135\n",
      "08:26:50 - INFO: Epoch 171/1000\n",
      "08:26:50 - INFO: Train Loss: 0.1050, Val Loss: 0.1045\n",
      "08:26:50 - INFO: Epoch 172/1000\n",
      "08:26:50 - INFO: Train Loss: 0.1109, Val Loss: 0.4693\n",
      "08:26:50 - INFO: Epoch 173/1000\n",
      "08:26:50 - INFO: Train Loss: 0.0935, Val Loss: 0.1247\n",
      "08:26:50 - INFO: Epoch 174/1000\n",
      "08:26:50 - INFO: Train Loss: 0.1665, Val Loss: 0.0938\n",
      "08:26:50 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0938\n",
      "08:26:50 - INFO: Epoch 175/1000\n",
      "08:26:50 - INFO: Train Loss: 0.1286, Val Loss: 0.0967\n",
      "08:26:50 - INFO: Epoch 176/1000\n",
      "08:26:50 - INFO: Train Loss: 0.0961, Val Loss: 0.1065\n",
      "08:26:50 - INFO: Epoch 177/1000\n",
      "08:26:50 - INFO: Train Loss: 0.0932, Val Loss: 0.0975\n",
      "08:26:50 - INFO: Epoch 178/1000\n",
      "08:26:50 - INFO: Train Loss: 0.1261, Val Loss: 0.0960\n",
      "08:26:50 - INFO: Epoch 179/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1049, Val Loss: 0.1308\n",
      "08:26:51 - INFO: Epoch 180/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1042, Val Loss: 0.1391\n",
      "08:26:51 - INFO: Epoch 181/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1128, Val Loss: 0.1066\n",
      "08:26:51 - INFO: Epoch 182/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1086, Val Loss: 0.1391\n",
      "08:26:51 - INFO: Epoch 183/1000\n",
      "08:26:51 - INFO: Train Loss: 0.0988, Val Loss: 0.1197\n",
      "08:26:51 - INFO: Epoch 184/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1635, Val Loss: 0.1900\n",
      "08:26:51 - INFO: Epoch 185/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1222, Val Loss: 0.1196\n",
      "08:26:51 - INFO: Epoch 186/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1244, Val Loss: 0.0979\n",
      "08:26:51 - INFO: Epoch 187/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1445, Val Loss: 0.1255\n",
      "08:26:51 - INFO: Epoch 188/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1795, Val Loss: 0.0929\n",
      "08:26:51 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0929\n",
      "08:26:51 - INFO: Epoch 189/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1707, Val Loss: 0.1566\n",
      "08:26:51 - INFO: Epoch 190/1000\n",
      "08:26:51 - INFO: Train Loss: 0.0926, Val Loss: 0.1166\n",
      "08:26:51 - INFO: Epoch 191/1000\n",
      "08:26:51 - INFO: Train Loss: 0.1841, Val Loss: 0.3573\n",
      "08:26:51 - INFO: Epoch 192/1000\n",
      "08:26:52 - INFO: Train Loss: 0.0958, Val Loss: 0.1012\n",
      "08:26:52 - INFO: Epoch 193/1000\n",
      "08:26:52 - INFO: Train Loss: 0.1028, Val Loss: 0.1280\n",
      "08:26:52 - INFO: Epoch 194/1000\n",
      "08:26:52 - INFO: Train Loss: 0.0987, Val Loss: 0.0946\n",
      "08:26:52 - INFO: Epoch 195/1000\n",
      "08:26:52 - INFO: Train Loss: 0.1434, Val Loss: 0.1006\n",
      "08:26:52 - INFO: Epoch 196/1000\n",
      "08:26:52 - INFO: Train Loss: 0.0954, Val Loss: 0.1346\n",
      "08:26:52 - INFO: Epoch 197/1000\n",
      "08:26:52 - INFO: Train Loss: 0.1036, Val Loss: 0.0927\n",
      "08:26:52 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0927\n",
      "08:26:52 - INFO: Epoch 198/1000\n",
      "08:26:52 - INFO: Train Loss: 0.1030, Val Loss: 0.1046\n",
      "08:26:52 - INFO: Epoch 199/1000\n",
      "08:26:52 - INFO: Train Loss: 0.1857, Val Loss: 0.0900\n",
      "08:26:52 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0900\n",
      "08:26:52 - INFO: Epoch 200/1000\n",
      "08:26:52 - INFO: Train Loss: 0.1531, Val Loss: 0.3372\n",
      "08:26:52 - INFO: Epoch 201/1000\n",
      "08:26:52 - INFO: Train Loss: 0.2152, Val Loss: 0.1200\n",
      "08:26:52 - INFO: Sampling 5 new images....\n",
      "999it [00:09, 100.72it/s]\n",
      "08:27:02 - INFO: Sampling 5 new images....\n",
      "999it [00:09, 101.50it/s]\n",
      "08:27:13 - INFO: Epoch 202/1000\n",
      "08:27:13 - INFO: Train Loss: 0.1102, Val Loss: 0.1247\n",
      "08:27:13 - INFO: Epoch 203/1000\n",
      "08:27:13 - INFO: Train Loss: 0.1178, Val Loss: 0.1017\n",
      "08:27:13 - INFO: Epoch 204/1000\n",
      "08:27:13 - INFO: Train Loss: 0.0927, Val Loss: 0.1846\n",
      "08:27:13 - INFO: Epoch 205/1000\n",
      "08:27:13 - INFO: Train Loss: 0.1390, Val Loss: 0.4566\n",
      "08:27:13 - INFO: Epoch 206/1000\n",
      "08:27:13 - INFO: Train Loss: 0.0955, Val Loss: 0.1048\n",
      "08:27:13 - INFO: Epoch 207/1000\n",
      "08:27:13 - INFO: Train Loss: 0.0908, Val Loss: 0.1100\n",
      "08:27:13 - INFO: Epoch 208/1000\n",
      "08:27:13 - INFO: Train Loss: 0.0934, Val Loss: 0.1103\n",
      "08:27:13 - INFO: Epoch 209/1000\n",
      "08:27:13 - INFO: Train Loss: 0.0983, Val Loss: 0.0873\n",
      "08:27:13 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0873\n",
      "08:27:13 - INFO: Epoch 210/1000\n",
      "08:27:14 - INFO: Train Loss: 0.0919, Val Loss: 0.1042\n",
      "08:27:14 - INFO: Epoch 211/1000\n",
      "08:27:14 - INFO: Train Loss: 0.0997, Val Loss: 0.0923\n",
      "08:27:14 - INFO: Epoch 212/1000\n",
      "08:27:14 - INFO: Train Loss: 0.1015, Val Loss: 0.0869\n",
      "08:27:14 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0869\n",
      "08:27:14 - INFO: Epoch 213/1000\n",
      "08:27:14 - INFO: Train Loss: 0.0880, Val Loss: 0.3310\n",
      "08:27:14 - INFO: Epoch 214/1000\n",
      "08:27:14 - INFO: Train Loss: 0.1031, Val Loss: 0.2848\n",
      "08:27:14 - INFO: Epoch 215/1000\n",
      "08:27:14 - INFO: Train Loss: 0.1123, Val Loss: 0.1340\n",
      "08:27:14 - INFO: Epoch 216/1000\n",
      "08:27:14 - INFO: Train Loss: 0.0887, Val Loss: 0.4342\n",
      "08:27:14 - INFO: Epoch 217/1000\n",
      "08:27:14 - INFO: Train Loss: 0.2016, Val Loss: 0.0974\n",
      "08:27:14 - INFO: Epoch 218/1000\n",
      "08:27:14 - INFO: Train Loss: 0.1197, Val Loss: 0.3092\n",
      "08:27:14 - INFO: Epoch 219/1000\n",
      "08:27:14 - INFO: Train Loss: 0.1207, Val Loss: 0.1044\n",
      "08:27:14 - INFO: Epoch 220/1000\n",
      "08:27:14 - INFO: Train Loss: 0.1015, Val Loss: 0.0969\n",
      "08:27:14 - INFO: Epoch 221/1000\n",
      "08:27:14 - INFO: Train Loss: 0.0903, Val Loss: 0.1217\n",
      "08:27:14 - INFO: Epoch 222/1000\n",
      "08:27:14 - INFO: Train Loss: 0.1384, Val Loss: 0.0942\n",
      "08:27:14 - INFO: Epoch 223/1000\n",
      "08:27:15 - INFO: Train Loss: 0.1064, Val Loss: 0.1052\n",
      "08:27:15 - INFO: Epoch 224/1000\n",
      "08:27:15 - INFO: Train Loss: 0.0850, Val Loss: 0.0977\n",
      "08:27:15 - INFO: Epoch 225/1000\n",
      "08:27:15 - INFO: Train Loss: 0.0923, Val Loss: 0.1037\n",
      "08:27:15 - INFO: Epoch 226/1000\n",
      "08:27:15 - INFO: Train Loss: 0.1438, Val Loss: 0.1047\n",
      "08:27:15 - INFO: Epoch 227/1000\n",
      "08:27:15 - INFO: Train Loss: 0.0954, Val Loss: 0.1708\n",
      "08:27:15 - INFO: Epoch 228/1000\n",
      "08:27:15 - INFO: Train Loss: 0.1353, Val Loss: 0.0994\n",
      "08:27:15 - INFO: Epoch 229/1000\n",
      "08:27:15 - INFO: Train Loss: 0.1341, Val Loss: 0.0889\n",
      "08:27:15 - INFO: Epoch 230/1000\n",
      "08:27:15 - INFO: Train Loss: 0.1378, Val Loss: 0.1131\n",
      "08:27:15 - INFO: Epoch 231/1000\n",
      "08:27:15 - INFO: Train Loss: 0.1648, Val Loss: 0.2159\n",
      "08:27:15 - INFO: Epoch 232/1000\n",
      "08:27:15 - INFO: Train Loss: 0.2974, Val Loss: 0.0849\n",
      "08:27:15 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0849\n",
      "08:27:15 - INFO: Epoch 233/1000\n",
      "08:27:15 - INFO: Train Loss: 0.0947, Val Loss: 0.0919\n",
      "08:27:15 - INFO: Epoch 234/1000\n",
      "08:27:15 - INFO: Train Loss: 0.0902, Val Loss: 0.0881\n",
      "08:27:15 - INFO: Epoch 235/1000\n",
      "08:27:15 - INFO: Train Loss: 0.0983, Val Loss: 0.0885\n",
      "08:27:15 - INFO: Epoch 236/1000\n",
      "08:27:15 - INFO: Train Loss: 0.1659, Val Loss: 0.2881\n",
      "08:27:15 - INFO: Epoch 237/1000\n",
      "08:27:16 - INFO: Train Loss: 0.2314, Val Loss: 0.0904\n",
      "08:27:16 - INFO: Epoch 238/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0996, Val Loss: 0.0859\n",
      "08:27:16 - INFO: Epoch 239/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0904, Val Loss: 0.0858\n",
      "08:27:16 - INFO: Epoch 240/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0838, Val Loss: 0.0841\n",
      "08:27:16 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0841\n",
      "08:27:16 - INFO: Epoch 241/1000\n",
      "08:27:16 - INFO: Train Loss: 0.1627, Val Loss: 0.0903\n",
      "08:27:16 - INFO: Epoch 242/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0899, Val Loss: 0.1702\n",
      "08:27:16 - INFO: Epoch 243/1000\n",
      "08:27:16 - INFO: Train Loss: 0.1038, Val Loss: 0.3041\n",
      "08:27:16 - INFO: Epoch 244/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0878, Val Loss: 0.0947\n",
      "08:27:16 - INFO: Epoch 245/1000\n",
      "08:27:16 - INFO: Train Loss: 0.1270, Val Loss: 0.1762\n",
      "08:27:16 - INFO: Epoch 246/1000\n",
      "08:27:16 - INFO: Train Loss: 0.1127, Val Loss: 0.1402\n",
      "08:27:16 - INFO: Epoch 247/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0992, Val Loss: 0.0900\n",
      "08:27:16 - INFO: Epoch 248/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0893, Val Loss: 0.0812\n",
      "08:27:16 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0812\n",
      "08:27:16 - INFO: Epoch 249/1000\n",
      "08:27:16 - INFO: Train Loss: 0.0909, Val Loss: 0.0812\n",
      "08:27:16 - INFO: Epoch 250/1000\n",
      "08:27:17 - INFO: Train Loss: 0.1168, Val Loss: 0.1478\n",
      "08:27:17 - INFO: Epoch 251/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0852, Val Loss: 0.0936\n",
      "08:27:17 - INFO: Epoch 252/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0911, Val Loss: 0.0815\n",
      "08:27:17 - INFO: Epoch 253/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0917, Val Loss: 0.0922\n",
      "08:27:17 - INFO: Epoch 254/1000\n",
      "08:27:17 - INFO: Train Loss: 0.1074, Val Loss: 0.0863\n",
      "08:27:17 - INFO: Epoch 255/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0827, Val Loss: 0.1807\n",
      "08:27:17 - INFO: Epoch 256/1000\n",
      "08:27:17 - INFO: Train Loss: 0.2049, Val Loss: 0.1498\n",
      "08:27:17 - INFO: Epoch 257/1000\n",
      "08:27:17 - INFO: Train Loss: 0.1063, Val Loss: 0.0808\n",
      "08:27:17 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0808\n",
      "08:27:17 - INFO: Epoch 258/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0848, Val Loss: 0.1218\n",
      "08:27:17 - INFO: Epoch 259/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0887, Val Loss: 0.0833\n",
      "08:27:17 - INFO: Epoch 260/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0883, Val Loss: 0.0865\n",
      "08:27:17 - INFO: Epoch 261/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0865, Val Loss: 0.3910\n",
      "08:27:17 - INFO: Epoch 262/1000\n",
      "08:27:17 - INFO: Train Loss: 0.0857, Val Loss: 0.1033\n",
      "08:27:17 - INFO: Epoch 263/1000\n",
      "08:27:18 - INFO: Train Loss: 0.1872, Val Loss: 0.0877\n",
      "08:27:18 - INFO: Epoch 264/1000\n",
      "08:27:18 - INFO: Train Loss: 0.1227, Val Loss: 0.0846\n",
      "08:27:18 - INFO: Epoch 265/1000\n",
      "08:27:18 - INFO: Train Loss: 0.1916, Val Loss: 0.1124\n",
      "08:27:18 - INFO: Epoch 266/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0966, Val Loss: 0.0855\n",
      "08:27:18 - INFO: Epoch 267/1000\n",
      "08:27:18 - INFO: Train Loss: 0.1576, Val Loss: 0.0934\n",
      "08:27:18 - INFO: Epoch 268/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0982, Val Loss: 0.1336\n",
      "08:27:18 - INFO: Epoch 269/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0908, Val Loss: 0.1086\n",
      "08:27:18 - INFO: Epoch 270/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0884, Val Loss: 0.0847\n",
      "08:27:18 - INFO: Epoch 271/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0890, Val Loss: 0.1082\n",
      "08:27:18 - INFO: Epoch 272/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0809, Val Loss: 0.0873\n",
      "08:27:18 - INFO: Epoch 273/1000\n",
      "08:27:18 - INFO: Train Loss: 0.1017, Val Loss: 0.0871\n",
      "08:27:18 - INFO: Epoch 274/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0823, Val Loss: 0.0959\n",
      "08:27:18 - INFO: Epoch 275/1000\n",
      "08:27:18 - INFO: Train Loss: 0.0863, Val Loss: 0.1536\n",
      "08:27:18 - INFO: Epoch 276/1000\n",
      "08:27:19 - INFO: Train Loss: 0.0808, Val Loss: 0.0972\n",
      "08:27:19 - INFO: Epoch 277/1000\n",
      "08:27:19 - INFO: Train Loss: 0.1290, Val Loss: 0.1264\n",
      "08:27:19 - INFO: Epoch 278/1000\n",
      "08:27:19 - INFO: Train Loss: 0.0852, Val Loss: 0.1492\n",
      "08:27:19 - INFO: Epoch 279/1000\n",
      "08:27:19 - INFO: Train Loss: 0.1942, Val Loss: 0.0770\n",
      "08:27:19 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0770\n",
      "08:27:19 - INFO: Epoch 280/1000\n",
      "08:27:19 - INFO: Train Loss: 0.1088, Val Loss: 0.2835\n",
      "08:27:19 - INFO: Epoch 281/1000\n",
      "08:27:19 - INFO: Train Loss: 0.0831, Val Loss: 0.0860\n",
      "08:27:19 - INFO: Epoch 282/1000\n",
      "08:27:19 - INFO: Train Loss: 0.2288, Val Loss: 0.0789\n",
      "08:27:19 - INFO: Epoch 283/1000\n",
      "08:27:19 - INFO: Train Loss: 0.0850, Val Loss: 0.2547\n",
      "08:27:19 - INFO: Epoch 284/1000\n",
      "08:27:19 - INFO: Train Loss: 0.0827, Val Loss: 0.0826\n",
      "08:27:19 - INFO: Epoch 285/1000\n",
      "08:27:19 - INFO: Train Loss: 0.1492, Val Loss: 0.3998\n",
      "08:27:19 - INFO: Epoch 286/1000\n",
      "08:27:19 - INFO: Train Loss: 0.1088, Val Loss: 0.0792\n",
      "08:27:19 - INFO: Epoch 287/1000\n",
      "08:27:19 - INFO: Train Loss: 0.0990, Val Loss: 0.0752\n",
      "08:27:19 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0752\n",
      "08:27:19 - INFO: Epoch 288/1000\n",
      "08:27:19 - INFO: Train Loss: 0.0816, Val Loss: 0.1022\n",
      "08:27:19 - INFO: Epoch 289/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0888, Val Loss: 0.1272\n",
      "08:27:20 - INFO: Epoch 290/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0832, Val Loss: 0.2016\n",
      "08:27:20 - INFO: Epoch 291/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0825, Val Loss: 0.0921\n",
      "08:27:20 - INFO: Epoch 292/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0890, Val Loss: 0.0824\n",
      "08:27:20 - INFO: Epoch 293/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0870, Val Loss: 0.0868\n",
      "08:27:20 - INFO: Epoch 294/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0750, Val Loss: 0.0917\n",
      "08:27:20 - INFO: Epoch 295/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0917, Val Loss: 0.0864\n",
      "08:27:20 - INFO: Epoch 296/1000\n",
      "08:27:20 - INFO: Train Loss: 0.1399, Val Loss: 0.2693\n",
      "08:27:20 - INFO: Epoch 297/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0763, Val Loss: 0.0771\n",
      "08:27:20 - INFO: Epoch 298/1000\n",
      "08:27:20 - INFO: Train Loss: 0.1090, Val Loss: 0.2565\n",
      "08:27:20 - INFO: Epoch 299/1000\n",
      "08:27:20 - INFO: Train Loss: 0.1040, Val Loss: 0.1888\n",
      "08:27:20 - INFO: Epoch 300/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0766, Val Loss: 0.0744\n",
      "08:27:20 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0744\n",
      "08:27:20 - INFO: Epoch 301/1000\n",
      "08:27:20 - INFO: Train Loss: 0.0785, Val Loss: 0.1070\n",
      "08:27:20 - INFO: Sampling 5 new images....\n",
      "999it [00:11, 84.60it/s] \n",
      "08:27:32 - INFO: Sampling 5 new images....\n",
      "999it [00:12, 79.30it/s]\n",
      "08:27:46 - INFO: Epoch 302/1000\n",
      "08:27:46 - INFO: Train Loss: 0.1006, Val Loss: 0.0845\n",
      "08:27:46 - INFO: Epoch 303/1000\n",
      "08:27:46 - INFO: Train Loss: 0.0793, Val Loss: 0.1024\n",
      "08:27:46 - INFO: Epoch 304/1000\n",
      "08:27:46 - INFO: Train Loss: 0.0889, Val Loss: 0.0974\n",
      "08:27:46 - INFO: Epoch 305/1000\n",
      "08:27:46 - INFO: Train Loss: 0.1157, Val Loss: 0.0879\n",
      "08:27:46 - INFO: Epoch 306/1000\n",
      "08:27:46 - INFO: Train Loss: 0.2797, Val Loss: 0.0784\n",
      "08:27:46 - INFO: Epoch 307/1000\n",
      "08:27:46 - INFO: Train Loss: 0.0989, Val Loss: 0.0975\n",
      "08:27:46 - INFO: Epoch 308/1000\n",
      "08:27:46 - INFO: Train Loss: 0.2080, Val Loss: 0.0930\n",
      "08:27:46 - INFO: Epoch 309/1000\n",
      "08:27:46 - INFO: Train Loss: 0.0945, Val Loss: 0.1735\n",
      "08:27:46 - INFO: Epoch 310/1000\n",
      "08:27:46 - INFO: Train Loss: 0.0785, Val Loss: 0.0864\n",
      "08:27:46 - INFO: Epoch 311/1000\n",
      "08:27:47 - INFO: Train Loss: 0.1046, Val Loss: 0.1226\n",
      "08:27:47 - INFO: Epoch 312/1000\n",
      "08:27:47 - INFO: Train Loss: 0.1114, Val Loss: 0.0887\n",
      "08:27:47 - INFO: Epoch 313/1000\n",
      "08:27:47 - INFO: Train Loss: 0.1045, Val Loss: 0.1828\n",
      "08:27:47 - INFO: Epoch 314/1000\n",
      "08:27:47 - INFO: Train Loss: 0.0804, Val Loss: 0.2381\n",
      "08:27:47 - INFO: Epoch 315/1000\n",
      "08:27:47 - INFO: Train Loss: 0.0775, Val Loss: 0.1316\n",
      "08:27:47 - INFO: Epoch 316/1000\n",
      "08:27:47 - INFO: Train Loss: 0.1032, Val Loss: 0.0858\n",
      "08:27:47 - INFO: Epoch 317/1000\n",
      "08:27:47 - INFO: Train Loss: 0.1043, Val Loss: 0.3818\n",
      "08:27:47 - INFO: Epoch 318/1000\n",
      "08:27:47 - INFO: Train Loss: 0.0884, Val Loss: 0.0770\n",
      "08:27:47 - INFO: Epoch 319/1000\n",
      "08:27:47 - INFO: Train Loss: 0.1836, Val Loss: 0.0761\n",
      "08:27:47 - INFO: Epoch 320/1000\n",
      "08:27:47 - INFO: Train Loss: 0.0848, Val Loss: 0.0750\n",
      "08:27:47 - INFO: Epoch 321/1000\n",
      "08:27:48 - INFO: Train Loss: 0.1477, Val Loss: 0.0879\n",
      "08:27:48 - INFO: Epoch 322/1000\n",
      "08:27:48 - INFO: Train Loss: 0.0802, Val Loss: 0.0838\n",
      "08:27:48 - INFO: Epoch 323/1000\n",
      "08:27:48 - INFO: Train Loss: 0.0964, Val Loss: 0.1213\n",
      "08:27:48 - INFO: Epoch 324/1000\n",
      "08:27:48 - INFO: Train Loss: 0.0908, Val Loss: 0.3516\n",
      "08:27:48 - INFO: Epoch 325/1000\n",
      "08:27:48 - INFO: Train Loss: 0.0734, Val Loss: 0.0957\n",
      "08:27:48 - INFO: Epoch 326/1000\n",
      "08:27:48 - INFO: Train Loss: 0.1587, Val Loss: 0.1086\n",
      "08:27:48 - INFO: Epoch 327/1000\n",
      "08:27:48 - INFO: Train Loss: 0.1438, Val Loss: 0.1004\n",
      "08:27:48 - INFO: Epoch 328/1000\n",
      "08:27:48 - INFO: Train Loss: 0.0760, Val Loss: 0.0815\n",
      "08:27:48 - INFO: Epoch 329/1000\n",
      "08:27:48 - INFO: Train Loss: 0.0908, Val Loss: 0.0782\n",
      "08:27:48 - INFO: Epoch 330/1000\n",
      "08:27:48 - INFO: Train Loss: 0.1317, Val Loss: 0.1400\n",
      "08:27:48 - INFO: Epoch 331/1000\n",
      "08:27:48 - INFO: Train Loss: 0.0833, Val Loss: 0.0741\n",
      "08:27:48 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0741\n",
      "08:27:48 - INFO: Epoch 332/1000\n",
      "08:27:49 - INFO: Train Loss: 0.0891, Val Loss: 0.0913\n",
      "08:27:49 - INFO: Epoch 333/1000\n",
      "08:27:49 - INFO: Train Loss: 0.1175, Val Loss: 0.0979\n",
      "08:27:49 - INFO: Epoch 334/1000\n",
      "08:27:49 - INFO: Train Loss: 0.0732, Val Loss: 0.1063\n",
      "08:27:49 - INFO: Epoch 335/1000\n",
      "08:27:49 - INFO: Train Loss: 0.1731, Val Loss: 0.0939\n",
      "08:27:49 - INFO: Epoch 336/1000\n",
      "08:27:49 - INFO: Train Loss: 0.0863, Val Loss: 0.0813\n",
      "08:27:49 - INFO: Epoch 337/1000\n",
      "08:27:49 - INFO: Train Loss: 0.1076, Val Loss: 0.0736\n",
      "08:27:49 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0736\n",
      "08:27:49 - INFO: Epoch 338/1000\n",
      "08:27:49 - INFO: Train Loss: 0.1475, Val Loss: 0.0809\n",
      "08:27:49 - INFO: Epoch 339/1000\n",
      "08:27:49 - INFO: Train Loss: 0.0797, Val Loss: 0.0775\n",
      "08:27:49 - INFO: Epoch 340/1000\n",
      "08:27:49 - INFO: Train Loss: 0.0706, Val Loss: 0.0722\n",
      "08:27:49 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0722\n",
      "08:27:49 - INFO: Epoch 341/1000\n",
      "08:27:49 - INFO: Train Loss: 0.1196, Val Loss: 0.0713\n",
      "08:27:49 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0713\n",
      "08:27:49 - INFO: Epoch 342/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0824, Val Loss: 0.0758\n",
      "08:27:50 - INFO: Epoch 343/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0870, Val Loss: 0.0759\n",
      "08:27:50 - INFO: Epoch 344/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0935, Val Loss: 0.0734\n",
      "08:27:50 - INFO: Epoch 345/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0920, Val Loss: 0.0814\n",
      "08:27:50 - INFO: Epoch 346/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0715, Val Loss: 0.0806\n",
      "08:27:50 - INFO: Epoch 347/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0705, Val Loss: 0.0814\n",
      "08:27:50 - INFO: Epoch 348/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0763, Val Loss: 0.0712\n",
      "08:27:50 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0712\n",
      "08:27:50 - INFO: Epoch 349/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0980, Val Loss: 0.0788\n",
      "08:27:50 - INFO: Epoch 350/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0739, Val Loss: 0.0822\n",
      "08:27:50 - INFO: Epoch 351/1000\n",
      "08:27:50 - INFO: Train Loss: 0.0784, Val Loss: 0.1692\n",
      "08:27:50 - INFO: Epoch 352/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0954, Val Loss: 0.0964\n",
      "08:27:51 - INFO: Epoch 353/1000\n",
      "08:27:51 - INFO: Train Loss: 0.1462, Val Loss: 0.0945\n",
      "08:27:51 - INFO: Epoch 354/1000\n",
      "08:27:51 - INFO: Train Loss: 0.1870, Val Loss: 0.0948\n",
      "08:27:51 - INFO: Epoch 355/1000\n",
      "08:27:51 - INFO: Train Loss: 0.1134, Val Loss: 0.2672\n",
      "08:27:51 - INFO: Epoch 356/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0810, Val Loss: 0.0951\n",
      "08:27:51 - INFO: Epoch 357/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0765, Val Loss: 0.0750\n",
      "08:27:51 - INFO: Epoch 358/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0754, Val Loss: 0.0761\n",
      "08:27:51 - INFO: Epoch 359/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0766, Val Loss: 0.0900\n",
      "08:27:51 - INFO: Epoch 360/1000\n",
      "08:27:51 - INFO: Train Loss: 0.1641, Val Loss: 0.0767\n",
      "08:27:51 - INFO: Epoch 361/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0735, Val Loss: 0.1537\n",
      "08:27:51 - INFO: Epoch 362/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0745, Val Loss: 0.3100\n",
      "08:27:51 - INFO: Epoch 363/1000\n",
      "08:27:51 - INFO: Train Loss: 0.0739, Val Loss: 0.0753\n",
      "08:27:51 - INFO: Epoch 364/1000\n",
      "08:27:52 - INFO: Train Loss: 0.1488, Val Loss: 0.1198\n",
      "08:27:52 - INFO: Epoch 365/1000\n",
      "08:27:52 - INFO: Train Loss: 0.1078, Val Loss: 0.0738\n",
      "08:27:52 - INFO: Epoch 366/1000\n",
      "08:27:52 - INFO: Train Loss: 0.0756, Val Loss: 0.0826\n",
      "08:27:52 - INFO: Epoch 367/1000\n",
      "08:27:52 - INFO: Train Loss: 0.0800, Val Loss: 0.0781\n",
      "08:27:52 - INFO: Epoch 368/1000\n",
      "08:27:52 - INFO: Train Loss: 0.0725, Val Loss: 0.0817\n",
      "08:27:52 - INFO: Epoch 369/1000\n",
      "08:27:52 - INFO: Train Loss: 0.0722, Val Loss: 0.2737\n",
      "08:27:52 - INFO: Epoch 370/1000\n",
      "08:27:52 - INFO: Train Loss: 0.1147, Val Loss: 0.0803\n",
      "08:27:52 - INFO: Epoch 371/1000\n",
      "08:27:52 - INFO: Train Loss: 0.0886, Val Loss: 0.0866\n",
      "08:27:52 - INFO: Epoch 372/1000\n",
      "08:27:52 - INFO: Train Loss: 0.1147, Val Loss: 0.0922\n",
      "08:27:52 - INFO: Epoch 373/1000\n",
      "08:27:52 - INFO: Train Loss: 0.0901, Val Loss: 0.0704\n",
      "08:27:52 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0704\n",
      "08:27:52 - INFO: Epoch 374/1000\n",
      "08:27:52 - INFO: Train Loss: 0.0708, Val Loss: 0.1871\n",
      "08:27:52 - INFO: Epoch 375/1000\n",
      "08:27:53 - INFO: Train Loss: 0.0821, Val Loss: 0.0913\n",
      "08:27:53 - INFO: Epoch 376/1000\n",
      "08:27:53 - INFO: Train Loss: 0.0959, Val Loss: 0.1470\n",
      "08:27:53 - INFO: Epoch 377/1000\n",
      "08:27:53 - INFO: Train Loss: 0.0960, Val Loss: 0.1055\n",
      "08:27:53 - INFO: Epoch 378/1000\n",
      "08:27:53 - INFO: Train Loss: 0.0730, Val Loss: 0.0966\n",
      "08:27:53 - INFO: Epoch 379/1000\n",
      "08:27:53 - INFO: Train Loss: 0.0774, Val Loss: 0.0767\n",
      "08:27:53 - INFO: Epoch 380/1000\n",
      "08:27:53 - INFO: Train Loss: 0.1551, Val Loss: 0.0891\n",
      "08:27:53 - INFO: Epoch 381/1000\n",
      "08:27:53 - INFO: Train Loss: 0.0851, Val Loss: 0.0737\n",
      "08:27:53 - INFO: Epoch 382/1000\n",
      "08:27:53 - INFO: Train Loss: 0.0728, Val Loss: 0.1093\n",
      "08:27:53 - INFO: Epoch 383/1000\n",
      "08:27:53 - INFO: Train Loss: 0.1984, Val Loss: 0.0685\n",
      "08:27:53 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0685\n",
      "08:27:53 - INFO: Epoch 384/1000\n",
      "08:27:53 - INFO: Train Loss: 0.2476, Val Loss: 0.0742\n",
      "08:27:53 - INFO: Epoch 385/1000\n",
      "08:27:53 - INFO: Train Loss: 0.2107, Val Loss: 0.1779\n",
      "08:27:53 - INFO: Epoch 386/1000\n",
      "08:27:54 - INFO: Train Loss: 0.1244, Val Loss: 0.0750\n",
      "08:27:54 - INFO: Epoch 387/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0734, Val Loss: 0.2104\n",
      "08:27:54 - INFO: Epoch 388/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0772, Val Loss: 0.5020\n",
      "08:27:54 - INFO: Epoch 389/1000\n",
      "08:27:54 - INFO: Train Loss: 0.1069, Val Loss: 0.2376\n",
      "08:27:54 - INFO: Epoch 390/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0838, Val Loss: 0.0733\n",
      "08:27:54 - INFO: Epoch 391/1000\n",
      "08:27:54 - INFO: Train Loss: 0.1509, Val Loss: 0.0754\n",
      "08:27:54 - INFO: Epoch 392/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0873, Val Loss: 0.1430\n",
      "08:27:54 - INFO: Epoch 393/1000\n",
      "08:27:54 - INFO: Train Loss: 0.1018, Val Loss: 0.0776\n",
      "08:27:54 - INFO: Epoch 394/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0909, Val Loss: 0.0898\n",
      "08:27:54 - INFO: Epoch 395/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0908, Val Loss: 0.0828\n",
      "08:27:54 - INFO: Epoch 396/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0983, Val Loss: 0.0766\n",
      "08:27:54 - INFO: Epoch 397/1000\n",
      "08:27:54 - INFO: Train Loss: 0.0737, Val Loss: 0.0955\n",
      "08:27:54 - INFO: Epoch 398/1000\n",
      "08:27:55 - INFO: Train Loss: 0.1577, Val Loss: 0.0720\n",
      "08:27:55 - INFO: Epoch 399/1000\n",
      "08:27:55 - INFO: Train Loss: 0.0865, Val Loss: 0.0858\n",
      "08:27:55 - INFO: Epoch 400/1000\n",
      "08:27:55 - INFO: Train Loss: 0.0860, Val Loss: 0.1026\n",
      "08:27:55 - INFO: Epoch 401/1000\n",
      "08:27:55 - INFO: Train Loss: 0.0745, Val Loss: 0.0792\n",
      "08:27:55 - INFO: Sampling 5 new images....\n",
      "999it [00:12, 79.85it/s]\n",
      "08:28:07 - INFO: Sampling 5 new images....\n",
      "999it [00:10, 94.44it/s] \n",
      "08:28:18 - INFO: Epoch 402/1000\n",
      "08:28:19 - INFO: Train Loss: 0.0823, Val Loss: 0.1042\n",
      "08:28:19 - INFO: Epoch 403/1000\n",
      "08:28:19 - INFO: Train Loss: 0.0773, Val Loss: 0.0725\n",
      "08:28:19 - INFO: Epoch 404/1000\n",
      "08:28:19 - INFO: Train Loss: 0.1064, Val Loss: 0.0709\n",
      "08:28:19 - INFO: Epoch 405/1000\n",
      "08:28:19 - INFO: Train Loss: 0.0738, Val Loss: 0.0716\n",
      "08:28:19 - INFO: Epoch 406/1000\n",
      "08:28:19 - INFO: Train Loss: 0.0842, Val Loss: 0.0708\n",
      "08:28:19 - INFO: Epoch 407/1000\n",
      "08:28:19 - INFO: Train Loss: 0.0830, Val Loss: 0.0752\n",
      "08:28:19 - INFO: Epoch 408/1000\n",
      "08:28:19 - INFO: Train Loss: 0.2619, Val Loss: 0.0739\n",
      "08:28:19 - INFO: Epoch 409/1000\n",
      "08:28:19 - INFO: Train Loss: 0.1770, Val Loss: 0.0761\n",
      "08:28:19 - INFO: Epoch 410/1000\n",
      "08:28:19 - INFO: Train Loss: 0.0723, Val Loss: 0.0817\n",
      "08:28:19 - INFO: Epoch 411/1000\n",
      "08:28:19 - INFO: Train Loss: 0.0828, Val Loss: 0.0887\n",
      "08:28:19 - INFO: Epoch 412/1000\n",
      "08:28:19 - INFO: Train Loss: 0.1211, Val Loss: 0.0777\n",
      "08:28:19 - INFO: Epoch 413/1000\n",
      "08:28:20 - INFO: Train Loss: 0.1428, Val Loss: 0.0753\n",
      "08:28:20 - INFO: Epoch 414/1000\n",
      "08:28:20 - INFO: Train Loss: 0.0806, Val Loss: 0.0698\n",
      "08:28:20 - INFO: Epoch 415/1000\n",
      "08:28:20 - INFO: Train Loss: 0.1040, Val Loss: 0.0710\n",
      "08:28:20 - INFO: Epoch 416/1000\n",
      "08:28:20 - INFO: Train Loss: 0.0743, Val Loss: 0.0791\n",
      "08:28:20 - INFO: Epoch 417/1000\n",
      "08:28:20 - INFO: Train Loss: 0.1040, Val Loss: 0.1450\n",
      "08:28:20 - INFO: Epoch 418/1000\n",
      "08:28:20 - INFO: Train Loss: 0.0961, Val Loss: 0.0760\n",
      "08:28:20 - INFO: Epoch 419/1000\n",
      "08:28:20 - INFO: Train Loss: 0.0697, Val Loss: 0.0718\n",
      "08:28:20 - INFO: Epoch 420/1000\n",
      "08:28:20 - INFO: Train Loss: 0.0697, Val Loss: 0.0934\n",
      "08:28:20 - INFO: Epoch 421/1000\n",
      "08:28:20 - INFO: Train Loss: 0.0964, Val Loss: 0.0692\n",
      "08:28:20 - INFO: Epoch 422/1000\n",
      "08:28:20 - INFO: Train Loss: 0.1227, Val Loss: 0.3799\n",
      "08:28:20 - INFO: Epoch 423/1000\n",
      "08:28:20 - INFO: Train Loss: 0.0978, Val Loss: 0.0814\n",
      "08:28:20 - INFO: Epoch 424/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0849, Val Loss: 0.0719\n",
      "08:28:21 - INFO: Epoch 425/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0696, Val Loss: 0.0695\n",
      "08:28:21 - INFO: Epoch 426/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0698, Val Loss: 0.0677\n",
      "08:28:21 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0677\n",
      "08:28:21 - INFO: Epoch 427/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0817, Val Loss: 0.0680\n",
      "08:28:21 - INFO: Epoch 428/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0763, Val Loss: 0.0825\n",
      "08:28:21 - INFO: Epoch 429/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0751, Val Loss: 0.2070\n",
      "08:28:21 - INFO: Epoch 430/1000\n",
      "08:28:21 - INFO: Train Loss: 0.1286, Val Loss: 0.1134\n",
      "08:28:21 - INFO: Epoch 431/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0737, Val Loss: 0.0773\n",
      "08:28:21 - INFO: Epoch 432/1000\n",
      "08:28:21 - INFO: Train Loss: 0.1089, Val Loss: 0.0963\n",
      "08:28:21 - INFO: Epoch 433/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0810, Val Loss: 0.0856\n",
      "08:28:21 - INFO: Epoch 434/1000\n",
      "08:28:21 - INFO: Train Loss: 0.0749, Val Loss: 0.0690\n",
      "08:28:21 - INFO: Epoch 435/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0776, Val Loss: 0.1162\n",
      "08:28:22 - INFO: Epoch 436/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0729, Val Loss: 0.0816\n",
      "08:28:22 - INFO: Epoch 437/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0821, Val Loss: 0.0894\n",
      "08:28:22 - INFO: Epoch 438/1000\n",
      "08:28:22 - INFO: Train Loss: 0.1027, Val Loss: 0.0888\n",
      "08:28:22 - INFO: Epoch 439/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0686, Val Loss: 0.0732\n",
      "08:28:22 - INFO: Epoch 440/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0733, Val Loss: 0.0944\n",
      "08:28:22 - INFO: Epoch 441/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0720, Val Loss: 0.1037\n",
      "08:28:22 - INFO: Epoch 442/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0691, Val Loss: 0.0708\n",
      "08:28:22 - INFO: Epoch 443/1000\n",
      "08:28:22 - INFO: Train Loss: 0.1409, Val Loss: 0.0888\n",
      "08:28:22 - INFO: Epoch 444/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0807, Val Loss: 0.1128\n",
      "08:28:22 - INFO: Epoch 445/1000\n",
      "08:28:22 - INFO: Train Loss: 0.0739, Val Loss: 0.1470\n",
      "08:28:22 - INFO: Epoch 446/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0732, Val Loss: 0.0648\n",
      "08:28:23 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0648\n",
      "08:28:23 - INFO: Epoch 447/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0873, Val Loss: 0.0678\n",
      "08:28:23 - INFO: Epoch 448/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0684, Val Loss: 0.0709\n",
      "08:28:23 - INFO: Epoch 449/1000\n",
      "08:28:23 - INFO: Train Loss: 0.1117, Val Loss: 0.0771\n",
      "08:28:23 - INFO: Epoch 450/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0803, Val Loss: 0.0668\n",
      "08:28:23 - INFO: Epoch 451/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0660, Val Loss: 0.1004\n",
      "08:28:23 - INFO: Epoch 452/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0736, Val Loss: 0.0863\n",
      "08:28:23 - INFO: Epoch 453/1000\n",
      "08:28:23 - INFO: Train Loss: 0.1101, Val Loss: 0.0695\n",
      "08:28:23 - INFO: Epoch 454/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0718, Val Loss: 0.0974\n",
      "08:28:23 - INFO: Epoch 455/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0995, Val Loss: 0.0675\n",
      "08:28:23 - INFO: Epoch 456/1000\n",
      "08:28:23 - INFO: Train Loss: 0.0849, Val Loss: 0.0717\n",
      "08:28:23 - INFO: Epoch 457/1000\n",
      "08:28:23 - INFO: Train Loss: 0.2076, Val Loss: 0.0925\n",
      "08:28:24 - INFO: Epoch 458/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0699, Val Loss: 0.0893\n",
      "08:28:24 - INFO: Epoch 459/1000\n",
      "08:28:24 - INFO: Train Loss: 0.1260, Val Loss: 0.1253\n",
      "08:28:24 - INFO: Epoch 460/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0955, Val Loss: 0.0757\n",
      "08:28:24 - INFO: Epoch 461/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0848, Val Loss: 0.1725\n",
      "08:28:24 - INFO: Epoch 462/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0845, Val Loss: 0.1535\n",
      "08:28:24 - INFO: Epoch 463/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0841, Val Loss: 0.0749\n",
      "08:28:24 - INFO: Epoch 464/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0691, Val Loss: 0.0734\n",
      "08:28:24 - INFO: Epoch 465/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0775, Val Loss: 0.0705\n",
      "08:28:24 - INFO: Epoch 466/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0943, Val Loss: 0.1324\n",
      "08:28:24 - INFO: Epoch 467/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0777, Val Loss: 0.0798\n",
      "08:28:24 - INFO: Epoch 468/1000\n",
      "08:28:24 - INFO: Train Loss: 0.0792, Val Loss: 0.0829\n",
      "08:28:24 - INFO: Epoch 469/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0658, Val Loss: 0.1111\n",
      "08:28:25 - INFO: Epoch 470/1000\n",
      "08:28:25 - INFO: Train Loss: 0.1178, Val Loss: 0.1031\n",
      "08:28:25 - INFO: Epoch 471/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0785, Val Loss: 0.0815\n",
      "08:28:25 - INFO: Epoch 472/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0797, Val Loss: 0.0855\n",
      "08:28:25 - INFO: Epoch 473/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0648, Val Loss: 0.0661\n",
      "08:28:25 - INFO: Epoch 474/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0651, Val Loss: 0.0717\n",
      "08:28:25 - INFO: Epoch 475/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0846, Val Loss: 0.0692\n",
      "08:28:25 - INFO: Epoch 476/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0659, Val Loss: 0.0707\n",
      "08:28:25 - INFO: Epoch 477/1000\n",
      "08:28:25 - INFO: Train Loss: 0.1233, Val Loss: 0.0662\n",
      "08:28:25 - INFO: Epoch 478/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0654, Val Loss: 0.0762\n",
      "08:28:25 - INFO: Epoch 479/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0675, Val Loss: 0.1204\n",
      "08:28:25 - INFO: Epoch 480/1000\n",
      "08:28:25 - INFO: Train Loss: 0.0859, Val Loss: 0.0633\n",
      "08:28:25 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0633\n",
      "08:28:25 - INFO: Epoch 481/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0849, Val Loss: 0.1585\n",
      "08:28:26 - INFO: Epoch 482/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0970, Val Loss: 0.0977\n",
      "08:28:26 - INFO: Epoch 483/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0676, Val Loss: 0.0770\n",
      "08:28:26 - INFO: Epoch 484/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0739, Val Loss: 0.0999\n",
      "08:28:26 - INFO: Epoch 485/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0999, Val Loss: 0.1537\n",
      "08:28:26 - INFO: Epoch 486/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0675, Val Loss: 0.0720\n",
      "08:28:26 - INFO: Epoch 487/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0909, Val Loss: 0.1693\n",
      "08:28:26 - INFO: Epoch 488/1000\n",
      "08:28:26 - INFO: Train Loss: 0.1191, Val Loss: 0.0708\n",
      "08:28:26 - INFO: Epoch 489/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0745, Val Loss: 0.0855\n",
      "08:28:26 - INFO: Epoch 490/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0711, Val Loss: 0.0858\n",
      "08:28:26 - INFO: Epoch 491/1000\n",
      "08:28:26 - INFO: Train Loss: 0.0769, Val Loss: 0.0753\n",
      "08:28:26 - INFO: Epoch 492/1000\n",
      "08:28:27 - INFO: Train Loss: 0.1090, Val Loss: 0.1895\n",
      "08:28:27 - INFO: Epoch 493/1000\n",
      "08:28:27 - INFO: Train Loss: 0.0980, Val Loss: 0.0837\n",
      "08:28:27 - INFO: Epoch 494/1000\n",
      "08:28:27 - INFO: Train Loss: 0.0819, Val Loss: 0.2821\n",
      "08:28:27 - INFO: Epoch 495/1000\n",
      "08:28:27 - INFO: Train Loss: 0.0739, Val Loss: 0.2533\n",
      "08:28:27 - INFO: Epoch 496/1000\n",
      "08:28:27 - INFO: Train Loss: 0.0746, Val Loss: 0.0666\n",
      "08:28:27 - INFO: Epoch 497/1000\n",
      "08:28:27 - INFO: Train Loss: 0.1052, Val Loss: 0.0830\n",
      "08:28:27 - INFO: Epoch 498/1000\n",
      "08:28:27 - INFO: Train Loss: 0.2036, Val Loss: 0.0850\n",
      "08:28:27 - INFO: Epoch 499/1000\n",
      "08:28:27 - INFO: Train Loss: 0.1057, Val Loss: 0.0709\n",
      "08:28:27 - INFO: Epoch 500/1000\n",
      "08:28:27 - INFO: Train Loss: 0.0810, Val Loss: 0.0861\n",
      "08:28:27 - INFO: Epoch 501/1000\n",
      "08:28:27 - INFO: Train Loss: 0.0768, Val Loss: 0.1377\n",
      "08:28:27 - INFO: Sampling 5 new images....\n",
      "999it [00:11, 88.06it/s] \n",
      "08:28:39 - INFO: Sampling 5 new images....\n",
      "999it [00:12, 83.12it/s]\n",
      "08:28:51 - INFO: Epoch 502/1000\n",
      "08:28:51 - INFO: Train Loss: 0.1259, Val Loss: 0.0876\n",
      "08:28:51 - INFO: Epoch 503/1000\n",
      "08:28:51 - INFO: Train Loss: 0.0725, Val Loss: 0.0822\n",
      "08:28:51 - INFO: Epoch 504/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0690, Val Loss: 0.0996\n",
      "08:28:52 - INFO: Epoch 505/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0640, Val Loss: 0.0652\n",
      "08:28:52 - INFO: Epoch 506/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0777, Val Loss: 0.0640\n",
      "08:28:52 - INFO: Epoch 507/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0966, Val Loss: 0.0765\n",
      "08:28:52 - INFO: Epoch 508/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0674, Val Loss: 0.1385\n",
      "08:28:52 - INFO: Epoch 509/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0816, Val Loss: 0.0908\n",
      "08:28:52 - INFO: Epoch 510/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0683, Val Loss: 0.0964\n",
      "08:28:52 - INFO: Epoch 511/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0683, Val Loss: 0.0709\n",
      "08:28:52 - INFO: Epoch 512/1000\n",
      "08:28:52 - INFO: Train Loss: 0.1600, Val Loss: 0.0671\n",
      "08:28:52 - INFO: Epoch 513/1000\n",
      "08:28:52 - INFO: Train Loss: 0.0672, Val Loss: 0.0636\n",
      "08:28:52 - INFO: Epoch 514/1000\n",
      "08:28:52 - INFO: Train Loss: 0.1074, Val Loss: 0.0788\n",
      "08:28:52 - INFO: Epoch 515/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0968, Val Loss: 0.0673\n",
      "08:28:53 - INFO: Epoch 516/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0885, Val Loss: 0.0646\n",
      "08:28:53 - INFO: Epoch 517/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0950, Val Loss: 0.0833\n",
      "08:28:53 - INFO: Epoch 518/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0666, Val Loss: 0.1327\n",
      "08:28:53 - INFO: Epoch 519/1000\n",
      "08:28:53 - INFO: Train Loss: 0.1512, Val Loss: 0.0784\n",
      "08:28:53 - INFO: Epoch 520/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0885, Val Loss: 0.0654\n",
      "08:28:53 - INFO: Epoch 521/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0888, Val Loss: 0.0677\n",
      "08:28:53 - INFO: Epoch 522/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0663, Val Loss: 0.0969\n",
      "08:28:53 - INFO: Epoch 523/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0684, Val Loss: 0.0619\n",
      "08:28:53 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0619\n",
      "08:28:53 - INFO: Epoch 524/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0712, Val Loss: 0.0598\n",
      "08:28:53 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0598\n",
      "08:28:53 - INFO: Epoch 525/1000\n",
      "08:28:53 - INFO: Train Loss: 0.0770, Val Loss: 0.0836\n",
      "08:28:53 - INFO: Epoch 526/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0919, Val Loss: 0.0658\n",
      "08:28:54 - INFO: Epoch 527/1000\n",
      "08:28:54 - INFO: Train Loss: 0.1157, Val Loss: 0.1726\n",
      "08:28:54 - INFO: Epoch 528/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0907, Val Loss: 0.0644\n",
      "08:28:54 - INFO: Epoch 529/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0926, Val Loss: 0.1207\n",
      "08:28:54 - INFO: Epoch 530/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0670, Val Loss: 0.1142\n",
      "08:28:54 - INFO: Epoch 531/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0621, Val Loss: 0.0661\n",
      "08:28:54 - INFO: Epoch 532/1000\n",
      "08:28:54 - INFO: Train Loss: 0.1016, Val Loss: 0.0867\n",
      "08:28:54 - INFO: Epoch 533/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0659, Val Loss: 0.0753\n",
      "08:28:54 - INFO: Epoch 534/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0998, Val Loss: 0.0780\n",
      "08:28:54 - INFO: Epoch 535/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0605, Val Loss: 0.0644\n",
      "08:28:54 - INFO: Epoch 536/1000\n",
      "08:28:54 - INFO: Train Loss: 0.0638, Val Loss: 0.0717\n",
      "08:28:54 - INFO: Epoch 537/1000\n",
      "08:28:55 - INFO: Train Loss: 0.1589, Val Loss: 0.2734\n",
      "08:28:55 - INFO: Epoch 538/1000\n",
      "08:28:55 - INFO: Train Loss: 0.0826, Val Loss: 0.1577\n",
      "08:28:55 - INFO: Epoch 539/1000\n",
      "08:28:55 - INFO: Train Loss: 0.0833, Val Loss: 0.0751\n",
      "08:28:55 - INFO: Epoch 540/1000\n",
      "08:28:55 - INFO: Train Loss: 0.0651, Val Loss: 0.2003\n",
      "08:28:55 - INFO: Epoch 541/1000\n",
      "08:28:55 - INFO: Train Loss: 0.1124, Val Loss: 0.0626\n",
      "08:28:55 - INFO: Epoch 542/1000\n",
      "08:28:55 - INFO: Train Loss: 0.0888, Val Loss: 0.0923\n",
      "08:28:55 - INFO: Epoch 543/1000\n",
      "08:28:55 - INFO: Train Loss: 0.0653, Val Loss: 0.0641\n",
      "08:28:55 - INFO: Epoch 544/1000\n",
      "08:28:55 - INFO: Train Loss: 0.1480, Val Loss: 0.0703\n",
      "08:28:55 - INFO: Epoch 545/1000\n",
      "08:28:55 - INFO: Train Loss: 0.0684, Val Loss: 0.0699\n",
      "08:28:55 - INFO: Epoch 546/1000\n",
      "08:28:55 - INFO: Train Loss: 0.0698, Val Loss: 0.2338\n",
      "08:28:55 - INFO: Epoch 547/1000\n",
      "08:28:55 - INFO: Train Loss: 0.1144, Val Loss: 0.0637\n",
      "08:28:55 - INFO: Epoch 548/1000\n",
      "08:28:56 - INFO: Train Loss: 0.1701, Val Loss: 0.0807\n",
      "08:28:56 - INFO: Epoch 549/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0671, Val Loss: 0.0698\n",
      "08:28:56 - INFO: Epoch 550/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0795, Val Loss: 0.1271\n",
      "08:28:56 - INFO: Epoch 551/1000\n",
      "08:28:56 - INFO: Train Loss: 0.2027, Val Loss: 0.0741\n",
      "08:28:56 - INFO: Epoch 552/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0667, Val Loss: 0.0794\n",
      "08:28:56 - INFO: Epoch 553/1000\n",
      "08:28:56 - INFO: Train Loss: 0.1215, Val Loss: 0.2827\n",
      "08:28:56 - INFO: Epoch 554/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0683, Val Loss: 0.0780\n",
      "08:28:56 - INFO: Epoch 555/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0980, Val Loss: 0.1874\n",
      "08:28:56 - INFO: Epoch 556/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0688, Val Loss: 0.1124\n",
      "08:28:56 - INFO: Epoch 557/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0645, Val Loss: 0.3085\n",
      "08:28:56 - INFO: Epoch 558/1000\n",
      "08:28:56 - INFO: Train Loss: 0.0656, Val Loss: 0.0737\n",
      "08:28:56 - INFO: Epoch 559/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0701, Val Loss: 0.0953\n",
      "08:28:57 - INFO: Epoch 560/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0698, Val Loss: 0.0714\n",
      "08:28:57 - INFO: Epoch 561/1000\n",
      "08:28:57 - INFO: Train Loss: 0.1004, Val Loss: 0.1263\n",
      "08:28:57 - INFO: Epoch 562/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0753, Val Loss: 0.0654\n",
      "08:28:57 - INFO: Epoch 563/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0769, Val Loss: 0.1487\n",
      "08:28:57 - INFO: Epoch 564/1000\n",
      "08:28:57 - INFO: Train Loss: 0.1025, Val Loss: 0.1037\n",
      "08:28:57 - INFO: Epoch 565/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0613, Val Loss: 0.0678\n",
      "08:28:57 - INFO: Epoch 566/1000\n",
      "08:28:57 - INFO: Train Loss: 0.1160, Val Loss: 0.0710\n",
      "08:28:57 - INFO: Epoch 567/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0929, Val Loss: 0.0757\n",
      "08:28:57 - INFO: Epoch 568/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0867, Val Loss: 0.0623\n",
      "08:28:57 - INFO: Epoch 569/1000\n",
      "08:28:57 - INFO: Train Loss: 0.0671, Val Loss: 0.1238\n",
      "08:28:57 - INFO: Epoch 570/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0771, Val Loss: 0.0783\n",
      "08:28:58 - INFO: Epoch 571/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0754, Val Loss: 0.0811\n",
      "08:28:58 - INFO: Epoch 572/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0745, Val Loss: 0.0630\n",
      "08:28:58 - INFO: Epoch 573/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0651, Val Loss: 0.0762\n",
      "08:28:58 - INFO: Epoch 574/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0846, Val Loss: 0.0993\n",
      "08:28:58 - INFO: Epoch 575/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0786, Val Loss: 0.0599\n",
      "08:28:58 - INFO: Epoch 576/1000\n",
      "08:28:58 - INFO: Train Loss: 0.1100, Val Loss: 0.1466\n",
      "08:28:58 - INFO: Epoch 577/1000\n",
      "08:28:58 - INFO: Train Loss: 0.1266, Val Loss: 0.0649\n",
      "08:28:58 - INFO: Epoch 578/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0680, Val Loss: 0.0606\n",
      "08:28:58 - INFO: Epoch 579/1000\n",
      "08:28:58 - INFO: Train Loss: 0.1316, Val Loss: 0.0708\n",
      "08:28:58 - INFO: Epoch 580/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0769, Val Loss: 0.2162\n",
      "08:28:58 - INFO: Epoch 581/1000\n",
      "08:28:58 - INFO: Train Loss: 0.0886, Val Loss: 0.1162\n",
      "08:28:59 - INFO: Epoch 582/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0647, Val Loss: 0.0639\n",
      "08:28:59 - INFO: Epoch 583/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0675, Val Loss: 0.0605\n",
      "08:28:59 - INFO: Epoch 584/1000\n",
      "08:28:59 - INFO: Train Loss: 0.1127, Val Loss: 0.0633\n",
      "08:28:59 - INFO: Epoch 585/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0958, Val Loss: 0.0675\n",
      "08:28:59 - INFO: Epoch 586/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0660, Val Loss: 0.0662\n",
      "08:28:59 - INFO: Epoch 587/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0981, Val Loss: 0.0661\n",
      "08:28:59 - INFO: Epoch 588/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0684, Val Loss: 0.0691\n",
      "08:28:59 - INFO: Epoch 589/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0760, Val Loss: 0.0975\n",
      "08:28:59 - INFO: Epoch 590/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0622, Val Loss: 0.0684\n",
      "08:28:59 - INFO: Epoch 591/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0701, Val Loss: 0.0623\n",
      "08:28:59 - INFO: Epoch 592/1000\n",
      "08:28:59 - INFO: Train Loss: 0.0674, Val Loss: 0.1995\n",
      "08:28:59 - INFO: Epoch 593/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0785, Val Loss: 0.0630\n",
      "08:29:00 - INFO: Epoch 594/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0723, Val Loss: 0.0606\n",
      "08:29:00 - INFO: Epoch 595/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0864, Val Loss: 0.2103\n",
      "08:29:00 - INFO: Epoch 596/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0759, Val Loss: 0.1220\n",
      "08:29:00 - INFO: Epoch 597/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0892, Val Loss: 0.2845\n",
      "08:29:00 - INFO: Epoch 598/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0813, Val Loss: 0.1068\n",
      "08:29:00 - INFO: Epoch 599/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0719, Val Loss: 0.0610\n",
      "08:29:00 - INFO: Epoch 600/1000\n",
      "08:29:00 - INFO: Train Loss: 0.0888, Val Loss: 0.0834\n",
      "08:29:00 - INFO: Epoch 601/1000\n",
      "08:29:00 - INFO: Train Loss: 0.1286, Val Loss: 0.0889\n",
      "08:29:00 - INFO: Sampling 5 new images....\n",
      "999it [00:13, 74.72it/s]\n",
      "08:29:14 - INFO: Sampling 5 new images....\n",
      "999it [00:13, 73.13it/s]\n",
      "08:29:28 - INFO: Epoch 602/1000\n",
      "08:29:28 - INFO: Train Loss: 0.0722, Val Loss: 0.1438\n",
      "08:29:28 - INFO: Epoch 603/1000\n",
      "08:29:28 - INFO: Train Loss: 0.0655, Val Loss: 0.0586\n",
      "08:29:28 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0586\n",
      "08:29:28 - INFO: Epoch 604/1000\n",
      "08:29:28 - INFO: Train Loss: 0.0881, Val Loss: 0.1588\n",
      "08:29:28 - INFO: Epoch 605/1000\n",
      "08:29:28 - INFO: Train Loss: 0.1666, Val Loss: 0.1946\n",
      "08:29:28 - INFO: Epoch 606/1000\n",
      "08:29:28 - INFO: Train Loss: 0.1005, Val Loss: 0.0604\n",
      "08:29:28 - INFO: Epoch 607/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0877, Val Loss: 0.0680\n",
      "08:29:29 - INFO: Epoch 608/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0817, Val Loss: 0.0715\n",
      "08:29:29 - INFO: Epoch 609/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0734, Val Loss: 0.0953\n",
      "08:29:29 - INFO: Epoch 610/1000\n",
      "08:29:29 - INFO: Train Loss: 0.1065, Val Loss: 0.1840\n",
      "08:29:29 - INFO: Epoch 611/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0674, Val Loss: 0.0716\n",
      "08:29:29 - INFO: Epoch 612/1000\n",
      "08:29:29 - INFO: Train Loss: 0.2337, Val Loss: 0.0902\n",
      "08:29:29 - INFO: Epoch 613/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0660, Val Loss: 0.0996\n",
      "08:29:29 - INFO: Epoch 614/1000\n",
      "08:29:29 - INFO: Train Loss: 0.1003, Val Loss: 0.0764\n",
      "08:29:29 - INFO: Epoch 615/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0692, Val Loss: 0.0834\n",
      "08:29:29 - INFO: Epoch 616/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0640, Val Loss: 0.1255\n",
      "08:29:29 - INFO: Epoch 617/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0652, Val Loss: 0.1147\n",
      "08:29:29 - INFO: Epoch 618/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0588, Val Loss: 0.0635\n",
      "08:29:29 - INFO: Epoch 619/1000\n",
      "08:29:29 - INFO: Train Loss: 0.0769, Val Loss: 0.0595\n",
      "08:29:29 - INFO: Epoch 620/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0652, Val Loss: 0.1537\n",
      "08:29:30 - INFO: Epoch 621/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0673, Val Loss: 0.1160\n",
      "08:29:30 - INFO: Epoch 622/1000\n",
      "08:29:30 - INFO: Train Loss: 0.1150, Val Loss: 0.0638\n",
      "08:29:30 - INFO: Epoch 623/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0604, Val Loss: 0.0970\n",
      "08:29:30 - INFO: Epoch 624/1000\n",
      "08:29:30 - INFO: Train Loss: 0.1054, Val Loss: 0.0678\n",
      "08:29:30 - INFO: Epoch 625/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0642, Val Loss: 0.0681\n",
      "08:29:30 - INFO: Epoch 626/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0607, Val Loss: 0.0774\n",
      "08:29:30 - INFO: Epoch 627/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0951, Val Loss: 0.0669\n",
      "08:29:30 - INFO: Epoch 628/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0901, Val Loss: 0.0607\n",
      "08:29:30 - INFO: Epoch 629/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0801, Val Loss: 0.0623\n",
      "08:29:30 - INFO: Epoch 630/1000\n",
      "08:29:30 - INFO: Train Loss: 0.0646, Val Loss: 0.0652\n",
      "08:29:30 - INFO: Epoch 631/1000\n",
      "08:29:30 - INFO: Train Loss: 0.1150, Val Loss: 0.0951\n",
      "08:29:30 - INFO: Epoch 632/1000\n",
      "08:29:31 - INFO: Train Loss: 0.0665, Val Loss: 0.0643\n",
      "08:29:31 - INFO: Epoch 633/1000\n",
      "08:29:31 - INFO: Train Loss: 0.0839, Val Loss: 0.2459\n",
      "08:29:31 - INFO: Epoch 634/1000\n",
      "08:29:31 - INFO: Train Loss: 0.0978, Val Loss: 0.0579\n",
      "08:29:31 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0579\n",
      "08:29:31 - INFO: Epoch 635/1000\n",
      "08:29:31 - INFO: Train Loss: 0.0945, Val Loss: 0.1084\n",
      "08:29:31 - INFO: Epoch 636/1000\n",
      "08:29:31 - INFO: Train Loss: 0.0801, Val Loss: 0.0681\n",
      "08:29:31 - INFO: Epoch 637/1000\n",
      "08:29:31 - INFO: Train Loss: 0.1048, Val Loss: 0.0840\n",
      "08:29:31 - INFO: Epoch 638/1000\n",
      "08:29:31 - INFO: Train Loss: 0.1183, Val Loss: 0.0849\n",
      "08:29:31 - INFO: Epoch 639/1000\n",
      "08:29:31 - INFO: Train Loss: 0.0591, Val Loss: 0.1344\n",
      "08:29:31 - INFO: Epoch 640/1000\n",
      "08:29:31 - INFO: Train Loss: 0.1199, Val Loss: 0.0601\n",
      "08:29:31 - INFO: Epoch 641/1000\n",
      "08:29:31 - INFO: Train Loss: 0.1555, Val Loss: 0.0859\n",
      "08:29:31 - INFO: Epoch 642/1000\n",
      "08:29:31 - INFO: Train Loss: 0.0806, Val Loss: 0.0727\n",
      "08:29:31 - INFO: Epoch 643/1000\n",
      "08:29:31 - INFO: Train Loss: 0.1003, Val Loss: 0.0950\n",
      "08:29:31 - INFO: Epoch 644/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0759, Val Loss: 0.0685\n",
      "08:29:32 - INFO: Epoch 645/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0832, Val Loss: 0.0704\n",
      "08:29:32 - INFO: Epoch 646/1000\n",
      "08:29:32 - INFO: Train Loss: 0.1503, Val Loss: 0.0670\n",
      "08:29:32 - INFO: Epoch 647/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0582, Val Loss: 0.0739\n",
      "08:29:32 - INFO: Epoch 648/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0742, Val Loss: 0.0663\n",
      "08:29:32 - INFO: Epoch 649/1000\n",
      "08:29:32 - INFO: Train Loss: 0.1400, Val Loss: 0.0611\n",
      "08:29:32 - INFO: Epoch 650/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0731, Val Loss: 0.0641\n",
      "08:29:32 - INFO: Epoch 651/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0857, Val Loss: 0.0605\n",
      "08:29:32 - INFO: Epoch 652/1000\n",
      "08:29:32 - INFO: Train Loss: 0.1084, Val Loss: 0.0617\n",
      "08:29:32 - INFO: Epoch 653/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0653, Val Loss: 0.0619\n",
      "08:29:32 - INFO: Epoch 654/1000\n",
      "08:29:32 - INFO: Train Loss: 0.0706, Val Loss: 0.1623\n",
      "08:29:32 - INFO: Epoch 655/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0703, Val Loss: 0.0588\n",
      "08:29:33 - INFO: Epoch 656/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0705, Val Loss: 0.1065\n",
      "08:29:33 - INFO: Epoch 657/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0841, Val Loss: 0.0956\n",
      "08:29:33 - INFO: Epoch 658/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0698, Val Loss: 0.0605\n",
      "08:29:33 - INFO: Epoch 659/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0719, Val Loss: 0.0628\n",
      "08:29:33 - INFO: Epoch 660/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0781, Val Loss: 0.0815\n",
      "08:29:33 - INFO: Epoch 661/1000\n",
      "08:29:33 - INFO: Train Loss: 0.1075, Val Loss: 0.1709\n",
      "08:29:33 - INFO: Epoch 662/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0620, Val Loss: 0.0947\n",
      "08:29:33 - INFO: Epoch 663/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0606, Val Loss: 0.0633\n",
      "08:29:33 - INFO: Epoch 664/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0622, Val Loss: 0.0781\n",
      "08:29:33 - INFO: Epoch 665/1000\n",
      "08:29:33 - INFO: Train Loss: 0.0811, Val Loss: 0.0827\n",
      "08:29:33 - INFO: Epoch 666/1000\n",
      "08:29:34 - INFO: Train Loss: 0.1266, Val Loss: 0.0584\n",
      "08:29:34 - INFO: Epoch 667/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0827, Val Loss: 0.0671\n",
      "08:29:34 - INFO: Epoch 668/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0612, Val Loss: 0.0673\n",
      "08:29:34 - INFO: Epoch 669/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0728, Val Loss: 0.0863\n",
      "08:29:34 - INFO: Epoch 670/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0898, Val Loss: 0.0679\n",
      "08:29:34 - INFO: Epoch 671/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0703, Val Loss: 0.0591\n",
      "08:29:34 - INFO: Epoch 672/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0619, Val Loss: 0.0662\n",
      "08:29:34 - INFO: Epoch 673/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0707, Val Loss: 0.1331\n",
      "08:29:34 - INFO: Epoch 674/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0624, Val Loss: 0.1106\n",
      "08:29:34 - INFO: Epoch 675/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0764, Val Loss: 0.0796\n",
      "08:29:34 - INFO: Epoch 676/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0808, Val Loss: 0.1025\n",
      "08:29:34 - INFO: Epoch 677/1000\n",
      "08:29:34 - INFO: Train Loss: 0.0653, Val Loss: 0.0645\n",
      "08:29:34 - INFO: Epoch 678/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0582, Val Loss: 0.0617\n",
      "08:29:35 - INFO: Epoch 679/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0698, Val Loss: 0.0616\n",
      "08:29:35 - INFO: Epoch 680/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0907, Val Loss: 0.2958\n",
      "08:29:35 - INFO: Epoch 681/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0838, Val Loss: 0.0693\n",
      "08:29:35 - INFO: Epoch 682/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0890, Val Loss: 0.0623\n",
      "08:29:35 - INFO: Epoch 683/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0731, Val Loss: 0.0675\n",
      "08:29:35 - INFO: Epoch 684/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0661, Val Loss: 0.1028\n",
      "08:29:35 - INFO: Epoch 685/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0630, Val Loss: 0.0603\n",
      "08:29:35 - INFO: Epoch 686/1000\n",
      "08:29:35 - INFO: Train Loss: 0.1000, Val Loss: 0.2831\n",
      "08:29:35 - INFO: Epoch 687/1000\n",
      "08:29:35 - INFO: Train Loss: 0.0624, Val Loss: 0.0790\n",
      "08:29:35 - INFO: Epoch 688/1000\n",
      "08:29:35 - INFO: Train Loss: 0.1366, Val Loss: 0.0695\n",
      "08:29:35 - INFO: Epoch 689/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0768, Val Loss: 0.0639\n",
      "08:29:36 - INFO: Epoch 690/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0696, Val Loss: 0.0650\n",
      "08:29:36 - INFO: Epoch 691/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0851, Val Loss: 0.0690\n",
      "08:29:36 - INFO: Epoch 692/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0711, Val Loss: 0.0872\n",
      "08:29:36 - INFO: Epoch 693/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0864, Val Loss: 0.0749\n",
      "08:29:36 - INFO: Epoch 694/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0595, Val Loss: 0.1006\n",
      "08:29:36 - INFO: Epoch 695/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0597, Val Loss: 0.0833\n",
      "08:29:36 - INFO: Epoch 696/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0761, Val Loss: 0.1034\n",
      "08:29:36 - INFO: Epoch 697/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0730, Val Loss: 0.0891\n",
      "08:29:36 - INFO: Epoch 698/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0598, Val Loss: 0.0581\n",
      "08:29:36 - INFO: Epoch 699/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0889, Val Loss: 0.0752\n",
      "08:29:36 - INFO: Epoch 700/1000\n",
      "08:29:36 - INFO: Train Loss: 0.0743, Val Loss: 0.0619\n",
      "08:29:36 - INFO: Epoch 701/1000\n",
      "08:29:37 - INFO: Train Loss: 0.0860, Val Loss: 0.0659\n",
      "08:29:37 - INFO: Sampling 5 new images....\n",
      "999it [00:13, 75.43it/s]\n",
      "08:29:50 - INFO: Sampling 5 new images....\n",
      "999it [00:13, 71.57it/s]\n",
      "08:30:05 - INFO: Epoch 702/1000\n",
      "08:30:05 - INFO: Train Loss: 0.0849, Val Loss: 0.0994\n",
      "08:30:05 - INFO: Epoch 703/1000\n",
      "08:30:05 - INFO: Train Loss: 0.0591, Val Loss: 0.1143\n",
      "08:30:05 - INFO: Epoch 704/1000\n",
      "08:30:05 - INFO: Train Loss: 0.1250, Val Loss: 0.0669\n",
      "08:30:05 - INFO: Epoch 705/1000\n",
      "08:30:05 - INFO: Train Loss: 0.0639, Val Loss: 0.0839\n",
      "08:30:05 - INFO: Epoch 706/1000\n",
      "08:30:05 - INFO: Train Loss: 0.0668, Val Loss: 0.2454\n",
      "08:30:05 - INFO: Epoch 707/1000\n",
      "08:30:05 - INFO: Train Loss: 0.1125, Val Loss: 0.1969\n",
      "08:30:05 - INFO: Epoch 708/1000\n",
      "08:30:05 - INFO: Train Loss: 0.0616, Val Loss: 0.0619\n",
      "08:30:05 - INFO: Epoch 709/1000\n",
      "08:30:05 - INFO: Train Loss: 0.0609, Val Loss: 0.0613\n",
      "08:30:05 - INFO: Epoch 710/1000\n",
      "08:30:05 - INFO: Train Loss: 0.0608, Val Loss: 0.0765\n",
      "08:30:05 - INFO: Epoch 711/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0840, Val Loss: 0.0620\n",
      "08:30:06 - INFO: Epoch 712/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0672, Val Loss: 0.0697\n",
      "08:30:06 - INFO: Epoch 713/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0632, Val Loss: 0.0636\n",
      "08:30:06 - INFO: Epoch 714/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0867, Val Loss: 0.0608\n",
      "08:30:06 - INFO: Epoch 715/1000\n",
      "08:30:06 - INFO: Train Loss: 0.2118, Val Loss: 0.0608\n",
      "08:30:06 - INFO: Epoch 716/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0864, Val Loss: 0.0622\n",
      "08:30:06 - INFO: Epoch 717/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0796, Val Loss: 0.1327\n",
      "08:30:06 - INFO: Epoch 718/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0603, Val Loss: 0.0584\n",
      "08:30:06 - INFO: Epoch 719/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0758, Val Loss: 0.0673\n",
      "08:30:06 - INFO: Epoch 720/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0730, Val Loss: 0.0595\n",
      "08:30:06 - INFO: Epoch 721/1000\n",
      "08:30:06 - INFO: Train Loss: 0.0747, Val Loss: 0.0798\n",
      "08:30:06 - INFO: Epoch 722/1000\n",
      "08:30:07 - INFO: Train Loss: 0.1283, Val Loss: 0.0586\n",
      "08:30:07 - INFO: Epoch 723/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0691, Val Loss: 0.0700\n",
      "08:30:07 - INFO: Epoch 724/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0685, Val Loss: 0.0882\n",
      "08:30:07 - INFO: Epoch 725/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0599, Val Loss: 0.0622\n",
      "08:30:07 - INFO: Epoch 726/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0642, Val Loss: 0.0636\n",
      "08:30:07 - INFO: Epoch 727/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0634, Val Loss: 0.0610\n",
      "08:30:07 - INFO: Epoch 728/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0627, Val Loss: 0.0853\n",
      "08:30:07 - INFO: Epoch 729/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0666, Val Loss: 0.0624\n",
      "08:30:07 - INFO: Epoch 730/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0953, Val Loss: 0.0614\n",
      "08:30:07 - INFO: Epoch 731/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0951, Val Loss: 0.1066\n",
      "08:30:07 - INFO: Epoch 732/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0706, Val Loss: 0.0627\n",
      "08:30:07 - INFO: Epoch 733/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0763, Val Loss: 0.0590\n",
      "08:30:07 - INFO: Epoch 734/1000\n",
      "08:30:07 - INFO: Train Loss: 0.0679, Val Loss: 0.1172\n",
      "08:30:07 - INFO: Epoch 735/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0891, Val Loss: 0.0620\n",
      "08:30:08 - INFO: Epoch 736/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0966, Val Loss: 0.0635\n",
      "08:30:08 - INFO: Epoch 737/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0724, Val Loss: 0.0578\n",
      "08:30:08 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0578\n",
      "08:30:08 - INFO: Epoch 738/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0916, Val Loss: 0.0665\n",
      "08:30:08 - INFO: Epoch 739/1000\n",
      "08:30:08 - INFO: Train Loss: 0.1228, Val Loss: 0.0580\n",
      "08:30:08 - INFO: Epoch 740/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0867, Val Loss: 0.4460\n",
      "08:30:08 - INFO: Epoch 741/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0753, Val Loss: 0.0674\n",
      "08:30:08 - INFO: Epoch 742/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0669, Val Loss: 0.0582\n",
      "08:30:08 - INFO: Epoch 743/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0986, Val Loss: 0.0653\n",
      "08:30:08 - INFO: Epoch 744/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0937, Val Loss: 0.0736\n",
      "08:30:08 - INFO: Epoch 745/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0667, Val Loss: 0.1061\n",
      "08:30:08 - INFO: Epoch 746/1000\n",
      "08:30:08 - INFO: Train Loss: 0.0568, Val Loss: 0.0619\n",
      "08:30:08 - INFO: Epoch 747/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0640, Val Loss: 0.0595\n",
      "08:30:09 - INFO: Epoch 748/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0671, Val Loss: 0.0614\n",
      "08:30:09 - INFO: Epoch 749/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0671, Val Loss: 0.0748\n",
      "08:30:09 - INFO: Epoch 750/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0862, Val Loss: 0.0774\n",
      "08:30:09 - INFO: Epoch 751/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0636, Val Loss: 0.0979\n",
      "08:30:09 - INFO: Epoch 752/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0611, Val Loss: 0.0788\n",
      "08:30:09 - INFO: Epoch 753/1000\n",
      "08:30:09 - INFO: Train Loss: 0.1296, Val Loss: 0.0581\n",
      "08:30:09 - INFO: Epoch 754/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0905, Val Loss: 0.2689\n",
      "08:30:09 - INFO: Epoch 755/1000\n",
      "08:30:09 - INFO: Train Loss: 0.0838, Val Loss: 0.0837\n",
      "08:30:09 - INFO: Epoch 756/1000\n",
      "08:30:09 - INFO: Train Loss: 0.1368, Val Loss: 0.0648\n",
      "08:30:09 - INFO: Epoch 757/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0768, Val Loss: 0.0874\n",
      "08:30:10 - INFO: Epoch 758/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0633, Val Loss: 0.0631\n",
      "08:30:10 - INFO: Epoch 759/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0633, Val Loss: 0.1216\n",
      "08:30:10 - INFO: Epoch 760/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0789, Val Loss: 0.0597\n",
      "08:30:10 - INFO: Epoch 761/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0572, Val Loss: 0.0651\n",
      "08:30:10 - INFO: Epoch 762/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0723, Val Loss: 0.0738\n",
      "08:30:10 - INFO: Epoch 763/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0613, Val Loss: 0.1103\n",
      "08:30:10 - INFO: Epoch 764/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0646, Val Loss: 0.1048\n",
      "08:30:10 - INFO: Epoch 765/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0908, Val Loss: 0.0614\n",
      "08:30:10 - INFO: Epoch 766/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0571, Val Loss: 0.0598\n",
      "08:30:10 - INFO: Epoch 767/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0662, Val Loss: 0.1042\n",
      "08:30:10 - INFO: Epoch 768/1000\n",
      "08:30:10 - INFO: Train Loss: 0.0589, Val Loss: 0.1951\n",
      "08:30:10 - INFO: Epoch 769/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0830, Val Loss: 0.0797\n",
      "08:30:11 - INFO: Epoch 770/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0660, Val Loss: 0.0688\n",
      "08:30:11 - INFO: Epoch 771/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0608, Val Loss: 0.0641\n",
      "08:30:11 - INFO: Epoch 772/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0730, Val Loss: 0.0548\n",
      "08:30:11 - INFO: Model saved to models\\nyu_depth_diffusion_654eyo9e.pth with val_loss 0.0548\n",
      "08:30:11 - INFO: Epoch 773/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0838, Val Loss: 0.0583\n",
      "08:30:11 - INFO: Epoch 774/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0642, Val Loss: 0.0593\n",
      "08:30:11 - INFO: Epoch 775/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0613, Val Loss: 0.0645\n",
      "08:30:11 - INFO: Epoch 776/1000\n",
      "08:30:11 - INFO: Train Loss: 0.1490, Val Loss: 0.0662\n",
      "08:30:11 - INFO: Epoch 777/1000\n",
      "08:30:11 - INFO: Train Loss: 0.1058, Val Loss: 0.0709\n",
      "08:30:11 - INFO: Epoch 778/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0852, Val Loss: 0.0600\n",
      "08:30:11 - INFO: Epoch 779/1000\n",
      "08:30:11 - INFO: Train Loss: 0.0725, Val Loss: 0.0670\n",
      "08:30:11 - INFO: Epoch 780/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0896, Val Loss: 0.0760\n",
      "08:30:12 - INFO: Epoch 781/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0914, Val Loss: 0.2675\n",
      "08:30:12 - INFO: Epoch 782/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0743, Val Loss: 0.0686\n",
      "08:30:12 - INFO: Epoch 783/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0630, Val Loss: 0.0666\n",
      "08:30:12 - INFO: Epoch 784/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0677, Val Loss: 0.0674\n",
      "08:30:12 - INFO: Epoch 785/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0570, Val Loss: 0.0793\n",
      "08:30:12 - INFO: Epoch 786/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0706, Val Loss: 0.0604\n",
      "08:30:12 - INFO: Epoch 787/1000\n",
      "08:30:12 - INFO: Train Loss: 0.1434, Val Loss: 0.0590\n",
      "08:30:12 - INFO: Epoch 788/1000\n",
      "08:30:12 - INFO: Train Loss: 0.0938, Val Loss: 0.2060\n",
      "08:30:12 - INFO: Epoch 789/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0648, Val Loss: 0.0634\n",
      "08:30:13 - INFO: Epoch 790/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0873, Val Loss: 0.2889\n",
      "08:30:13 - INFO: Epoch 791/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0745, Val Loss: 0.0804\n",
      "08:30:13 - INFO: Epoch 792/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0559, Val Loss: 0.1756\n",
      "08:30:13 - INFO: Epoch 793/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0851, Val Loss: 0.5199\n",
      "08:30:13 - INFO: Epoch 794/1000\n",
      "08:30:13 - INFO: Train Loss: 0.1546, Val Loss: 0.0891\n",
      "08:30:13 - INFO: Epoch 795/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0702, Val Loss: 0.3681\n",
      "08:30:13 - INFO: Epoch 796/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0663, Val Loss: 0.0651\n",
      "08:30:13 - INFO: Epoch 797/1000\n",
      "08:30:13 - INFO: Train Loss: 0.1116, Val Loss: 0.0637\n",
      "08:30:13 - INFO: Epoch 798/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0598, Val Loss: 0.1296\n",
      "08:30:13 - INFO: Epoch 799/1000\n",
      "08:30:13 - INFO: Train Loss: 0.0760, Val Loss: 0.0716\n",
      "08:30:13 - INFO: Epoch 800/1000\n",
      "08:30:14 - INFO: Train Loss: 0.0751, Val Loss: 0.0593\n",
      "08:30:14 - INFO: Epoch 801/1000\n",
      "08:30:14 - INFO: Train Loss: 0.0952, Val Loss: 0.0590\n",
      "08:30:14 - INFO: Sampling 5 new images....\n",
      "999it [00:14, 70.77it/s]\n",
      "08:30:28 - INFO: Sampling 5 new images....\n",
      "999it [00:10, 92.74it/s]\n",
      "08:30:39 - INFO: Epoch 802/1000\n",
      "08:30:39 - INFO: Train Loss: 0.0621, Val Loss: 0.1420\n",
      "08:30:39 - INFO: Epoch 803/1000\n",
      "08:30:39 - INFO: Train Loss: 0.1293, Val Loss: 0.0562\n",
      "08:30:39 - INFO: Epoch 804/1000\n",
      "08:30:39 - INFO: Train Loss: 0.0696, Val Loss: 0.0597\n",
      "08:30:39 - INFO: Epoch 805/1000\n",
      "08:30:39 - INFO: Train Loss: 0.0591, Val Loss: 0.0582\n",
      "08:30:39 - INFO: Epoch 806/1000\n",
      "08:30:39 - INFO: Train Loss: 0.1361, Val Loss: 0.0885\n",
      "08:30:39 - INFO: Epoch 807/1000\n",
      "08:30:40 - INFO: Train Loss: 0.1137, Val Loss: 0.0595\n",
      "08:30:40 - INFO: Epoch 808/1000\n",
      "08:30:40 - INFO: Train Loss: 0.1666, Val Loss: 0.0759\n",
      "08:30:40 - INFO: Epoch 809/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0638, Val Loss: 0.0605\n",
      "08:30:40 - INFO: Epoch 810/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0919, Val Loss: 0.0750\n",
      "08:30:40 - INFO: Epoch 811/1000\n",
      "08:30:40 - INFO: Train Loss: 0.1091, Val Loss: 0.0743\n",
      "08:30:40 - INFO: Epoch 812/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0934, Val Loss: 0.1480\n",
      "08:30:40 - INFO: Epoch 813/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0791, Val Loss: 0.0691\n",
      "08:30:40 - INFO: Epoch 814/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0580, Val Loss: 0.1834\n",
      "08:30:40 - INFO: Epoch 815/1000\n",
      "08:30:40 - INFO: Train Loss: 0.1080, Val Loss: 0.0672\n",
      "08:30:40 - INFO: Epoch 816/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0605, Val Loss: 0.0969\n",
      "08:30:40 - INFO: Epoch 817/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0654, Val Loss: 0.0570\n",
      "08:30:40 - INFO: Epoch 818/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0794, Val Loss: 0.0704\n",
      "08:30:40 - INFO: Epoch 819/1000\n",
      "08:30:40 - INFO: Train Loss: 0.0842, Val Loss: 0.0746\n",
      "08:30:40 - INFO: Epoch 820/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0855, Val Loss: 0.0885\n",
      "08:30:41 - INFO: Epoch 821/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0655, Val Loss: 0.0674\n",
      "08:30:41 - INFO: Epoch 822/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0607, Val Loss: 0.0557\n",
      "08:30:41 - INFO: Epoch 823/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0642, Val Loss: 0.0772\n",
      "08:30:41 - INFO: Epoch 824/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0819, Val Loss: 0.0689\n",
      "08:30:41 - INFO: Epoch 825/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0654, Val Loss: 0.0769\n",
      "08:30:41 - INFO: Epoch 826/1000\n",
      "08:30:41 - INFO: Train Loss: 0.1510, Val Loss: 0.0740\n",
      "08:30:41 - INFO: Epoch 827/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0635, Val Loss: 0.0587\n",
      "08:30:41 - INFO: Epoch 828/1000\n",
      "08:30:41 - INFO: Train Loss: 0.1538, Val Loss: 0.0668\n",
      "08:30:41 - INFO: Epoch 829/1000\n",
      "08:30:41 - INFO: Train Loss: 0.1163, Val Loss: 0.1170\n",
      "08:30:41 - INFO: Epoch 830/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0631, Val Loss: 0.1045\n",
      "08:30:41 - INFO: Epoch 831/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0739, Val Loss: 0.0654\n",
      "08:30:41 - INFO: Epoch 832/1000\n",
      "08:30:41 - INFO: Train Loss: 0.0833, Val Loss: 0.1233\n",
      "08:30:41 - INFO: Epoch 833/1000\n",
      "08:30:41 - INFO: Train Loss: 0.1235, Val Loss: 0.1160\n",
      "08:30:41 - INFO: Epoch 834/1000\n",
      "08:30:42 - INFO: Train Loss: 0.1152, Val Loss: 0.0583\n",
      "08:30:42 - INFO: Epoch 835/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0631, Val Loss: 0.0714\n",
      "08:30:42 - INFO: Epoch 836/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0757, Val Loss: 0.1128\n",
      "08:30:42 - INFO: Epoch 837/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0700, Val Loss: 0.0897\n",
      "08:30:42 - INFO: Epoch 838/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0730, Val Loss: 0.0894\n",
      "08:30:42 - INFO: Epoch 839/1000\n",
      "08:30:42 - INFO: Train Loss: 0.1156, Val Loss: 0.0647\n",
      "08:30:42 - INFO: Epoch 840/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0622, Val Loss: 0.0606\n",
      "08:30:42 - INFO: Epoch 841/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0923, Val Loss: 0.0600\n",
      "08:30:42 - INFO: Epoch 842/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0610, Val Loss: 0.2094\n",
      "08:30:42 - INFO: Epoch 843/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0766, Val Loss: 0.0739\n",
      "08:30:42 - INFO: Epoch 844/1000\n",
      "08:30:42 - INFO: Train Loss: 0.1050, Val Loss: 0.0578\n",
      "08:30:42 - INFO: Epoch 845/1000\n",
      "08:30:42 - INFO: Train Loss: 0.1011, Val Loss: 0.0742\n",
      "08:30:42 - INFO: Epoch 846/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0908, Val Loss: 0.0760\n",
      "08:30:42 - INFO: Epoch 847/1000\n",
      "08:30:42 - INFO: Train Loss: 0.0812, Val Loss: 0.0831\n",
      "08:30:42 - INFO: Epoch 848/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0715, Val Loss: 0.0596\n",
      "08:30:43 - INFO: Epoch 849/1000\n",
      "08:30:43 - INFO: Train Loss: 0.1391, Val Loss: 0.1465\n",
      "08:30:43 - INFO: Epoch 850/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0637, Val Loss: 0.0953\n",
      "08:30:43 - INFO: Epoch 851/1000\n",
      "08:30:43 - INFO: Train Loss: 0.1371, Val Loss: 0.0584\n",
      "08:30:43 - INFO: Epoch 852/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0548, Val Loss: 0.1067\n",
      "08:30:43 - INFO: Epoch 853/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0641, Val Loss: 0.1302\n",
      "08:30:43 - INFO: Epoch 854/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0642, Val Loss: 0.0802\n",
      "08:30:43 - INFO: Epoch 855/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0934, Val Loss: 0.0630\n",
      "08:30:43 - INFO: Epoch 856/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0632, Val Loss: 0.0696\n",
      "08:30:43 - INFO: Epoch 857/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0605, Val Loss: 0.1462\n",
      "08:30:43 - INFO: Epoch 858/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0668, Val Loss: 0.0737\n",
      "08:30:43 - INFO: Epoch 859/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0983, Val Loss: 0.0697\n",
      "08:30:43 - INFO: Epoch 860/1000\n",
      "08:30:43 - INFO: Train Loss: 0.2094, Val Loss: 0.2446\n",
      "08:30:43 - INFO: Epoch 861/1000\n",
      "08:30:43 - INFO: Train Loss: 0.0686, Val Loss: 0.0700\n",
      "08:30:43 - INFO: Epoch 862/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0626, Val Loss: 0.0598\n",
      "08:30:44 - INFO: Epoch 863/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0816, Val Loss: 0.0592\n",
      "08:30:44 - INFO: Epoch 864/1000\n",
      "08:30:44 - INFO: Train Loss: 0.2023, Val Loss: 0.0656\n",
      "08:30:44 - INFO: Epoch 865/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0590, Val Loss: 0.0726\n",
      "08:30:44 - INFO: Epoch 866/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0891, Val Loss: 0.1029\n",
      "08:30:44 - INFO: Epoch 867/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0773, Val Loss: 0.0680\n",
      "08:30:44 - INFO: Epoch 868/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0664, Val Loss: 0.0668\n",
      "08:30:44 - INFO: Epoch 869/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0802, Val Loss: 0.1035\n",
      "08:30:44 - INFO: Epoch 870/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0868, Val Loss: 0.1215\n",
      "08:30:44 - INFO: Epoch 871/1000\n",
      "08:30:44 - INFO: Train Loss: 0.1134, Val Loss: 0.0591\n",
      "08:30:44 - INFO: Epoch 872/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0586, Val Loss: 0.0686\n",
      "08:30:44 - INFO: Epoch 873/1000\n",
      "08:30:44 - INFO: Train Loss: 0.0741, Val Loss: 0.1203\n",
      "08:30:44 - INFO: Epoch 874/1000\n",
      "08:30:44 - INFO: Train Loss: 0.1013, Val Loss: 0.0566\n",
      "08:30:44 - INFO: Epoch 875/1000\n",
      "08:30:44 - INFO: Train Loss: 0.2030, Val Loss: 0.0663\n",
      "08:30:44 - INFO: Epoch 876/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0802, Val Loss: 0.0606\n",
      "08:30:45 - INFO: Epoch 877/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0730, Val Loss: 0.0609\n",
      "08:30:45 - INFO: Epoch 878/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0938, Val Loss: 0.0667\n",
      "08:30:45 - INFO: Epoch 879/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0602, Val Loss: 0.3021\n",
      "08:30:45 - INFO: Epoch 880/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0687, Val Loss: 0.0805\n",
      "08:30:45 - INFO: Epoch 881/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0767, Val Loss: 0.1600\n",
      "08:30:45 - INFO: Epoch 882/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0689, Val Loss: 0.0705\n",
      "08:30:45 - INFO: Epoch 883/1000\n",
      "08:30:45 - INFO: Train Loss: 0.1084, Val Loss: 0.0721\n",
      "08:30:45 - INFO: Epoch 884/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0805, Val Loss: 0.0730\n",
      "08:30:45 - INFO: Epoch 885/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0877, Val Loss: 0.0608\n",
      "08:30:45 - INFO: Epoch 886/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0737, Val Loss: 0.1140\n",
      "08:30:45 - INFO: Epoch 887/1000\n",
      "08:30:45 - INFO: Train Loss: 0.1999, Val Loss: 0.2295\n",
      "08:30:45 - INFO: Epoch 888/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0658, Val Loss: 0.2200\n",
      "08:30:45 - INFO: Epoch 889/1000\n",
      "08:30:45 - INFO: Train Loss: 0.0605, Val Loss: 0.1358\n",
      "08:30:45 - INFO: Epoch 890/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0586, Val Loss: 0.1237\n",
      "08:30:46 - INFO: Epoch 891/1000\n",
      "08:30:46 - INFO: Train Loss: 0.1132, Val Loss: 0.0589\n",
      "08:30:46 - INFO: Epoch 892/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0950, Val Loss: 0.0582\n",
      "08:30:46 - INFO: Epoch 893/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0776, Val Loss: 0.0573\n",
      "08:30:46 - INFO: Epoch 894/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0566, Val Loss: 0.0600\n",
      "08:30:46 - INFO: Epoch 895/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0825, Val Loss: 0.0676\n",
      "08:30:46 - INFO: Epoch 896/1000\n",
      "08:30:46 - INFO: Train Loss: 0.1049, Val Loss: 0.0951\n",
      "08:30:46 - INFO: Epoch 897/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0705, Val Loss: 0.0634\n",
      "08:30:46 - INFO: Epoch 898/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0811, Val Loss: 0.1251\n",
      "08:30:46 - INFO: Epoch 899/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0948, Val Loss: 0.3082\n",
      "08:30:46 - INFO: Epoch 900/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0717, Val Loss: 0.0937\n",
      "08:30:46 - INFO: Epoch 901/1000\n",
      "08:30:46 - INFO: Train Loss: 0.0628, Val Loss: 0.0950\n",
      "08:30:46 - INFO: Sampling 5 new images....\n",
      "999it [00:10, 98.92it/s] \n",
      "08:30:56 - INFO: Sampling 5 new images....\n",
      "999it [00:09, 101.83it/s]\n",
      "08:31:07 - INFO: Epoch 902/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0621, Val Loss: 0.1263\n",
      "08:31:07 - INFO: Epoch 903/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0744, Val Loss: 0.0753\n",
      "08:31:07 - INFO: Epoch 904/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0614, Val Loss: 0.0571\n",
      "08:31:07 - INFO: Epoch 905/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0949, Val Loss: 0.0586\n",
      "08:31:07 - INFO: Epoch 906/1000\n",
      "08:31:07 - INFO: Train Loss: 0.1935, Val Loss: 0.0623\n",
      "08:31:07 - INFO: Epoch 907/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0645, Val Loss: 0.0584\n",
      "08:31:07 - INFO: Epoch 908/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0687, Val Loss: 0.0578\n",
      "08:31:07 - INFO: Epoch 909/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0798, Val Loss: 0.0595\n",
      "08:31:07 - INFO: Epoch 910/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0608, Val Loss: 0.0582\n",
      "08:31:07 - INFO: Epoch 911/1000\n",
      "08:31:07 - INFO: Train Loss: 0.0747, Val Loss: 0.1820\n",
      "08:31:07 - INFO: Epoch 912/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0701, Val Loss: 0.1073\n",
      "08:31:08 - INFO: Epoch 913/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0796, Val Loss: 0.0659\n",
      "08:31:08 - INFO: Epoch 914/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0799, Val Loss: 0.0552\n",
      "08:31:08 - INFO: Epoch 915/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0735, Val Loss: 0.0799\n",
      "08:31:08 - INFO: Epoch 916/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0789, Val Loss: 0.0569\n",
      "08:31:08 - INFO: Epoch 917/1000\n",
      "08:31:08 - INFO: Train Loss: 0.1163, Val Loss: 0.0731\n",
      "08:31:08 - INFO: Epoch 918/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0613, Val Loss: 0.0622\n",
      "08:31:08 - INFO: Epoch 919/1000\n",
      "08:31:08 - INFO: Train Loss: 0.1425, Val Loss: 0.0583\n",
      "08:31:08 - INFO: Epoch 920/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0584, Val Loss: 0.0616\n",
      "08:31:08 - INFO: Epoch 921/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0722, Val Loss: 0.0775\n",
      "08:31:08 - INFO: Epoch 922/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0571, Val Loss: 0.0587\n",
      "08:31:08 - INFO: Epoch 923/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0666, Val Loss: 0.0729\n",
      "08:31:08 - INFO: Epoch 924/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0713, Val Loss: 0.0693\n",
      "08:31:08 - INFO: Epoch 925/1000\n",
      "08:31:08 - INFO: Train Loss: 0.0589, Val Loss: 0.1203\n",
      "08:31:08 - INFO: Epoch 926/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0740, Val Loss: 0.0660\n",
      "08:31:09 - INFO: Epoch 927/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0959, Val Loss: 0.0616\n",
      "08:31:09 - INFO: Epoch 928/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0656, Val Loss: 0.0780\n",
      "08:31:09 - INFO: Epoch 929/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0675, Val Loss: 0.0810\n",
      "08:31:09 - INFO: Epoch 930/1000\n",
      "08:31:09 - INFO: Train Loss: 0.1306, Val Loss: 0.1018\n",
      "08:31:09 - INFO: Epoch 931/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0752, Val Loss: 0.0749\n",
      "08:31:09 - INFO: Epoch 932/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0701, Val Loss: 0.0728\n",
      "08:31:09 - INFO: Epoch 933/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0611, Val Loss: 0.0895\n",
      "08:31:09 - INFO: Epoch 934/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0781, Val Loss: 0.0586\n",
      "08:31:09 - INFO: Epoch 935/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0906, Val Loss: 0.0589\n",
      "08:31:09 - INFO: Epoch 936/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0650, Val Loss: 0.0628\n",
      "08:31:09 - INFO: Epoch 937/1000\n",
      "08:31:09 - INFO: Train Loss: 0.1126, Val Loss: 0.0600\n",
      "08:31:09 - INFO: Epoch 938/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0706, Val Loss: 0.0655\n",
      "08:31:09 - INFO: Epoch 939/1000\n",
      "08:31:09 - INFO: Train Loss: 0.0886, Val Loss: 0.0619\n",
      "08:31:09 - INFO: Epoch 940/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0905, Val Loss: 0.0898\n",
      "08:31:10 - INFO: Epoch 941/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0908, Val Loss: 0.0580\n",
      "08:31:10 - INFO: Epoch 942/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0993, Val Loss: 0.1039\n",
      "08:31:10 - INFO: Epoch 943/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0713, Val Loss: 0.0744\n",
      "08:31:10 - INFO: Epoch 944/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0783, Val Loss: 0.0624\n",
      "08:31:10 - INFO: Epoch 945/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0663, Val Loss: 0.0614\n",
      "08:31:10 - INFO: Epoch 946/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0885, Val Loss: 0.0861\n",
      "08:31:10 - INFO: Epoch 947/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0638, Val Loss: 0.0587\n",
      "08:31:10 - INFO: Epoch 948/1000\n",
      "08:31:10 - INFO: Train Loss: 0.1049, Val Loss: 0.0643\n",
      "08:31:10 - INFO: Epoch 949/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0591, Val Loss: 0.0711\n",
      "08:31:10 - INFO: Epoch 950/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0740, Val Loss: 0.0596\n",
      "08:31:10 - INFO: Epoch 951/1000\n",
      "08:31:10 - INFO: Train Loss: 0.1418, Val Loss: 0.0621\n",
      "08:31:10 - INFO: Epoch 952/1000\n",
      "08:31:10 - INFO: Train Loss: 0.0985, Val Loss: 0.0758\n",
      "08:31:10 - INFO: Epoch 953/1000\n",
      "08:31:10 - INFO: Train Loss: 0.1104, Val Loss: 0.0759\n",
      "08:31:10 - INFO: Epoch 954/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0641, Val Loss: 0.2188\n",
      "08:31:11 - INFO: Epoch 955/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0645, Val Loss: 0.0604\n",
      "08:31:11 - INFO: Epoch 956/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0633, Val Loss: 0.0593\n",
      "08:31:11 - INFO: Epoch 957/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0656, Val Loss: 0.0627\n",
      "08:31:11 - INFO: Epoch 958/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0671, Val Loss: 0.0586\n",
      "08:31:11 - INFO: Epoch 959/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0691, Val Loss: 0.0612\n",
      "08:31:11 - INFO: Epoch 960/1000\n",
      "08:31:11 - INFO: Train Loss: 0.1364, Val Loss: 0.1870\n",
      "08:31:11 - INFO: Epoch 961/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0749, Val Loss: 0.0650\n",
      "08:31:11 - INFO: Epoch 962/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0813, Val Loss: 0.0607\n",
      "08:31:11 - INFO: Epoch 963/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0648, Val Loss: 0.0833\n",
      "08:31:11 - INFO: Epoch 964/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0648, Val Loss: 0.0898\n",
      "08:31:11 - INFO: Epoch 965/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0595, Val Loss: 0.0755\n",
      "08:31:11 - INFO: Epoch 966/1000\n",
      "08:31:11 - INFO: Train Loss: 0.2025, Val Loss: 0.0791\n",
      "08:31:11 - INFO: Epoch 967/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0707, Val Loss: 0.1638\n",
      "08:31:11 - INFO: Epoch 968/1000\n",
      "08:31:11 - INFO: Train Loss: 0.0620, Val Loss: 0.1518\n",
      "08:31:11 - INFO: Epoch 969/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0775, Val Loss: 0.0665\n",
      "08:31:12 - INFO: Epoch 970/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0685, Val Loss: 0.3164\n",
      "08:31:12 - INFO: Epoch 971/1000\n",
      "08:31:12 - INFO: Train Loss: 0.2708, Val Loss: 0.1246\n",
      "08:31:12 - INFO: Epoch 972/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0683, Val Loss: 0.0590\n",
      "08:31:12 - INFO: Epoch 973/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0951, Val Loss: 0.0603\n",
      "08:31:12 - INFO: Epoch 974/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0644, Val Loss: 0.1109\n",
      "08:31:12 - INFO: Epoch 975/1000\n",
      "08:31:12 - INFO: Train Loss: 0.2593, Val Loss: 0.0616\n",
      "08:31:12 - INFO: Epoch 976/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0642, Val Loss: 0.1182\n",
      "08:31:12 - INFO: Epoch 977/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0896, Val Loss: 0.1246\n",
      "08:31:12 - INFO: Epoch 978/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0674, Val Loss: 0.0565\n",
      "08:31:12 - INFO: Epoch 979/1000\n",
      "08:31:12 - INFO: Train Loss: 0.1039, Val Loss: 0.0815\n",
      "08:31:12 - INFO: Epoch 980/1000\n",
      "08:31:12 - INFO: Train Loss: 0.1764, Val Loss: 0.0593\n",
      "08:31:12 - INFO: Epoch 981/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0643, Val Loss: 0.0659\n",
      "08:31:12 - INFO: Epoch 982/1000\n",
      "08:31:12 - INFO: Train Loss: 0.0802, Val Loss: 0.0652\n",
      "08:31:12 - INFO: Epoch 983/1000\n",
      "08:31:13 - INFO: Train Loss: 0.1304, Val Loss: 0.0648\n",
      "08:31:13 - INFO: Epoch 984/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0946, Val Loss: 0.0675\n",
      "08:31:13 - INFO: Epoch 985/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0910, Val Loss: 0.0570\n",
      "08:31:13 - INFO: Epoch 986/1000\n",
      "08:31:13 - INFO: Train Loss: 0.1094, Val Loss: 0.0715\n",
      "08:31:13 - INFO: Epoch 987/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0819, Val Loss: 0.0732\n",
      "08:31:13 - INFO: Epoch 988/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0591, Val Loss: 0.0681\n",
      "08:31:13 - INFO: Epoch 989/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0665, Val Loss: 0.2628\n",
      "08:31:13 - INFO: Epoch 990/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0621, Val Loss: 0.1064\n",
      "08:31:13 - INFO: Epoch 991/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0702, Val Loss: 0.0680\n",
      "08:31:13 - INFO: Epoch 992/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0637, Val Loss: 0.0589\n",
      "08:31:13 - INFO: Epoch 993/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0707, Val Loss: 0.0594\n",
      "08:31:13 - INFO: Epoch 994/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0798, Val Loss: 0.0814\n",
      "08:31:13 - INFO: Epoch 995/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0768, Val Loss: 0.0903\n",
      "08:31:13 - INFO: Epoch 996/1000\n",
      "08:31:13 - INFO: Train Loss: 0.0682, Val Loss: 0.1383\n",
      "08:31:13 - INFO: Epoch 997/1000\n",
      "08:31:14 - INFO: Train Loss: 0.0904, Val Loss: 0.0890\n",
      "08:31:14 - INFO: Epoch 998/1000\n",
      "08:31:14 - INFO: Train Loss: 0.0736, Val Loss: 0.1377\n",
      "08:31:14 - INFO: Epoch 999/1000\n",
      "08:31:14 - INFO: Train Loss: 0.0834, Val Loss: 0.0889\n",
      "08:31:14 - INFO: Epoch 1000/1000\n",
      "08:31:14 - INFO: Train Loss: 0.0765, Val Loss: 0.0676\n"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "batch_size = 1\n",
    "learning_rate = 1e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "#train_loader = DataLoader(dataset[:1], batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 18\n",
    "model = UNet_Baseline(num_classes=num_classes, device=device)\n",
    "\n",
    "# Initialize diffusion process\n",
    "diffusion = Diffusion(noise_steps=1000, beta_start=0.0001, beta_end=0.002, img_size=64, device=\"cuda\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    train_dataloader=train_loader,\n",
    "    val_dataloader=val_loader,\n",
    "    run_name='nyu_depth_diffusion',\n",
    "    project_name='vdl',\n",
    "    save_dir='models',\n",
    "    resolved_names=dataset.resolved_names,\n",
    "    scheduler=scheduler\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet_Baseline(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load('models/nyu_depth_diffusion_tvso1gep.pth', weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 894-dimensional class vector with some 1s and 0s\n",
    "class_vector = torch.tensor(dataset[0][3]).unsqueeze(0)\n",
    "depth_vector = torch.tensor(dataset[0][4]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 18]), torch.Size([1, 18]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_vector.shape, depth_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:10:36 - INFO: Sampling 1 new images....\n",
      "999it [00:05, 176.81it/s]\n"
     ]
    }
   ],
   "source": [
    "diffusion = Diffusion(noise_steps=1000, img_size=64, device=device)\n",
    "\n",
    "sampled_images = diffusion.sample(model, 1, torch.tensor(dataset[0][3]).unsqueeze(0).to(device), torch.tensor(dataset[0][4]).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the sampled image next to the original image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.imshow(dataset[0][0].permute(1, 2, 0))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sampled_images[0].cpu().permute(1, 2, 0))\n",
    "plt.title('Sampled Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
