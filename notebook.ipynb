{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# vdl Depth and Class Constrained Diffusion\n",
    "- **Name:** Nils Fahrni\n",
    "- **Date:** 07.01.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Preparation\n",
    "\n",
    "- 894 classes -> 18 classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already exists, skipping download.\n"
     ]
    }
   ],
   "source": [
    "from data.nyuv2 import NYUDepthV2\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_t = transforms.Compose([\n",
    "    transforms.CenterCrop(400),\n",
    "    transforms.Resize(64),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "crop_t = transforms.Compose([\n",
    "    transforms.CenterCrop(400)\n",
    "])\n",
    "\n",
    "dataset = NYUDepthV2(root='data', \n",
    "                     download=True, \n",
    "                     preload=False, \n",
    "                     image_transform=image_t, \n",
    "                     seg_transform=crop_t, \n",
    "                     depth_transform=crop_t, \n",
    "                     filtered_classes=[5, 11, 21, 26, 2, 3, 7, 64, 144, 19, 119, 157, 28, 55, 15, 59, 4, 83])\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def unnormalize(img):\n",
    "    mean = torch.tensor([1.3268, 1.1391, 1.0176]).view(-1, 1, 1)  # Reshape to [C, 1, 1]\n",
    "    std = torch.tensor([0.5704, 0.5411, 0.5077]).view(-1, 1, 1)    # Reshape to [C, 1, 1]\n",
    "    img = std * img + mean\n",
    "    img = torch.clip(img, 0, 1)\n",
    "    return img\n",
    "\n",
    "dataset[0][4].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EMA:\n",
    "    def __init__(self, beta):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        self.step = 0\n",
    "\n",
    "    def update_model_average(self, ma_model, current_model):\n",
    "        for current_params, ma_params in zip(current_model.parameters(), ma_model.parameters()):\n",
    "            old_weight, up_weight = ma_params.data, current_params.data\n",
    "            ma_params.data = self.update_average(old_weight, up_weight)\n",
    "\n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.beta + (1 - self.beta) * new\n",
    "\n",
    "    def step_ema(self, ema_model, model, step_start_ema=2000):\n",
    "        if self.step < step_start_ema:\n",
    "            self.reset_parameters(ema_model, model)\n",
    "            self.step += 1\n",
    "            return\n",
    "        self.update_model_average(ema_model, model)\n",
    "        self.step += 1\n",
    "\n",
    "    def reset_parameters(self, ema_model, model):\n",
    "        ema_model.load_state_dict(model.state_dict())\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    def __init__(self, channels, size):\n",
    "        super(SelfAttention, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.size = size\n",
    "        self.mha = nn.MultiheadAttention(channels, 4, batch_first=True)\n",
    "        self.ln = nn.LayerNorm([channels])\n",
    "        self.ff_self = nn.Sequential(\n",
    "            nn.LayerNorm([channels]),\n",
    "            nn.Linear(channels, channels),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(channels, channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.channels, self.size * self.size).swapaxes(1, 2)\n",
    "        x_ln = self.ln(x)\n",
    "        attention_value, _ = self.mha(x_ln, x_ln, x_ln)\n",
    "        attention_value = attention_value + x\n",
    "        attention_value = self.ff_self(attention_value) + attention_value\n",
    "        return attention_value.swapaxes(2, 1).view(-1, self.channels, self.size, self.size)\n",
    "\n",
    "class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            return F.gelu(x + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "\n",
    "\n",
    "class Down(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "        self.maxpool_conv = nn.Sequential(\n",
    "            nn.MaxPool2d(2),\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = self.maxpool_conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "\n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels, in_channels, residual=True),\n",
    "            DoubleConv(in_channels, out_channels, in_channels // 2),\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)\n",
    "        x = torch.cat([skip_x, x], dim=1)\n",
    "        x = self.conv(x)\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])\n",
    "        return x + emb\n",
    "\n",
    "class UNet_conditional(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256, num_classes=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.sa1 = SelfAttention(128, 32)\n",
    "        self.down2 = Down(128, 256)\n",
    "        self.sa2 = SelfAttention(256, 16)\n",
    "        self.down3 = Down(256, 256)\n",
    "        self.sa3 = SelfAttention(256, 8)\n",
    "\n",
    "        self.bot1 = DoubleConv(256, 512)\n",
    "        self.bot2 = DoubleConv(512, 512)\n",
    "        self.bot3 = DoubleConv(512, 256)\n",
    "\n",
    "        self.up1 = Up(512, 128)\n",
    "        self.sa4 = SelfAttention(128, 16)\n",
    "        self.up2 = Up(256, 64)\n",
    "        self.sa5 = SelfAttention(64, 32)\n",
    "        self.up3 = Up(128, 64)\n",
    "        self.sa6 = SelfAttention(64, 64)\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "\n",
    "        if num_classes is not None:\n",
    "            self.class_emb = nn.Linear(num_classes, time_dim)\n",
    "            self.depth_emb = nn.Linear(num_classes, time_dim)\n",
    "            self.t_proj = nn.Linear(3 * time_dim, time_dim)  # Project concatenated embeddings back to time_dim\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000\n",
    "            ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t, class_vector, depth_vector):\n",
    "        # Process time embedding\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        # Process class and depth embeddings\n",
    "        class_emb = self.class_emb(class_vector)\n",
    "        depth_emb = self.depth_emb(depth_vector)\n",
    "\n",
    "        # Combine embeddings\n",
    "        t_combined = torch.cat([t, class_emb, depth_emb], dim=-1)\n",
    "        t_combined = self.t_proj(t_combined)\n",
    "\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1, t_combined)\n",
    "        x2 = self.sa1(x2)\n",
    "        x3 = self.down2(x2, t_combined)\n",
    "        x3 = self.sa2(x3)\n",
    "        x4 = self.down3(x3, t_combined)\n",
    "        x4 = self.sa3(x4)\n",
    "\n",
    "        x4 = self.bot1(x4)\n",
    "        x4 = self.bot2(x4)\n",
    "        x4 = self.bot3(x4)\n",
    "\n",
    "        x = self.up1(x4, x3, t_combined)\n",
    "        x = self.sa4(x)\n",
    "        x = self.up2(x, x2, t_combined)\n",
    "        x = self.sa5(x)\n",
    "        x = self.up3(x, x1, t_combined)\n",
    "        x = self.sa6(x)\n",
    "        output = self.outc(x)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\"\"\" class DoubleConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, mid_channels=None, residual=False):\n",
    "        super().__init__()\n",
    "        self.residual = residual\n",
    "        if not mid_channels:\n",
    "            mid_channels = out_channels\n",
    "        self.double_conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, mid_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1, bias=False),\n",
    "            nn.GroupNorm(1, out_channels),\n",
    "        )\n",
    "        # Add a 1x1 convolution to match dimensions for residuals if needed\n",
    "        if residual and in_channels != out_channels:\n",
    "            self.residual_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        else:\n",
    "            self.residual_conv = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.residual:\n",
    "            residual = self.residual_conv(x) if self.residual_conv else x\n",
    "            return F.gelu(residual + self.double_conv(x))\n",
    "        else:\n",
    "            return self.double_conv(x)\n",
    "        \n",
    "class Up(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, emb_dim=256):\n",
    "        super().__init__()\n",
    "\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.conv = nn.Sequential(\n",
    "            DoubleConv(in_channels + out_channels, out_channels, residual=True),  # Fix concatenated channels\n",
    "            DoubleConv(out_channels, out_channels // 2),  # Halve channels for the next stage\n",
    "        )\n",
    "\n",
    "        self.emb_layer = nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(\n",
    "                emb_dim,\n",
    "                out_channels // 2  # Match the reduced channel size\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, skip_x, t):\n",
    "        x = self.up(x)  # Upsample\n",
    "        x = torch.cat([skip_x, x], dim=1)  # Concatenate along the channel dimension\n",
    "        x = self.conv(x)  # Apply convolution\n",
    "        emb = self.emb_layer(t)[:, :, None, None].repeat(1, 1, x.shape[-2], x.shape[-1])  # Add time embedding\n",
    "        return x + emb \"\"\"\n",
    "\n",
    "\n",
    "class UNet_conditional_small(nn.Module):\n",
    "    def __init__(self, c_in=3, c_out=3, time_dim=256, num_classes=None, device=\"cuda\"):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        self.time_dim = time_dim\n",
    "\n",
    "        self.inc = DoubleConv(c_in, 64)\n",
    "        self.down1 = Down(64, 128)\n",
    "        self.bot = DoubleConv(128, 128)\n",
    "        \n",
    "        # Change here: input after concatenation is (skip_x:64 + x_bot:128) = 192 channels\n",
    "        self.up1 = Up(192, 64)\n",
    "\n",
    "        self.outc = nn.Conv2d(64, c_out, kernel_size=1)\n",
    "\n",
    "        if num_classes is not None:\n",
    "            self.class_emb = nn.Linear(num_classes, time_dim)\n",
    "            self.depth_emb = nn.Linear(num_classes, time_dim)\n",
    "            self.t_proj = nn.Linear(3 * time_dim, time_dim)\n",
    "        else:\n",
    "            self.class_emb = nn.Identity()\n",
    "            self.depth_emb = nn.Identity()\n",
    "            self.t_proj = nn.Identity()\n",
    "\n",
    "    def pos_encoding(self, t, channels):\n",
    "        inv_freq = 1.0 / (\n",
    "            10000 ** (torch.arange(0, channels, 2, device=self.device).float() / channels)\n",
    "        )\n",
    "        pos_enc_a = torch.sin(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc_b = torch.cos(t.repeat(1, channels // 2) * inv_freq)\n",
    "        pos_enc = torch.cat([pos_enc_a, pos_enc_b], dim=-1)\n",
    "        return pos_enc\n",
    "\n",
    "    def forward(self, x, t, class_vector, depth_vector):\n",
    "        t = t.unsqueeze(-1).type(torch.float)\n",
    "        t = self.pos_encoding(t, self.time_dim)\n",
    "\n",
    "        class_emb = self.class_emb(class_vector)\n",
    "        depth_emb = self.depth_emb(depth_vector)\n",
    "        t_combined = torch.cat([t, class_emb, depth_emb], dim=-1)\n",
    "        t_combined = self.t_proj(t_combined)\n",
    "\n",
    "        x1 = self.inc(x)       # [B, 64, 64, 64]\n",
    "        x2 = self.down1(x1, t_combined)  # [B, 128, 32, 32]\n",
    "        x_bot = self.bot(x2)   # [B, 128, 32, 32]\n",
    "\n",
    "        # Here concatenation: skip_x = x1 (64 channels), x_bot (128 channels) => total 192\n",
    "        x = self.up1(x_bot, x1, t_combined)  # [B, 64, 64, 64]\n",
    "        output = self.outc(x)                # [B, 3, 64, 64]\n",
    "\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 23539331\n",
      "Output shape: torch.Size([1, 3, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model\n",
    "net = UNet_conditional(num_classes=18, device=\"cuda\").to(\"cuda\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in net.parameters())}\")\n",
    "\n",
    "# Dummy inputs\n",
    "x = torch.randn(1, 3, 64, 64).to(\"cuda\")\n",
    "t = torch.randint(0, 1000, (x.shape[0],)).to(\"cuda\")\n",
    "class_vector = torch.ones((x.shape[0], 18)).to(\"cuda\")\n",
    "depth_vector = torch.zeros((x.shape[0], 18)).to(\"cuda\")\n",
    "\n",
    "# Forward pass\n",
    "output = net(x, t, class_vector, depth_vector)\n",
    "print(f\"Output shape: {output.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s - %(levelname)s: %(message)s\", level=logging.INFO, datefmt=\"%I:%M:%S\")\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, noise_steps=1000, beta_start=1e-4, beta_end=0.02, img_size=64, device=\"cuda\"):\n",
    "        self.noise_steps = noise_steps\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_end = beta_end\n",
    "\n",
    "        self.beta = self.prepare_noise_schedule().to(device)\n",
    "        self.alpha = 1. - self.beta\n",
    "        self.alpha_hat = torch.cumprod(self.alpha, dim=0)\n",
    "\n",
    "        self.img_size = img_size\n",
    "        self.device = device\n",
    "\n",
    "    def prepare_noise_schedule(self):\n",
    "        return torch.linspace(self.beta_start, self.beta_end, self.noise_steps)\n",
    "\n",
    "    def noise_images(self, x, t):\n",
    "        sqrt_alpha_hat = torch.sqrt(self.alpha_hat[t])[:, None, None, None]\n",
    "        sqrt_one_minus_alpha_hat = torch.sqrt(1 - self.alpha_hat[t])[:, None, None, None]\n",
    "        epsilon = torch.randn_like(x)\n",
    "        return sqrt_alpha_hat * x + sqrt_one_minus_alpha_hat * epsilon, epsilon\n",
    "\n",
    "    def sample_timesteps(self, n):\n",
    "        return torch.randint(low=1, high=self.noise_steps, size=(n,))\n",
    "\n",
    "    def sample(self, model, n, class_vectors=None, depth_vectors=None, cfg_scale=0):\n",
    "        logging.info(f\"Sampling {n} new images....\")\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            x = torch.randn((n, 3, self.img_size, self.img_size)).to(self.device)\n",
    "            for i in tqdm(reversed(range(1, self.noise_steps)), position=0):\n",
    "                t = (torch.ones(n) * i).long().to(self.device)\n",
    "                \n",
    "                current_class_vectors = class_vectors if class_vectors is not None else None\n",
    "                current_depth_vectors = depth_vectors if depth_vectors is not None else None\n",
    "              \n",
    "                predicted_noise = model(x, t, current_class_vectors, current_depth_vectors)\n",
    "                \n",
    "                alpha = self.alpha[t][:, None, None, None]\n",
    "                alpha_hat = self.alpha_hat[t][:, None, None, None]\n",
    "                beta = self.beta[t][:, None, None, None]\n",
    "                if i > 1:\n",
    "                    noise = torch.randn_like(x)\n",
    "                else:\n",
    "                    noise = torch.zeros_like(x)\n",
    "                x = 1 / torch.sqrt(alpha) * (x - ((1 - alpha) / (torch.sqrt(1 - alpha_hat))) * predicted_noise) + torch.sqrt(beta) * noise\n",
    "        \n",
    "        model.train()\n",
    "        x = (x.clamp(-1, 1) + 1) / 2\n",
    "        x = (x * 255).type(torch.uint8)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "class Trainer:\n",
    "    def __init__(self, model, diffusion, optimizer, epochs, device, train_dataloader, val_dataloader=None, run_name='diffusion_model', project_name='diffusion_project', save_dir='models', ema_decay=0.995):\n",
    "        self.model = model.to(device)\n",
    "        self.diffusion = diffusion\n",
    "        self.optimizer = optimizer\n",
    "        self.epochs = epochs\n",
    "        self.device = device\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.run_name = run_name\n",
    "        self.project_name = project_name\n",
    "        self.save_dir = save_dir\n",
    "        self.ema_decay = ema_decay\n",
    "\n",
    "        # Initialize EMA\n",
    "        self.ema = EMA(ema_decay)\n",
    "        self.ema_model = copy.deepcopy(model).eval().requires_grad_(False)\n",
    "\n",
    "        # Initialize Weights & Biases\n",
    "        wandb.init(project=self.project_name, name=self.run_name)\n",
    "        self.run_id = wandb.run.id\n",
    "        self.best_val_loss = float('inf')\n",
    "\n",
    "        # Create a models directory if it doesn't exist\n",
    "        os.makedirs(self.save_dir, exist_ok=True)\n",
    "\n",
    "    def train(self):\n",
    "        for epoch in range(self.epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            pbar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch+1}/{self.epochs}\")\n",
    "            for i, (images, segments, depths, class_vectors, depth_vectors) in enumerate(pbar):\n",
    "                images = images.to(self.device)\n",
    "                class_vectors = class_vectors.to(self.device)\n",
    "                depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "                # Sample time steps\n",
    "                t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "\n",
    "                # Add noise to images\n",
    "                x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "                # Predict noise using the model\n",
    "                predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "                \n",
    "                # Compute loss\n",
    "                loss = F.mse_loss(predicted_noise, noise)\n",
    "\n",
    "                # Backpropagation\n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                # Update EMA model\n",
    "                self.ema.step_ema(self.ema_model, self.model)\n",
    "\n",
    "                epoch_loss += loss.item()\n",
    "                pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "                # Log the loss\n",
    "                wandb.log({'train_loss': loss.item()})\n",
    "\n",
    "            avg_loss = epoch_loss / len(self.train_dataloader)\n",
    "            #print(f\"Epoch [{epoch+1}/{self.epochs}], Loss: {avg_loss:.4f}\")\n",
    "\n",
    "            # Validate and save the model\n",
    "            if self.val_dataloader is not None:\n",
    "                val_loss = self.validate()\n",
    "                wandb.log({'val_loss': val_loss})\n",
    "                self._save_model(val_loss)\n",
    "            else:\n",
    "                self._save_model(avg_loss)\n",
    "\n",
    "    def validate(self):\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for images, segs, depths, class_vectors, depth_vectors in self.val_dataloader:\n",
    "                images = images.to(self.device)\n",
    "                class_vectors = class_vectors.to(self.device)\n",
    "                depth_vectors = depth_vectors.to(self.device)\n",
    "\n",
    "                t = self.diffusion.sample_timesteps(images.size(0)).to(self.device)\n",
    "                x_t, noise = self.diffusion.noise_images(images, t)\n",
    "\n",
    "                predicted_noise = self.model(x_t, t, class_vectors, depth_vectors)\n",
    "                loss = F.mse_loss(predicted_noise, noise)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(self.val_dataloader)\n",
    "        #print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "        return avg_val_loss\n",
    "\n",
    "    def _save_model(self, val_loss):\n",
    "        if val_loss < self.best_val_loss:\n",
    "            self.best_val_loss = val_loss\n",
    "            model_name = f\"{self.model.__class__.__name__}_{self.run_id}.pth\"\n",
    "            save_path = os.path.join(self.save_dir, model_name)\n",
    "            torch.save(self.model.state_dict(), save_path)\n",
    "            #wandb.save(save_path)\n",
    "            #print(f\"Model saved to {save_path} with val_loss {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 64, 64]) torch.Size([1, 400, 400]) torch.Size([1, 400, 400]) torch.Size([1, 18]) torch.Size([1, 18])\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(dataset[:1], batch_size=1, shuffle=True, num_workers=0)\n",
    "\n",
    "for i, (images, segments, depths, class_vectors, depth_vectors) in enumerate(train_loader):\n",
    "    print(images.shape, segments.shape, depths.shape, class_vectors.shape, depth_vectors.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mokaynils\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.18.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\wandb\\run-20241219_154454-8swxltnh</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/okaynils/vdl/runs/8swxltnh' target=\"_blank\">nyu_depth_diffusion</a></strong> to <a href='https://wandb.ai/okaynils/vdl' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/okaynils/vdl' target=\"_blank\">https://wandb.ai/okaynils/vdl</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/okaynils/vdl/runs/8swxltnh' target=\"_blank\">https://wandb.ai/okaynils/vdl/runs/8swxltnh</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000:   0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [512, 640, 1, 1], expected input[1, 512, 16, 16] to have 640 channels, but got 512 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 32\u001b[0m\n\u001b[0;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     20\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     21\u001b[0m     diffusion\u001b[38;5;241m=\u001b[39mdiffusion,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     28\u001b[0m     save_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodels\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     29\u001b[0m )\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 51\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     48\u001b[0m x_t, noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion\u001b[38;5;241m.\u001b[39mnoise_images(images, t)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;66;03m# Predict noise using the model\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m predicted_noise \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_t\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdepth_vectors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Compute loss\u001b[39;00m\n\u001b[0;32m     54\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(predicted_noise, noise)\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[34], line 248\u001b[0m, in \u001b[0;36mUNet_conditional.forward\u001b[1;34m(self, x, t, class_vector, depth_vector)\u001b[0m\n\u001b[0;32m    245\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot2(x4)\n\u001b[0;32m    246\u001b[0m x4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot3(x4)\n\u001b[1;32m--> 248\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mup1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx4\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_combined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    249\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msa4(x)\n\u001b[0;32m    250\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup2(x, x2, t_combined)\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 26\u001b[0m, in \u001b[0;36mUp.forward\u001b[1;34m(self, x, skip_x, t)\u001b[0m\n\u001b[0;32m     24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mup(x)  \u001b[38;5;66;03m# Upsample\u001b[39;00m\n\u001b[0;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([skip_x, x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Concatenate along the channel dimension\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply convolution\u001b[39;00m\n\u001b[0;32m     27\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39memb_layer(t)[:, :, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m]\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])  \u001b[38;5;66;03m# Add time embedding\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m emb\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    248\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 250\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[35], line 51\u001b[0m, in \u001b[0;36mDoubleConv.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     50\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual:\n\u001b[1;32m---> 51\u001b[0m         residual \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresidual_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresidual_conv \u001b[38;5;28;01melse\u001b[39;00m x\n\u001b[0;32m     52\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mgelu(residual \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdouble_conv(x))\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:554\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\fahrn\\Documents\\Classes\\vdl\\fhnw-ds-vdl\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:549\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    537\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\n\u001b[0;32m    539\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    540\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    548\u001b[0m     )\n\u001b[1;32m--> 549\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    550\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[0;32m    551\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=1, weight of size [512, 640, 1, 1], expected input[1, 512, 16, 16] to have 640 channels, but got 512 channels instead"
     ]
    }
   ],
   "source": [
    "epochs = 1000\n",
    "batch_size = 1\n",
    "learning_rate = 1e-5\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'mps')\n",
    "\n",
    "train_loader = DataLoader(dataset[:1], batch_size=batch_size, shuffle=True, num_workers=0)\n",
    "\n",
    "# Initialize model\n",
    "num_classes = 18\n",
    "model = UNet_conditional(num_classes=num_classes, device=device)\n",
    "\n",
    "# Initialize diffusion process\n",
    "diffusion = Diffusion(noise_steps=1000, beta_start=0.0001, beta_end=0.002, img_size=64, device=\"cuda\")\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    diffusion=diffusion,\n",
    "    optimizer=optimizer,\n",
    "    epochs=epochs,\n",
    "    device=device,\n",
    "    train_dataloader=train_loader,\n",
    "    run_name='nyu_depth_diffusion',\n",
    "    project_name='vdl',\n",
    "    save_dir='models'\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\fahrn\\AppData\\Local\\Temp\\ipykernel_2352\\3882087651.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/UNet_conditional_small_bo17ca9c.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = UNet_conditional_small(num_classes=num_classes).to(device)\n",
    "model.load_state_dict(torch.load('models/UNet_conditional_small_bo17ca9c.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 894-dimensional class vector with some 1s and 0s\n",
    "class_vector = torch.tensor(dataset[0][3]).unsqueeze(0)\n",
    "depth_vector = torch.tensor(dataset[0][4]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 18]), torch.Size([1, 18]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_vector.shape, depth_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12:59:48 - INFO: Sampling 1 new images....\n",
      "999it [00:05, 175.27it/s]\n"
     ]
    }
   ],
   "source": [
    "diffusion = Diffusion(noise_steps=1000, img_size=64, device=device)\n",
    "\n",
    "sampled_images = diffusion.sample(model, 1, torch.tensor(dataset[0][3]).unsqueeze(0).to(device), torch.tensor(dataset[0][4]).unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 64, 64])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampled_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the sampled image next to the original image\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "\n",
    "plt.imshow(dataset[0][0].permute(1, 2, 0))\n",
    "plt.title('Original Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(sampled_images[0].cpu().permute(1, 2, 0))\n",
    "plt.title('Sampled Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
